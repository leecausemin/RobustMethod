{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel Pruning v3 (CPv3) Example - Quality-Based Selective Gating\n",
    "\n",
    "## Quality-Based Selective Channel Gating\n",
    "\n",
    "**핵심 개선사항 (vs v1/v2)**:\n",
    "- ✅ **Intrinsic channel quality 기반**: 노이즈 타입 무관!\n",
    "- ✅ **Quality = Discriminability / sqrt(Variance)**: 직관적이고 elegant\n",
    "- ✅ **Two-stage hybrid gating**: Extreme outlier만 prune, 나머지는 soft\n",
    "- ✅ **Clean data만 필요**: 일반화 가능\n",
    "- ✅ **Simple & Elegant**: 복잡한 adaptation 불필요\n",
    "\n",
    "**사용자 관찰**:\n",
    "> \"몇 개의 채널이 유독 노이즈에 민감함\"\n",
    "- 소수(5~10%) 채널만 문제\n",
    "- 나머지(90~95%)는 robust\n",
    "\n",
    "**방법론**:\n",
    "```python\n",
    "# 1. Pre-compute (clean data만):\n",
    "discriminability = |fake_mean - real_mean|  # Artifact detection 능력\n",
    "intrinsic_var = (real_var + fake_var) / 2   # 고유 변동성\n",
    "quality = disc / sqrt(var)                   # 채널 품질\n",
    "\n",
    "# 2. Two-stage gating:\n",
    "if quality < 5th percentile:   # Bottom 5%\n",
    "    gate = 0.0                  # Hard prune\n",
    "elif quality < 20th percentile: # 5~20%\n",
    "    gate = 0.3 ~ 1.0            # Soft weight\n",
    "else:                           # Top 80%\n",
    "    gate = 1.0                  # Keep\n",
    "```\n",
    "\n",
    "**기대 효과**:\n",
    "- 새로운 노이즈에도 일반화\n",
    "- Artifact detection 채널 보존\n",
    "- 직관적이고 simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Clear cache\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if any(x in mod for x in ['NPR', 'npr', 'LGrad', 'lgrad', 'networks', 'method', 'channel']):\n",
    "        del sys.modules[mod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Literal\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.data.dataset import CorruptedDataset\n",
    "from utils.visual.visualizer import DatasetVisualizer\n",
    "from utils.eval.metrics import PredictionCollector, MetricsCalculator\n",
    "\n",
    "# Channel Pruning v3 import\n",
    "from model.method import (\n",
    "    UnifiedChannelPruningV3,\n",
    "    CPv3Config,\n",
    "    compute_separated_statistics,  # v1에서 재사용\n",
    "    compute_channel_quality,        # v3 신규!\n",
    ")\n",
    "from model.LGrad.lgrad_model import LGrad\n",
    "from model.NPR.npr_model import NPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU and Model select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  2 19:58:01 2026       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              27W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              24W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              27W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              25W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              26W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              27W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              26W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              25W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "MODEL_LIST = [\"lgrad\", \"npr\"]\n",
    "MODEL = MODEL_LIST[0]  # \"lgrad\" or \"npr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"corrupted_dataset\"\n",
    "DATASETS = [\n",
    "    \"corrupted_test_data_progan\",\n",
    "    \"corrupted_test_data_stylegan\",\n",
    "    \"corrupted_test_data_stylegan2\",\n",
    "    \"corrupted_test_data_biggan\",\n",
    "]\n",
    "\n",
    "CORRUPTIONS = [\n",
    "    \"original\",\n",
    "    \"gaussian_noise\",\n",
    "    \"jpeg_compression\",\n",
    "]\n",
    "\n",
    "if MODEL == \"lgrad\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 119874\n"
     ]
    }
   ],
   "source": [
    "dataset = CorruptedDataset(\n",
    "    root=ROOT,\n",
    "    datasets=DATASETS,\n",
    "    corruptions=CORRUPTIONS,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/robust_deepfake_ai/model/LGrad/lgrad_model.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(stylegan_weights, map_location=\"cpu\"),\n",
      "/workspace/robust_deepfake_ai/model/LGrad/lgrad_model.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(classifier_weights, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded: LGrad\n"
     ]
    }
   ],
   "source": [
    "# LGrad\n",
    "STYLEGAN_WEIGHTS_ROOT = \"model/LGrad/weights/karras2019stylegan-bedrooms-256x256_discriminator.pth\"\n",
    "CLASSIFIER_WEIGHTS_ROOT = \"model/LGrad/weights/LGrad-Pretrained-Model/LGrad-4class-Trainon-Progan_car_cat_chair_horse.pth\"\n",
    "\n",
    "# NPR\n",
    "NPR_WEIGHTS_ROOT = \"model/NPR/weights/NPR.pth\"\n",
    "\n",
    "if MODEL == \"lgrad\":\n",
    "    base_model = LGrad(\n",
    "        stylegan_weights=STYLEGAN_WEIGHTS_ROOT,\n",
    "        classifier_weights=CLASSIFIER_WEIGHTS_ROOT,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    model_name = \"LGrad\"\n",
    "elif MODEL == \"npr\":\n",
    "    base_model = NPR(\n",
    "        weights=NPR_WEIGHTS_ROOT,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    model_name = \"NPR\"\n",
    "\n",
    "print(f\"Base model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compute Separated Statistics from Clean Data\n",
    "\n",
    "**CPv3도 v1과 동일하게 separated statistics가 필요합니다.**\n",
    "\n",
    "- ProGAN의 original (uncorrupted) 데이터로 statistics 수집\n",
    "- **Labels 필수**: Real (0) vs Fake (1) 구분을 위해\n",
    "- v1에서 이미 계산했다면 재사용 가능!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProGAN clean samples: 8000\n"
     ]
    }
   ],
   "source": [
    "# Clean data 준비 (ProGAN original - LABELS 필수!)\n",
    "progan_clean_indices = [\n",
    "    i for i, s in enumerate(dataset.samples)\n",
    "    if s['dataset'] == \"corrupted_test_data_progan\" and s['corruption'] == \"original\"\n",
    "]\n",
    "\n",
    "print(f\"ProGAN clean samples: {len(progan_clean_indices)}\")\n",
    "\n",
    "# Subset & DataLoader (labels 포함!)\n",
    "clean_subset = Subset(dataset, progan_clean_indices)\n",
    "clean_loader = DataLoader(\n",
    "    clean_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed separated statistics from separated_stats_lgrad_progan.pth\n",
      "Statistics loaded for 106 layers\n"
     ]
    }
   ],
   "source": [
    "# Separated statistics 파일 경로\n",
    "STATS_PATH = f\"separated_stats_{MODEL}_progan.pth\"\n",
    "\n",
    "# 기존 statistics가 있으면 로드, 없으면 계산\n",
    "if os.path.exists(STATS_PATH):\n",
    "    print(f\"Loading pre-computed separated statistics from {STATS_PATH}\")\n",
    "    separated_stats = torch.load(STATS_PATH, weights_only=False)\n",
    "    print(f\"Statistics loaded for {len(separated_stats)} layers\")\n",
    "else:\n",
    "    print(\"Computing separated statistics from clean data...\")\n",
    "    print(\"This may take a few minutes...\\n\")\n",
    "    \n",
    "    # Target layers: None for auto-detection\n",
    "    target_layers_for_stats = None\n",
    "    \n",
    "    # Compute (Real/Fake separated!)\n",
    "    separated_stats = compute_separated_statistics(\n",
    "        model=base_model,\n",
    "        dataloader=clean_loader,  # MUST have labels!\n",
    "        target_layers=target_layers_for_stats,\n",
    "        device=DEVICE,\n",
    "        max_batches=None,\n",
    "    )\n",
    "    \n",
    "    # 저장\n",
    "    torch.save(separated_stats, STATS_PATH)\n",
    "    print(f\"\\nStatistics saved to {STATS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute Channel Quality (CPv3 신규!)\n",
    "\n",
    "**CPv3의 핵심**: Intrinsic channel quality 계산\n",
    "\n",
    "```python\n",
    "Quality = Discriminability / sqrt(Variance)\n",
    "```\n",
    "\n",
    "- **High quality**: High discriminability + Low variance → Robust & useful\n",
    "- **Low quality**: Low discriminability + High variance → Unstable & useless\n",
    "\n",
    "이 quality score는 노이즈 타입과 무관하게 채널의 고유한 특성을 반영합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing channel quality from separated statistics...\n",
      "\n",
      "[CPv3] Computing channel quality for 106 layers...\n",
      "  classifier.conv1 (C=64):\n",
      "    Quality: min=0.0001, Q25=0.0010, median=0.0023, Q75=0.0051, max=0.0149\n",
      "    Disc: mean=0.0062, Var: mean=4.1982\n",
      "  classifier.bn1 (C=64):\n",
      "    Quality: min=0.0000, Q25=0.0015, median=0.0038, Q75=0.0070, max=0.0512\n",
      "    Disc: mean=0.0011, Var: mean=0.0544\n",
      "  classifier.layer1.0.conv1 (C=64):\n",
      "    Quality: min=0.0000, Q25=0.0323, median=0.0723, Q75=0.0970, max=0.1334\n",
      "    Disc: mean=0.0402, Var: mean=0.4505\n",
      "  classifier.layer1.0.bn1 (C=64):\n",
      "    Quality: min=0.0000, Q25=0.0016, median=0.0321, Q75=0.1022, max=0.1696\n",
      "    Disc: mean=0.0088, Var: mean=0.0287\n",
      "  classifier.layer1.0.conv2 (C=64):\n",
      "    Quality: min=0.0002, Q25=0.0169, median=0.0418, Q75=0.1144, max=0.1906\n",
      "    Disc: mean=0.1382, Var: mean=8.1203\n",
      "  classifier.layer1.0.bn2 (C=64):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0129, Q75=0.0425, max=0.1955\n",
      "    Disc: mean=0.0044, Var: mean=0.0408\n",
      "  classifier.layer1.0.conv3 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0096, median=0.0197, Q75=0.0373, max=0.1788\n",
      "    Disc: mean=0.0143, Var: mean=0.3736\n",
      "  classifier.layer1.0.bn3 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0114, Q75=0.0485, max=0.2068\n",
      "    Disc: mean=0.0071, Var: mean=0.0398\n",
      "  classifier.layer1.0.downsample.0 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0214, median=0.0448, Q75=0.0911, max=0.1379\n",
      "    Disc: mean=0.0329, Var: mean=0.4139\n",
      "  classifier.layer1.0.downsample.1 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0139, median=0.0410, Q75=0.0859, max=0.1379\n",
      "    Disc: mean=0.0126, Var: mean=0.1256\n",
      "  classifier.layer1.1.conv1 (C=64):\n",
      "    Quality: min=0.0003, Q25=0.0320, median=0.0610, Q75=0.1012, max=0.1877\n",
      "    Disc: mean=0.1097, Var: mean=2.4983\n",
      "  classifier.layer1.1.bn1 (C=64):\n",
      "    Quality: min=0.0000, Q25=0.0054, median=0.0403, Q75=0.0928, max=0.2528\n",
      "    Disc: mean=0.0096, Var: mean=0.0309\n",
      "  classifier.layer1.1.conv2 (C=64):\n",
      "    Quality: min=0.0073, Q25=0.0351, median=0.0705, Q75=0.1348, max=0.2400\n",
      "    Disc: mean=0.1494, Var: mean=3.2840\n",
      "  classifier.layer1.1.bn2 (C=64):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0085, Q75=0.0519, max=0.1912\n",
      "    Disc: mean=0.0055, Var: mean=0.0288\n",
      "  classifier.layer1.1.conv3 (C=256):\n",
      "    Quality: min=0.0002, Q25=0.0172, median=0.0366, Q75=0.0724, max=0.2031\n",
      "    Disc: mean=0.0255, Var: mean=0.2397\n",
      "  classifier.layer1.1.bn3 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0002, median=0.0222, Q75=0.0605, max=0.1930\n",
      "    Disc: mean=0.0074, Var: mean=0.0452\n",
      "  classifier.layer1.2.conv1 (C=64):\n",
      "    Quality: min=0.0013, Q25=0.0271, median=0.0463, Q75=0.0716, max=0.1927\n",
      "    Disc: mean=0.0771, Var: mean=2.4898\n",
      "  classifier.layer1.2.bn1 (C=64):\n",
      "    Quality: min=0.0000, Q25=0.0018, median=0.0289, Q75=0.0623, max=0.1634\n",
      "    Disc: mean=0.0061, Var: mean=0.0346\n",
      "  classifier.layer1.2.conv2 (C=64):\n",
      "    Quality: min=0.0016, Q25=0.0149, median=0.0251, Q75=0.0371, max=0.1067\n",
      "    Disc: mean=0.0606, Var: mean=4.2885\n",
      "  classifier.layer1.2.bn2 (C=64):\n",
      "    Quality: min=0.0000, Q25=0.0047, median=0.0286, Q75=0.0551, max=0.1332\n",
      "    Disc: mean=0.0044, Var: mean=0.0181\n",
      "  classifier.layer1.2.conv3 (C=256):\n",
      "    Quality: min=0.0001, Q25=0.0169, median=0.0381, Q75=0.0609, max=0.2287\n",
      "    Disc: mean=0.0150, Var: mean=0.1139\n",
      "  classifier.layer1.2.bn3 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0042, median=0.0254, Q75=0.0641, max=0.2624\n",
      "    Disc: mean=0.0073, Var: mean=0.0415\n",
      "  classifier.layer2.0.conv1 (C=128):\n",
      "    Quality: min=0.0000, Q25=0.0196, median=0.0449, Q75=0.0854, max=0.2189\n",
      "    Disc: mean=0.0675, Var: mean=1.8190\n",
      "  classifier.layer2.0.bn1 (C=128):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0222, Q75=0.0720, max=0.2302\n",
      "    Disc: mean=0.0044, Var: mean=0.0090\n",
      "  classifier.layer2.0.conv2 (C=128):\n",
      "    Quality: min=0.0033, Q25=0.0871, median=0.1583, Q75=0.2207, max=0.5285\n",
      "    Disc: mean=0.2968, Var: mean=3.0953\n",
      "  classifier.layer2.0.bn2 (C=128):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.1000, Q75=0.1922, max=0.4573\n",
      "    Disc: mean=0.0223, Var: mean=0.0290\n",
      "  classifier.layer2.0.conv3 (C=512):\n",
      "    Quality: min=0.0001, Q25=0.0956, median=0.1936, Q75=0.2909, max=0.5916\n",
      "    Disc: mean=0.1807, Var: mean=1.0886\n",
      "  classifier.layer2.0.bn3 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0133, Q75=0.0982, max=0.4371\n",
      "    Disc: mean=0.0206, Var: mean=0.0755\n",
      "  classifier.layer2.0.downsample.0 (C=512):\n",
      "    Quality: min=0.0001, Q25=0.0206, median=0.0437, Q75=0.0822, max=0.2098\n",
      "    Disc: mean=0.0592, Var: mean=1.4354\n",
      "  classifier.layer2.0.downsample.1 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0167, median=0.0401, Q75=0.0815, max=0.2098\n",
      "    Disc: mean=0.0130, Var: mean=0.1114\n",
      "  classifier.layer2.1.conv1 (C=128):\n",
      "    Quality: min=0.0004, Q25=0.0578, median=0.1026, Q75=0.1464, max=0.4204\n",
      "    Disc: mean=0.3714, Var: mean=15.8072\n",
      "  classifier.layer2.1.bn1 (C=128):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0085, Q75=0.0894, max=0.3460\n",
      "    Disc: mean=0.0098, Var: mean=0.0251\n",
      "  classifier.layer2.1.conv2 (C=128):\n",
      "    Quality: min=0.0018, Q25=0.0267, median=0.0610, Q75=0.1072, max=0.3483\n",
      "    Disc: mean=0.2251, Var: mean=12.3753\n",
      "  classifier.layer2.1.bn2 (C=128):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0096, Q75=0.0507, max=0.2024\n",
      "    Disc: mean=0.0047, Var: mean=0.0169\n",
      "  classifier.layer2.1.conv3 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0131, median=0.0308, Q75=0.0583, max=0.2136\n",
      "    Disc: mean=0.0215, Var: mean=0.5311\n",
      "  classifier.layer2.1.bn3 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0373, Q75=0.0886, max=0.4369\n",
      "    Disc: mean=0.0200, Var: mean=0.0753\n",
      "  classifier.layer2.2.conv1 (C=128):\n",
      "    Quality: min=0.0014, Q25=0.0388, median=0.0927, Q75=0.1415, max=0.2981\n",
      "    Disc: mean=0.3301, Var: mean=11.4048\n",
      "  classifier.layer2.2.bn1 (C=128):\n",
      "    Quality: min=0.0000, Q25=0.0035, median=0.0620, Q75=0.1059, max=0.2554\n",
      "    Disc: mean=0.0146, Var: mean=0.0439\n",
      "  classifier.layer2.2.conv2 (C=128):\n",
      "    Quality: min=0.0005, Q25=0.0329, median=0.0770, Q75=0.1211, max=0.2734\n",
      "    Disc: mean=0.3028, Var: mean=14.8235\n",
      "  classifier.layer2.2.bn2 (C=128):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0347, Q75=0.0932, max=0.2057\n",
      "    Disc: mean=0.0082, Var: mean=0.0215\n",
      "  classifier.layer2.2.conv3 (C=512):\n",
      "    Quality: min=0.0006, Q25=0.0425, median=0.0925, Q75=0.1555, max=0.2464\n",
      "    Disc: mean=0.0988, Var: mean=1.2307\n",
      "  classifier.layer2.2.bn3 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0523, Q75=0.0995, max=0.4360\n",
      "    Disc: mean=0.0191, Var: mean=0.0714\n",
      "  classifier.layer2.3.conv1 (C=128):\n",
      "    Quality: min=0.0002, Q25=0.0345, median=0.0644, Q75=0.1123, max=0.4842\n",
      "    Disc: mean=0.2182, Var: mean=8.9283\n",
      "  classifier.layer2.3.bn1 (C=128):\n",
      "    Quality: min=0.0000, Q25=0.0133, median=0.0449, Q75=0.1001, max=0.2243\n",
      "    Disc: mean=0.0138, Var: mean=0.0529\n",
      "  classifier.layer2.3.conv2 (C=128):\n",
      "    Quality: min=0.0004, Q25=0.0507, median=0.0917, Q75=0.1531, max=0.2872\n",
      "    Disc: mean=0.5492, Var: mean=27.7024\n",
      "  classifier.layer2.3.bn2 (C=128):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0675, Q75=0.1435, max=0.2444\n",
      "    Disc: mean=0.0119, Var: mean=0.0266\n",
      "  classifier.layer2.3.conv3 (C=512):\n",
      "    Quality: min=0.0001, Q25=0.0959, median=0.1466, Q75=0.1621, max=0.2989\n",
      "    Disc: mean=0.2046, Var: mean=3.0027\n",
      "  classifier.layer2.3.bn3 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0009, median=0.0436, Q75=0.1019, max=0.4312\n",
      "    Disc: mean=0.0173, Var: mean=0.0576\n",
      "  classifier.layer3.0.conv1 (C=256):\n",
      "    Quality: min=0.0017, Q25=0.1076, median=0.2082, Q75=0.3036, max=0.4965\n",
      "    Disc: mean=0.5084, Var: mean=6.2266\n",
      "  classifier.layer3.0.bn1 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0280, Q75=0.2062, max=0.4260\n",
      "    Disc: mean=0.0162, Var: mean=0.0174\n",
      "  classifier.layer3.0.conv2 (C=256):\n",
      "    Quality: min=0.0003, Q25=0.3034, median=0.6388, Q75=0.8475, max=1.0174\n",
      "    Disc: mean=3.4272, Var: mean=35.0928\n",
      "  classifier.layer3.0.bn2 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.2287, Q75=0.6630, max=1.0068\n",
      "    Disc: mean=0.0616, Var: mean=0.0277\n",
      "  classifier.layer3.0.conv3 (C=1024):\n",
      "    Quality: min=0.0015, Q25=0.6049, median=0.8328, Q75=0.9352, max=1.0808\n",
      "    Disc: mean=1.6148, Var: mean=6.1980\n",
      "  classifier.layer3.0.bn3 (C=1024):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0736, Q75=0.5044, max=1.0564\n",
      "    Disc: mean=0.0961, Var: mean=0.0955\n",
      "  classifier.layer3.0.downsample.0 (C=1024):\n",
      "    Quality: min=0.0001, Q25=0.1152, median=0.2164, Q75=0.3161, max=0.6543\n",
      "    Disc: mean=0.5039, Var: mean=5.4301\n",
      "  classifier.layer3.0.downsample.1 (C=1024):\n",
      "    Quality: min=0.0000, Q25=0.1116, median=0.2163, Q75=0.3159, max=0.6543\n",
      "    Disc: mean=0.0371, Var: mean=0.0601\n",
      "  classifier.layer3.1.conv1 (C=256):\n",
      "    Quality: min=0.0030, Q25=0.6332, median=0.7571, Q75=0.8173, max=1.0087\n",
      "    Disc: mean=9.1496, Var: mean=240.6984\n",
      "  classifier.layer3.1.bn1 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.7503, max=0.8640\n",
      "    Disc: mean=0.0492, Var: mean=0.0163\n",
      "  classifier.layer3.1.conv2 (C=256):\n",
      "    Quality: min=0.0016, Q25=0.3355, median=0.7132, Q75=1.0405, max=1.1665\n",
      "    Disc: mean=6.2591, Var: mean=98.5563\n",
      "  classifier.layer3.1.bn2 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0287, Q75=0.2947, max=1.0680\n",
      "    Disc: mean=0.0379, Var: mean=0.0180\n",
      "  classifier.layer3.1.conv3 (C=1024):\n",
      "    Quality: min=0.0030, Q25=0.3416, median=0.7265, Q75=0.9383, max=1.1356\n",
      "    Disc: mean=0.8168, Var: mean=2.3638\n",
      "  classifier.layer3.1.bn3 (C=1024):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0786, Q75=0.3216, max=1.2935\n",
      "    Disc: mean=0.0805, Var: mean=0.0940\n",
      "  classifier.layer3.2.conv1 (C=256):\n",
      "    Quality: min=0.0022, Q25=0.4169, median=0.7129, Q75=0.9326, max=1.1848\n",
      "    Disc: mean=5.8673, Var: mean=102.9754\n",
      "  classifier.layer3.2.bn1 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.5713, max=1.1170\n",
      "    Disc: mean=0.0625, Var: mean=0.0264\n",
      "  classifier.layer3.2.conv2 (C=256):\n",
      "    Quality: min=0.0014, Q25=0.3509, median=1.0331, Q75=1.3214, max=1.8005\n",
      "    Disc: mean=5.8444, Var: mean=76.0583\n",
      "  classifier.layer3.2.bn2 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.3306, max=1.1669\n",
      "    Disc: mean=0.0324, Var: mean=0.0162\n",
      "  classifier.layer3.2.conv3 (C=1024):\n",
      "    Quality: min=0.0001, Q25=0.2361, median=0.5156, Q75=0.7406, max=1.2995\n",
      "    Disc: mean=0.6223, Var: mean=1.9361\n",
      "  classifier.layer3.2.bn3 (C=1024):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.1814, Q75=0.3875, max=1.5021\n",
      "    Disc: mean=0.0958, Var: mean=0.0961\n",
      "  classifier.layer3.3.conv1 (C=256):\n",
      "    Quality: min=0.0003, Q25=0.5828, median=0.9013, Q75=1.0997, max=1.4272\n",
      "    Disc: mean=5.2223, Var: mean=70.5436\n",
      "  classifier.layer3.3.bn1 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.1801, max=1.2812\n",
      "    Disc: mean=0.0307, Var: mean=0.0103\n",
      "  classifier.layer3.3.conv2 (C=256):\n",
      "    Quality: min=0.0048, Q25=0.3427, median=1.1199, Q75=1.3743, max=1.6455\n",
      "    Disc: mean=2.0989, Var: mean=9.2742\n",
      "  classifier.layer3.3.bn2 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.1741, max=1.0360\n",
      "    Disc: mean=0.0193, Var: mean=0.0104\n",
      "  classifier.layer3.3.conv3 (C=1024):\n",
      "    Quality: min=0.0000, Q25=0.2264, median=0.4601, Q75=0.7201, max=1.0205\n",
      "    Disc: mean=0.4466, Var: mean=1.1812\n",
      "  classifier.layer3.3.bn3 (C=1024):\n",
      "    Quality: min=0.0000, Q25=0.0015, median=0.1581, Q75=0.3603, max=1.9223\n",
      "    Disc: mean=0.0931, Var: mean=0.0902\n",
      "  classifier.layer3.4.conv1 (C=256):\n",
      "    Quality: min=0.0035, Q25=0.4210, median=1.0754, Q75=1.4350, max=2.2527\n",
      "    Disc: mean=7.0041, Var: mean=52.8829\n",
      "  classifier.layer3.4.bn1 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0704, Q75=0.8904, max=2.3670\n",
      "    Disc: mean=0.0733, Var: mean=0.0108\n",
      "  classifier.layer3.4.conv2 (C=256):\n",
      "    Quality: min=0.0025, Q25=0.6789, median=1.4992, Q75=2.1876, max=2.9742\n",
      "    Disc: mean=4.7535, Var: mean=18.3295\n",
      "  classifier.layer3.4.bn2 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.0140, max=1.1900\n",
      "    Disc: mean=0.0078, Var: mean=0.0040\n",
      "  classifier.layer3.4.conv3 (C=1024):\n",
      "    Quality: min=0.0009, Q25=0.0795, median=0.1584, Q75=0.2814, max=1.2405\n",
      "    Disc: mean=0.1055, Var: mean=0.4645\n",
      "  classifier.layer3.4.bn3 (C=1024):\n",
      "    Quality: min=0.0000, Q25=0.0102, median=0.0972, Q75=0.2533, max=1.9214\n",
      "    Disc: mean=0.0807, Var: mean=0.0851\n",
      "  classifier.layer3.5.conv1 (C=256):\n",
      "    Quality: min=0.0075, Q25=0.3676, median=0.7258, Q75=1.0504, max=1.5835\n",
      "    Disc: mean=3.7051, Var: mean=37.0830\n",
      "  classifier.layer3.5.bn1 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.0735, max=1.6102\n",
      "    Disc: mean=0.0329, Var: mean=0.0166\n",
      "  classifier.layer3.5.conv2 (C=256):\n",
      "    Quality: min=0.0095, Q25=0.2511, median=0.6234, Q75=0.8828, max=2.4052\n",
      "    Disc: mean=1.8479, Var: mean=13.6059\n",
      "  classifier.layer3.5.bn2 (C=256):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.0635, max=1.3948\n",
      "    Disc: mean=0.0170, Var: mean=0.0089\n",
      "  classifier.layer3.5.conv3 (C=1024):\n",
      "    Quality: min=0.0011, Q25=0.1562, median=0.3994, Q75=0.6178, max=1.4125\n",
      "    Disc: mean=0.3841, Var: mean=1.3593\n",
      "  classifier.layer3.5.bn3 (C=1024):\n",
      "    Quality: min=0.0000, Q25=0.0017, median=0.0585, Q75=0.2073, max=2.2274\n",
      "    Disc: mean=0.0736, Var: mean=0.0734\n",
      "  classifier.layer4.0.conv1 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.5093, median=0.9356, Q75=1.2293, max=2.0986\n",
      "    Disc: mean=4.5690, Var: mean=40.2827\n",
      "  classifier.layer4.0.bn1 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.2015, max=1.4230\n",
      "    Disc: mean=0.0400, Var: mean=0.0223\n",
      "  classifier.layer4.0.conv2 (C=512):\n",
      "    Quality: min=0.0125, Q25=0.7254, median=1.2510, Q75=1.6041, max=2.1159\n",
      "    Disc: mean=11.9316, Var: mean=142.6716\n",
      "  classifier.layer4.0.bn2 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.4372, max=1.8761\n",
      "    Disc: mean=0.0613, Var: mean=0.0130\n",
      "  classifier.layer4.0.conv3 (C=2048):\n",
      "    Quality: min=0.0001, Q25=0.8691, median=1.3938, Q75=1.6638, max=2.0334\n",
      "    Disc: mean=3.8554, Var: mean=10.9892\n",
      "  classifier.layer4.0.bn3 (C=2048):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.3791, Q75=0.7556, max=2.6341\n",
      "    Disc: mean=0.1975, Var: mean=0.0963\n",
      "  classifier.layer4.0.downsample.0 (C=2048):\n",
      "    Quality: min=0.0007, Q25=0.4437, median=0.9943, Q75=1.5444, max=2.5161\n",
      "    Disc: mean=6.5633, Var: mean=52.7280\n",
      "  classifier.layer4.0.downsample.1 (C=2048):\n",
      "    Quality: min=0.0007, Q25=0.4437, median=0.9943, Q75=1.5444, max=2.5161\n",
      "    Disc: mean=0.2407, Var: mean=0.1041\n",
      "  classifier.layer4.1.conv1 (C=512):\n",
      "    Quality: min=0.0008, Q25=0.6276, median=1.0529, Q75=1.6356, max=3.1056\n",
      "    Disc: mean=17.4976, Var: mean=238.7991\n",
      "  classifier.layer4.1.bn1 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.0000, max=2.9464\n",
      "    Disc: mean=0.0308, Var: mean=0.0035\n",
      "  classifier.layer4.1.conv2 (C=512):\n",
      "    Quality: min=0.0909, Q25=2.2987, median=2.6639, Q75=3.2779, max=3.8998\n",
      "    Disc: mean=8.9025, Var: mean=15.4365\n",
      "  classifier.layer4.1.bn2 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.3413, max=3.7750\n",
      "    Disc: mean=0.0110, Var: mean=0.0009\n",
      "  classifier.layer4.1.conv3 (C=2048):\n",
      "    Quality: min=0.0009, Q25=0.3979, median=0.7134, Q75=1.3829, max=4.0247\n",
      "    Disc: mean=0.6057, Var: mean=0.9800\n",
      "  classifier.layer4.1.bn3 (C=2048):\n",
      "    Quality: min=0.0000, Q25=0.0699, median=0.3700, Q75=0.8919, max=3.2531\n",
      "    Disc: mean=0.2194, Var: mean=0.1116\n",
      "  classifier.layer4.2.conv1 (C=512):\n",
      "    Quality: min=0.0032, Q25=0.2659, median=0.6487, Q75=1.1616, max=3.5699\n",
      "    Disc: mean=8.7317, Var: mean=169.8919\n",
      "  classifier.layer4.2.bn1 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.0000, max=4.7965\n",
      "    Disc: mean=0.0145, Var: mean=0.0014\n",
      "  classifier.layer4.2.conv2 (C=512):\n",
      "    Quality: min=0.0367, Q25=1.3649, median=2.3131, Q75=3.1491, max=4.6752\n",
      "    Disc: mean=3.6970, Var: mean=3.3027\n",
      "  classifier.layer4.2.bn2 (C=512):\n",
      "    Quality: min=0.0000, Q25=0.0000, median=0.0000, Q75=0.1300, max=4.4855\n",
      "    Disc: mean=0.0101, Var: mean=0.0083\n",
      "  classifier.layer4.2.conv3 (C=2048):\n",
      "    Quality: min=0.0022, Q25=0.2360, median=0.3288, Q75=0.4919, max=5.0375\n",
      "    Disc: mean=0.6284, Var: mean=4.0393\n",
      "  classifier.layer4.2.bn3 (C=2048):\n",
      "    Quality: min=0.0000, Q25=0.1768, median=0.2531, Q75=0.4569, max=3.3292\n",
      "    Disc: mean=0.2949, Var: mean=1.2899\n",
      "[CPv3] Channel quality computed for 106 layers\n",
      "\n",
      "Quality saved to channel_quality_lgrad_progan.pth\n"
     ]
    }
   ],
   "source": [
    "# Channel quality 파일 경로\n",
    "QUALITY_PATH = f\"channel_quality_{MODEL}_progan.pth\"\n",
    "\n",
    "# 기존 quality가 있으면 로드, 없으면 계산\n",
    "if os.path.exists(QUALITY_PATH):\n",
    "    print(f\"Loading pre-computed channel quality from {QUALITY_PATH}\")\n",
    "    quality_stats = torch.load(QUALITY_PATH, weights_only=False)\n",
    "    print(f\"Quality loaded for {len(quality_stats)} layers\")\n",
    "else:\n",
    "    print(\"Computing channel quality from separated statistics...\\n\")\n",
    "    \n",
    "    # Compute quality (separated_stats 필요!)\n",
    "    quality_stats = compute_channel_quality(separated_stats)\n",
    "    \n",
    "    # 저장\n",
    "    torch.save(quality_stats, QUALITY_PATH)\n",
    "    print(f\"\\nQuality saved to {QUALITY_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Channel Pruning v3 Model\n",
    "\n",
    "**CPv3 Config**:\n",
    "- `low_quality_percentile`: Bottom X% → Hard prune (gate=0)\n",
    "- `medium_quality_percentile`: Bottom Y% → Soft weight (gate=0.3~1.0)\n",
    "- Top (100-Y)% → Keep (gate=1.0)\n",
    "\n",
    "**기본 설정**: 5% hard prune, 20% soft weight, 80% keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration (CPv3):\n",
      "  Low quality percentile: 5.0% (hard prune)\n",
      "  Medium quality percentile: 20.0% (soft weight)\n",
      "  Medium quality min weight: 0.3\n",
      "\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 21/64 (32.8%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 43/64 (67.2%)\n",
      "      Gate mean: 0.6719, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 36/64 (56.2%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 28/64 (43.8%)\n",
      "      Gate mean: 0.4375, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 5/64 (7.8%)\n",
      "      Medium quality (soft): 7/64 (10.9%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8806, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/64 (39.1%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 39/64 (60.9%)\n",
      "      Gate mean: 0.6094, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 4/64 (6.2%)\n",
      "      Medium quality (soft): 8/64 (12.5%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8689, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 27/64 (42.2%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 37/64 (57.8%)\n",
      "      Gate mean: 0.5781, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 39/256 (15.2%)\n",
      "      Medium quality (soft): 28/256 (10.9%)\n",
      "      High quality (keep): 189/256 (73.8%)\n",
      "      Gate mean: 0.8170, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 114/256 (44.5%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 142/256 (55.5%)\n",
      "      Gate mean: 0.5547, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 38/256 (14.8%)\n",
      "      Medium quality (soft): 35/256 (13.7%)\n",
      "      High quality (keep): 183/256 (71.5%)\n",
      "      Gate mean: 0.7988, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 58/256 (22.7%)\n",
      "      Medium quality (soft): 18/256 (7.0%)\n",
      "      High quality (keep): 180/256 (70.3%)\n",
      "      Gate mean: 0.7565, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 5/64 (7.8%)\n",
      "      Medium quality (soft): 7/64 (10.9%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8953, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 17/64 (26.6%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 47/64 (73.4%)\n",
      "      Gate mean: 0.7344, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 3/64 (4.7%)\n",
      "      Medium quality (soft): 9/64 (14.1%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8951, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 33/64 (51.6%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 31/64 (48.4%)\n",
      "      Gate mean: 0.4844, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 21/256 (8.2%)\n",
      "      Medium quality (soft): 36/256 (14.1%)\n",
      "      High quality (keep): 199/256 (77.7%)\n",
      "      Gate mean: 0.8730, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 89/256 (34.8%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 167/256 (65.2%)\n",
      "      Gate mean: 0.6523, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 3/64 (4.7%)\n",
      "      Medium quality (soft): 9/64 (14.1%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8810, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 21/64 (32.8%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 43/64 (67.2%)\n",
      "      Gate mean: 0.6719, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 3/64 (4.7%)\n",
      "      Medium quality (soft): 9/64 (14.1%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8915, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 17/64 (26.6%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 47/64 (73.4%)\n",
      "      Gate mean: 0.7344, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 16/256 (6.2%)\n",
      "      Medium quality (soft): 35/256 (13.7%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8884, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 89/256 (34.8%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 167/256 (65.2%)\n",
      "      Gate mean: 0.6523, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.8899, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 59/128 (46.1%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 69/128 (53.9%)\n",
      "      Gate mean: 0.5391, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.9148, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 36/128 (28.1%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 92/128 (71.9%)\n",
      "      Gate mean: 0.7188, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 75/512 (14.6%)\n",
      "      Medium quality (soft): 56/512 (10.9%)\n",
      "      High quality (keep): 381/512 (74.4%)\n",
      "      Gate mean: 0.8151, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 243/512 (47.5%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 269/512 (52.5%)\n",
      "      Gate mean: 0.5254, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 82/512 (16.0%)\n",
      "      Medium quality (soft): 53/512 (10.4%)\n",
      "      High quality (keep): 377/512 (73.6%)\n",
      "      Gate mean: 0.8010, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 177/512 (34.6%)\n",
      "      Medium quality (soft): 15/512 (2.9%)\n",
      "      High quality (keep): 320/512 (62.5%)\n",
      "      Gate mean: 0.6441, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.9017, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 66/128 (51.6%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 62/128 (48.4%)\n",
      "      Gate mean: 0.4844, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.8997, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 66/128 (51.6%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 62/128 (48.4%)\n",
      "      Gate mean: 0.4844, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 41/512 (8.0%)\n",
      "      Medium quality (soft): 68/512 (13.3%)\n",
      "      High quality (keep): 403/512 (78.7%)\n",
      "      Gate mean: 0.8714, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 188/512 (36.7%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 324/512 (63.3%)\n",
      "      Gate mean: 0.6328, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.9047, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 37/128 (28.9%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 91/128 (71.1%)\n",
      "      Gate mean: 0.7109, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.8976, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 55/128 (43.0%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 73/128 (57.0%)\n",
      "      Gate mean: 0.5703, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8927, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 155/512 (30.3%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 357/512 (69.7%)\n",
      "      Gate mean: 0.6973, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.8961, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 31/128 (24.2%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 97/128 (75.8%)\n",
      "      Gate mean: 0.7578, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.9002, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 47/128 (36.7%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 81/128 (63.3%)\n",
      "      Gate mean: 0.6328, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.9003, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 181/512 (35.4%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 331/512 (64.6%)\n",
      "      Gate mean: 0.6465, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.9007, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 122/256 (47.7%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 134/256 (52.3%)\n",
      "      Gate mean: 0.5234, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8990, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 107/256 (41.8%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 149/256 (58.2%)\n",
      "      Gate mean: 0.5820, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 78/1024 (7.6%)\n",
      "      Medium quality (soft): 137/1024 (13.4%)\n",
      "      High quality (keep): 809/1024 (79.0%)\n",
      "      Gate mean: 0.8802, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 498/1024 (48.6%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 526/1024 (51.4%)\n",
      "      Gate mean: 0.5137, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 81/1024 (7.9%)\n",
      "      Medium quality (soft): 138/1024 (13.5%)\n",
      "      High quality (keep): 805/1024 (78.6%)\n",
      "      Gate mean: 0.8737, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 206/1024 (20.1%)\n",
      "      Medium quality (soft): 73/1024 (7.1%)\n",
      "      High quality (keep): 745/1024 (72.8%)\n",
      "      Gate mean: 0.7799, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.9073, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 132/256 (51.6%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 124/256 (48.4%)\n",
      "      Gate mean: 0.4844, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8879, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 127/256 (49.6%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 129/256 (50.4%)\n",
      "      Gate mean: 0.5039, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 55/1024 (5.4%)\n",
      "      Medium quality (soft): 153/1024 (14.9%)\n",
      "      High quality (keep): 816/1024 (79.7%)\n",
      "      Gate mean: 0.8920, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 455/1024 (44.4%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 569/1024 (55.6%)\n",
      "      Gate mean: 0.5557, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.9050, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 138/256 (53.9%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 118/256 (46.1%)\n",
      "      Gate mean: 0.4609, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8922, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 151/256 (59.0%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 105/256 (41.0%)\n",
      "      Gate mean: 0.4102, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 51/1024 (5.0%)\n",
      "      Medium quality (soft): 153/1024 (14.9%)\n",
      "      High quality (keep): 820/1024 (80.1%)\n",
      "      Gate mean: 0.8976, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 338/1024 (33.0%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 686/1024 (67.0%)\n",
      "      Gate mean: 0.6699, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8991, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 181/256 (70.7%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 75/256 (29.3%)\n",
      "      Gate mean: 0.2930, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8900, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 175/256 (68.4%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 81/256 (31.6%)\n",
      "      Gate mean: 0.3164, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 51/1024 (5.0%)\n",
      "      Medium quality (soft): 153/1024 (14.9%)\n",
      "      High quality (keep): 820/1024 (80.1%)\n",
      "      Gate mean: 0.8967, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 301/1024 (29.4%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 723/1024 (70.6%)\n",
      "      Gate mean: 0.7061, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8986, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 129/256 (50.4%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 127/256 (49.6%)\n",
      "      Gate mean: 0.4961, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.9037, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 195/256 (76.2%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 61/256 (23.8%)\n",
      "      Gate mean: 0.2383, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 53/1024 (5.2%)\n",
      "      Medium quality (soft): 151/1024 (14.7%)\n",
      "      High quality (keep): 820/1024 (80.1%)\n",
      "      Gate mean: 0.8982, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 352/1024 (34.4%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 672/1024 (65.6%)\n",
      "      Gate mean: 0.6562, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8928, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 168/256 (65.6%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 88/256 (34.4%)\n",
      "      Gate mean: 0.3438, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.9031, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 170/256 (66.4%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 86/256 (33.6%)\n",
      "      Gate mean: 0.3359, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 51/1024 (5.0%)\n",
      "      Medium quality (soft): 153/1024 (14.9%)\n",
      "      High quality (keep): 820/1024 (80.1%)\n",
      "      Gate mean: 0.8991, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 465/1024 (45.4%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 559/1024 (54.6%)\n",
      "      Gate mean: 0.5459, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8947, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 332/512 (64.8%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 180/512 (35.2%)\n",
      "      Gate mean: 0.3516, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8936, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 331/512 (64.6%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 181/512 (35.4%)\n",
      "      Gate mean: 0.3535, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 102/2048 (5.0%)\n",
      "      Medium quality (soft): 307/2048 (15.0%)\n",
      "      High quality (keep): 1639/2048 (80.0%)\n",
      "      Gate mean: 0.8983, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 739/2048 (36.1%)\n",
      "      Medium quality (soft): 0/2048 (0.0%)\n",
      "      High quality (keep): 1309/2048 (63.9%)\n",
      "      Gate mean: 0.6392, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 102/2048 (5.0%)\n",
      "      Medium quality (soft): 307/2048 (15.0%)\n",
      "      High quality (keep): 1639/2048 (80.0%)\n",
      "      Gate mean: 0.8949, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 106/2048 (5.2%)\n",
      "      Medium quality (soft): 307/2048 (15.0%)\n",
      "      High quality (keep): 1635/2048 (79.8%)\n",
      "      Gate mean: 0.8929, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8962, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 452/512 (88.3%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 60/512 (11.7%)\n",
      "      Gate mean: 0.1172, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.9161, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 355/512 (69.3%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 157/512 (30.7%)\n",
      "      Gate mean: 0.3066, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 102/2048 (5.0%)\n",
      "      Medium quality (soft): 307/2048 (15.0%)\n",
      "      High quality (keep): 1639/2048 (80.0%)\n",
      "      Gate mean: 0.8996, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 467/2048 (22.8%)\n",
      "      Medium quality (soft): 1/2048 (0.0%)\n",
      "      High quality (keep): 1580/2048 (77.1%)\n",
      "      Gate mean: 0.7718, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8943, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 474/512 (92.6%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 38/512 (7.4%)\n",
      "      Gate mean: 0.0742, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8968, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 376/512 (73.4%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 136/512 (26.6%)\n",
      "      Gate mean: 0.2656, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 102/2048 (5.0%)\n",
      "      Medium quality (soft): 307/2048 (15.0%)\n",
      "      High quality (keep): 1639/2048 (80.0%)\n",
      "      Gate mean: 0.9055, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 219/2048 (10.7%)\n",
      "      Medium quality (soft): 194/2048 (9.5%)\n",
      "      High quality (keep): 1635/2048 (79.8%)\n",
      "      Gate mean: 0.8660, min: 0.0000, max: 1.0000\n",
      "[CPv3] Initialized for LGrad\n",
      "[CPv3] Target layers: 106\n",
      "[CPv3] Low quality threshold: 5.0%\n",
      "[CPv3] Medium quality threshold: 20.0%\n",
      "\n",
      "Channel Pruning v3 model created!\n"
     ]
    }
   ],
   "source": [
    "# Config 설정\n",
    "config = CPv3Config(\n",
    "    model=model_name,\n",
    "    target_layers=None,  # Use all layers from quality_stats\n",
    "    \n",
    "    # Quality-based thresholds\n",
    "    low_quality_percentile=0.05,    # Bottom 5% → hard prune\n",
    "    medium_quality_percentile=0.20,  # Bottom 20% → soft weight\n",
    "    \n",
    "    # Gating parameters\n",
    "    medium_quality_min_weight=0.3,  # Minimum weight for medium quality\n",
    "    \n",
    "    # Optional: Discriminability threshold\n",
    "    use_discriminability_threshold=True,\n",
    "    discriminability_min=0.001,  # Always prune if disc < this\n",
    "    \n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"Configuration (CPv3):\")\n",
    "print(f\"  Low quality percentile: {config.low_quality_percentile:.1%} (hard prune)\")\n",
    "print(f\"  Medium quality percentile: {config.medium_quality_percentile:.1%} (soft weight)\")\n",
    "print(f\"  Medium quality min weight: {config.medium_quality_min_weight}\")\n",
    "print()\n",
    "\n",
    "# Model 생성\n",
    "model = UnifiedChannelPruningV3(\n",
    "    base_model=base_model,\n",
    "    quality_stats=quality_stats,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nChannel Pruning v3 model created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG: Quality vs Discriminability 관계 분석\n",
    "\n",
    "**핵심 질문**: High discriminability 채널들이 실수로 pruned되고 있는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEBUG: Quality vs Discriminability 분석\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# 첫 5개 레이어만 분석\n",
    "for layer_name in list(quality_stats.keys())[:5]:\n",
    "    stats = quality_stats[layer_name]\n",
    "    \n",
    "    quality = stats['quality']\n",
    "    disc = stats['discriminability']\n",
    "    var = stats['variance']\n",
    "    \n",
    "    # Percentile thresholds\n",
    "    quality_sorted = torch.sort(quality)[0]\n",
    "    low_idx = int(len(quality) * 0.05)\n",
    "    medium_idx = int(len(quality) * 0.20)\n",
    "    \n",
    "    low_threshold = quality_sorted[low_idx]\n",
    "    medium_threshold = quality_sorted[medium_idx]\n",
    "    \n",
    "    # Masks\n",
    "    low_mask = quality < low_threshold\n",
    "    medium_mask = (quality >= low_threshold) & (quality < medium_threshold)\n",
    "    high_mask = quality >= medium_threshold\n",
    "    \n",
    "    print(f\"{layer_name}:\")\n",
    "    print(f\"  Total channels: {len(quality)}\")\n",
    "    print(f\"  Low quality (pruned): {low_mask.sum().item()}\")\n",
    "    print(f\"  Medium quality (soft): {medium_mask.sum().item()}\")\n",
    "    print(f\"  High quality (keep): {high_mask.sum().item()}\")\n",
    "    print()\n",
    "    \n",
    "    # 🚨 핵심: Pruned된 채널들의 discriminability 확인!\n",
    "    if low_mask.any():\n",
    "        pruned_disc = disc[low_mask]\n",
    "        kept_disc = disc[high_mask]\n",
    "        \n",
    "        print(f\"  Discriminability (PRUNED channels):\")\n",
    "        print(f\"    Mean: {pruned_disc.mean():.6f}\")\n",
    "        print(f\"    Max:  {pruned_disc.max():.6f}\")\n",
    "        print(f\"    Min:  {pruned_disc.min():.6f}\")\n",
    "        print()\n",
    "        print(f\"  Discriminability (KEPT channels):\")\n",
    "        print(f\"    Mean: {kept_disc.mean():.6f}\")\n",
    "        print(f\"    Max:  {kept_disc.max():.6f}\")\n",
    "        print(f\"    Min:  {kept_disc.min():.6f}\")\n",
    "        print()\n",
    "        \n",
    "        # 🚨🚨🚨 문제 진단!\n",
    "        if pruned_disc.mean() > kept_disc.mean():\n",
    "            print(f\"  🚨🚨🚨 치명적 문제 발견!\")\n",
    "            print(f\"  Pruned 채널들의 discriminability가 더 높습니다!\")\n",
    "            print(f\"  → Artifact detection에 중요한 채널을 제거하고 있음!\")\n",
    "        print()\n",
    "    \n",
    "    # Quality vs Discriminability 상관관계\n",
    "    print(f\"  Quality vs Discriminability:\")\n",
    "    print(f\"    Quality mean: {quality.mean():.4f}\")\n",
    "    print(f\"    Disc mean: {disc.mean():.6f}\")\n",
    "    print(f\"    Var mean: {var.mean():.6f}\")\n",
    "    print(\"=\"*70)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG: Top Discriminability 채널 분석\n",
    "\n",
    "Discriminability가 가장 높은 채널들이 어떻게 처리되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DEBUG: Top Discriminability 채널들의 운명\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# 전체 레이어에서 top discriminability 채널들 찾기\n",
    "all_disc = []\n",
    "all_quality = []\n",
    "all_layers = []\n",
    "\n",
    "for layer_name, stats in quality_stats.items():\n",
    "    disc = stats['discriminability']\n",
    "    quality = stats['quality']\n",
    "    \n",
    "    for i in range(len(disc)):\n",
    "        all_disc.append(disc[i].item())\n",
    "        all_quality.append(quality[i].item())\n",
    "        all_layers.append((layer_name, i))\n",
    "\n",
    "# Top 10 discriminability 채널\n",
    "import numpy as np\n",
    "all_disc = np.array(all_disc)\n",
    "all_quality = np.array(all_quality)\n",
    "\n",
    "top_indices = np.argsort(all_disc)[-10:][::-1]\n",
    "\n",
    "print(\"Top 10 Discriminability 채널:\")\n",
    "print()\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    layer_name, ch_idx = all_layers[idx]\n",
    "    disc_val = all_disc[idx]\n",
    "    quality_val = all_quality[idx]\n",
    "    \n",
    "    # Gate 계산\n",
    "    sanitized_name = layer_name.replace('.', '_')\n",
    "    if sanitized_name in model.gates:\n",
    "        gate_val = model.gates[sanitized_name].static_gate[ch_idx].item()\n",
    "        status = \"PRUNED\" if gate_val == 0 else (\"SOFT\" if gate_val < 1.0 else \"KEEP\")\n",
    "    else:\n",
    "        gate_val = -1\n",
    "        status = \"N/A\"\n",
    "    \n",
    "    print(f\"#{rank}: {layer_name}[{ch_idx}]\")\n",
    "    print(f\"     Disc: {disc_val:.6f}, Quality: {quality_val:.4f}, Gate: {gate_val:.4f} ({status})\")\n",
    "    \n",
    "    if status == \"PRUNED\":\n",
    "        print(f\"     🚨 문제! Top discriminability 채널이 pruned됨!\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Dataset별, Corruption별로 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_progan-original\n",
      "샘플 수: 8000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_progan-original: 100%|██████████| 500/500 [01:39<00:00,  5.04it/s]\n",
      "/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 50.00%\n",
      "  AUC:      50.00%\n",
      "  AP:       50.00%\n",
      "  F1:       0.00%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_progan-gaussian_noise\n",
      "샘플 수: 8000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_progan-gaussian_noise: 100%|██████████| 500/500 [01:37<00:00,  5.10it/s]\n",
      "/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 50.00%\n",
      "  AUC:      50.00%\n",
      "  AP:       50.00%\n",
      "  F1:       0.00%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_progan-jpeg_compression\n",
      "샘플 수: 8000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_progan-jpeg_compression: 100%|██████████| 500/500 [01:37<00:00,  5.10it/s]\n",
      "/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 50.00%\n",
      "  AUC:      50.00%\n",
      "  AP:       50.00%\n",
      "  F1:       0.00%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_stylegan-original\n",
      "샘플 수: 11982\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_stylegan-original:   6%|▋         | 47/748 [00:09<02:23,  4.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     23\u001b[39m dataloader = DataLoader(\n\u001b[32m     24\u001b[39m     subset,\n\u001b[32m     25\u001b[39m     batch_size=\u001b[32m16\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 평가\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m metrics = \u001b[43mcalc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcorruption\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# 즉시 결과 출력\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m결과:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/utils/eval/metrics.py:164\u001b[39m, in \u001b[36mMetricsCalculator.evaluate\u001b[39m\u001b[34m(self, model, dataloader, device, name)\u001b[39m\n\u001b[32m    161\u001b[39m images.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Forward pass (model.eval() prevents parameter updates)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Handle different output formats\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outputs.min() < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m outputs.max() > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/method/channel_pruningv3.py:401\u001b[39m, in \u001b[36mUnifiedChannelPruningV3.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.model == \u001b[33m\"\u001b[39m\u001b[33mLGrad\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# NPR\u001b[39;00m\n\u001b[32m    403\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.model(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/LGrad/lgrad_model.py:152\u001b[39m, in \u001b[36mLGrad.forward\u001b[39m\u001b[34m(self, x, return_grad)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    x: Input images [B, 3, H, W], range [0, 1]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m \u001b[33;03m    grad (optional): [B, 3, 256, 256] gradient images\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    151\u001b[39m grad = \u001b[38;5;28mself\u001b[39m.img2grad(x)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_grad:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/LGrad/lgrad_model.py:134\u001b[39m, in \u001b[36mLGrad.classify\u001b[39m\u001b[34m(self, grad)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclassify\u001b[39m(\u001b[38;5;28mself\u001b[39m, grad: torch.Tensor) -> torch.Tensor:\n\u001b[32m    127\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03m    classification of Gradient image\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    132\u001b[39m \u001b[33;03m        Logits [B, 1] (positive = fake)\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrad2clf_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.classifier(x)\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:920\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    917\u001b[39m     tensor = tensor.clone()\n\u001b[32m    919\u001b[39m dtype = tensor.dtype\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m mean = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m std = torch.as_tensor(std, dtype=dtype, device=tensor.device)\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (std == \u001b[32m0\u001b[39m).any():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "calc = MetricsCalculator()\n",
    "all_results = {}\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    for corruption in CORRUPTIONS:\n",
    "        combination_indices = [\n",
    "            i for i, s in enumerate(dataset.samples)\n",
    "            if s['dataset'] == dataset_name and s['corruption'] == corruption\n",
    "        ]\n",
    "        \n",
    "        if len(combination_indices) == 0:\n",
    "            print(f\"{dataset_name}-{corruption}: 샘플 없음, 스킵\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"평가 중: {dataset_name}-{corruption}\")\n",
    "        print(f\"샘플 수: {len(combination_indices)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Subset과 DataLoader 생성\n",
    "        subset = Subset(dataset, combination_indices)\n",
    "        dataloader = DataLoader(\n",
    "            subset,\n",
    "            batch_size=16,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        # 평가\n",
    "        metrics = calc.evaluate(\n",
    "            model=model,\n",
    "            dataloader=dataloader,\n",
    "            device=DEVICE,\n",
    "            name=f\"{dataset_name}-{corruption}\"\n",
    "        )\n",
    "        \n",
    "        # 즉시 결과 출력\n",
    "        print(f\"\\n결과:\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "        print(f\"  AUC:      {metrics['auc']*100:.2f}%\")\n",
    "        print(f\"  AP:       {metrics['ap']*100:.2f}%\")\n",
    "        print(f\"  F1:       {metrics['f1']*100:.2f}%\")\n",
    "        \n",
    "        # 결과 저장\n",
    "        all_results[(dataset_name, corruption)] = metrics\n",
    "\n",
    "# 전체 결과 테이블 출력\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"전체 결과 요약\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "calc.print_results_table()\n",
    "calc.summarize_by_corruption(all_results)\n",
    "calc.summarize_by_dataset(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Test (No Gating)\n",
    "\n",
    "Gating 없이 모든 채널을 사용했을 때의 성능을 확인합니다.\n",
    "\n",
    "CPv3는 static gate를 사용하므로, gate를 1.0으로 설정하여 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gating 강제로 끄기 (모든 채널 사용)\n",
    "print(\"Disabling all gates (forcing gate=1.0)...\")\n",
    "\n",
    "for gate_module in model.gates.values():\n",
    "    # Static gate를 1.0으로 설정\n",
    "    gate_module.static_gate.data.fill_(1.0)\n",
    "\n",
    "print(\"All gates disabled. Re-evaluating...\")\n",
    "\n",
    "# ProGAN original 다시 평가\n",
    "progan_orig_indices = [i for i, s in enumerate(dataset.samples)\n",
    "                        if s['dataset'] == \"corrupted_test_data_progan\"\n",
    "                        and s['corruption'] == \"original\"]\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Subset(dataset, progan_orig_indices),\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "calc = MetricsCalculator()\n",
    "metrics = calc.evaluate(model, test_loader, DEVICE, \"no-gating-baseline\")\n",
    "\n",
    "print(f\"\\nBaseline (no gating):\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"  AUC:      {metrics['auc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gate Statistics\n",
    "\n",
    "각 레이어의 gating 통계를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gate Statistics (from initial configuration):\n",
      "======================================================================\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 21/64 (32.8%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 43/64 (67.2%)\n",
      "      Gate mean: 0.6719, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 36/64 (56.2%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 28/64 (43.8%)\n",
      "      Gate mean: 0.4375, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 5/64 (7.8%)\n",
      "      Medium quality (soft): 7/64 (10.9%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8806, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/64 (39.1%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 39/64 (60.9%)\n",
      "      Gate mean: 0.6094, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 4/64 (6.2%)\n",
      "      Medium quality (soft): 8/64 (12.5%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8689, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 27/64 (42.2%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 37/64 (57.8%)\n",
      "      Gate mean: 0.5781, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 39/256 (15.2%)\n",
      "      Medium quality (soft): 28/256 (10.9%)\n",
      "      High quality (keep): 189/256 (73.8%)\n",
      "      Gate mean: 0.8170, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 114/256 (44.5%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 142/256 (55.5%)\n",
      "      Gate mean: 0.5547, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 38/256 (14.8%)\n",
      "      Medium quality (soft): 35/256 (13.7%)\n",
      "      High quality (keep): 183/256 (71.5%)\n",
      "      Gate mean: 0.7988, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 58/256 (22.7%)\n",
      "      Medium quality (soft): 18/256 (7.0%)\n",
      "      High quality (keep): 180/256 (70.3%)\n",
      "      Gate mean: 0.7565, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 5/64 (7.8%)\n",
      "      Medium quality (soft): 7/64 (10.9%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8953, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 17/64 (26.6%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 47/64 (73.4%)\n",
      "      Gate mean: 0.7344, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 3/64 (4.7%)\n",
      "      Medium quality (soft): 9/64 (14.1%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8951, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 33/64 (51.6%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 31/64 (48.4%)\n",
      "      Gate mean: 0.4844, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 21/256 (8.2%)\n",
      "      Medium quality (soft): 36/256 (14.1%)\n",
      "      High quality (keep): 199/256 (77.7%)\n",
      "      Gate mean: 0.8730, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 89/256 (34.8%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 167/256 (65.2%)\n",
      "      Gate mean: 0.6523, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 3/64 (4.7%)\n",
      "      Medium quality (soft): 9/64 (14.1%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8810, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 21/64 (32.8%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 43/64 (67.2%)\n",
      "      Gate mean: 0.6719, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 3/64 (4.7%)\n",
      "      Medium quality (soft): 9/64 (14.1%)\n",
      "      High quality (keep): 52/64 (81.2%)\n",
      "      Gate mean: 0.8915, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 17/64 (26.6%)\n",
      "      Medium quality (soft): 0/64 (0.0%)\n",
      "      High quality (keep): 47/64 (73.4%)\n",
      "      Gate mean: 0.7344, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 16/256 (6.2%)\n",
      "      Medium quality (soft): 35/256 (13.7%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8884, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 89/256 (34.8%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 167/256 (65.2%)\n",
      "      Gate mean: 0.6523, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.8899, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 59/128 (46.1%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 69/128 (53.9%)\n",
      "      Gate mean: 0.5391, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.9148, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 36/128 (28.1%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 92/128 (71.9%)\n",
      "      Gate mean: 0.7188, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 75/512 (14.6%)\n",
      "      Medium quality (soft): 56/512 (10.9%)\n",
      "      High quality (keep): 381/512 (74.4%)\n",
      "      Gate mean: 0.8151, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 243/512 (47.5%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 269/512 (52.5%)\n",
      "      Gate mean: 0.5254, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 82/512 (16.0%)\n",
      "      Medium quality (soft): 53/512 (10.4%)\n",
      "      High quality (keep): 377/512 (73.6%)\n",
      "      Gate mean: 0.8010, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 177/512 (34.6%)\n",
      "      Medium quality (soft): 15/512 (2.9%)\n",
      "      High quality (keep): 320/512 (62.5%)\n",
      "      Gate mean: 0.6441, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.9017, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 66/128 (51.6%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 62/128 (48.4%)\n",
      "      Gate mean: 0.4844, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.8997, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 66/128 (51.6%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 62/128 (48.4%)\n",
      "      Gate mean: 0.4844, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 41/512 (8.0%)\n",
      "      Medium quality (soft): 68/512 (13.3%)\n",
      "      High quality (keep): 403/512 (78.7%)\n",
      "      Gate mean: 0.8714, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 188/512 (36.7%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 324/512 (63.3%)\n",
      "      Gate mean: 0.6328, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.9047, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 37/128 (28.9%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 91/128 (71.1%)\n",
      "      Gate mean: 0.7109, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.8976, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 55/128 (43.0%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 73/128 (57.0%)\n",
      "      Gate mean: 0.5703, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8927, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 155/512 (30.3%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 357/512 (69.7%)\n",
      "      Gate mean: 0.6973, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.8961, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 31/128 (24.2%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 97/128 (75.8%)\n",
      "      Gate mean: 0.7578, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 6/128 (4.7%)\n",
      "      Medium quality (soft): 19/128 (14.8%)\n",
      "      High quality (keep): 103/128 (80.5%)\n",
      "      Gate mean: 0.9002, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 47/128 (36.7%)\n",
      "      Medium quality (soft): 0/128 (0.0%)\n",
      "      High quality (keep): 81/128 (63.3%)\n",
      "      Gate mean: 0.6328, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.9003, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 181/512 (35.4%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 331/512 (64.6%)\n",
      "      Gate mean: 0.6465, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.9007, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 122/256 (47.7%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 134/256 (52.3%)\n",
      "      Gate mean: 0.5234, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8990, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 107/256 (41.8%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 149/256 (58.2%)\n",
      "      Gate mean: 0.5820, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 78/1024 (7.6%)\n",
      "      Medium quality (soft): 137/1024 (13.4%)\n",
      "      High quality (keep): 809/1024 (79.0%)\n",
      "      Gate mean: 0.8802, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 498/1024 (48.6%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 526/1024 (51.4%)\n",
      "      Gate mean: 0.5137, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 81/1024 (7.9%)\n",
      "      Medium quality (soft): 138/1024 (13.5%)\n",
      "      High quality (keep): 805/1024 (78.6%)\n",
      "      Gate mean: 0.8737, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 206/1024 (20.1%)\n",
      "      Medium quality (soft): 73/1024 (7.1%)\n",
      "      High quality (keep): 745/1024 (72.8%)\n",
      "      Gate mean: 0.7799, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.9073, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 132/256 (51.6%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 124/256 (48.4%)\n",
      "      Gate mean: 0.4844, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8879, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 127/256 (49.6%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 129/256 (50.4%)\n",
      "      Gate mean: 0.5039, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 55/1024 (5.4%)\n",
      "      Medium quality (soft): 153/1024 (14.9%)\n",
      "      High quality (keep): 816/1024 (79.7%)\n",
      "      Gate mean: 0.8920, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 455/1024 (44.4%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 569/1024 (55.6%)\n",
      "      Gate mean: 0.5557, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.9050, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 138/256 (53.9%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 118/256 (46.1%)\n",
      "      Gate mean: 0.4609, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8922, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 151/256 (59.0%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 105/256 (41.0%)\n",
      "      Gate mean: 0.4102, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 51/1024 (5.0%)\n",
      "      Medium quality (soft): 153/1024 (14.9%)\n",
      "      High quality (keep): 820/1024 (80.1%)\n",
      "      Gate mean: 0.8976, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 338/1024 (33.0%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 686/1024 (67.0%)\n",
      "      Gate mean: 0.6699, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8991, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 181/256 (70.7%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 75/256 (29.3%)\n",
      "      Gate mean: 0.2930, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8900, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 175/256 (68.4%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 81/256 (31.6%)\n",
      "      Gate mean: 0.3164, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 51/1024 (5.0%)\n",
      "      Medium quality (soft): 153/1024 (14.9%)\n",
      "      High quality (keep): 820/1024 (80.1%)\n",
      "      Gate mean: 0.8967, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 301/1024 (29.4%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 723/1024 (70.6%)\n",
      "      Gate mean: 0.7061, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8986, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 129/256 (50.4%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 127/256 (49.6%)\n",
      "      Gate mean: 0.4961, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.9037, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 195/256 (76.2%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 61/256 (23.8%)\n",
      "      Gate mean: 0.2383, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 53/1024 (5.2%)\n",
      "      Medium quality (soft): 151/1024 (14.7%)\n",
      "      High quality (keep): 820/1024 (80.1%)\n",
      "      Gate mean: 0.8982, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 352/1024 (34.4%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 672/1024 (65.6%)\n",
      "      Gate mean: 0.6562, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.8928, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 168/256 (65.6%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 88/256 (34.4%)\n",
      "      Gate mean: 0.3438, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 12/256 (4.7%)\n",
      "      Medium quality (soft): 39/256 (15.2%)\n",
      "      High quality (keep): 205/256 (80.1%)\n",
      "      Gate mean: 0.9031, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 170/256 (66.4%)\n",
      "      Medium quality (soft): 0/256 (0.0%)\n",
      "      High quality (keep): 86/256 (33.6%)\n",
      "      Gate mean: 0.3359, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 51/1024 (5.0%)\n",
      "      Medium quality (soft): 153/1024 (14.9%)\n",
      "      High quality (keep): 820/1024 (80.1%)\n",
      "      Gate mean: 0.8991, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 465/1024 (45.4%)\n",
      "      Medium quality (soft): 0/1024 (0.0%)\n",
      "      High quality (keep): 559/1024 (54.6%)\n",
      "      Gate mean: 0.5459, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8947, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 332/512 (64.8%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 180/512 (35.2%)\n",
      "      Gate mean: 0.3516, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8936, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 331/512 (64.6%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 181/512 (35.4%)\n",
      "      Gate mean: 0.3535, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 102/2048 (5.0%)\n",
      "      Medium quality (soft): 307/2048 (15.0%)\n",
      "      High quality (keep): 1639/2048 (80.0%)\n",
      "      Gate mean: 0.8983, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 739/2048 (36.1%)\n",
      "      Medium quality (soft): 0/2048 (0.0%)\n",
      "      High quality (keep): 1309/2048 (63.9%)\n",
      "      Gate mean: 0.6392, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 102/2048 (5.0%)\n",
      "      Medium quality (soft): 307/2048 (15.0%)\n",
      "      High quality (keep): 1639/2048 (80.0%)\n",
      "      Gate mean: 0.8949, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 106/2048 (5.2%)\n",
      "      Medium quality (soft): 307/2048 (15.0%)\n",
      "      High quality (keep): 1635/2048 (79.8%)\n",
      "      Gate mean: 0.8929, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8962, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 452/512 (88.3%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 60/512 (11.7%)\n",
      "      Gate mean: 0.1172, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.9161, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 355/512 (69.3%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 157/512 (30.7%)\n",
      "      Gate mean: 0.3066, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 102/2048 (5.0%)\n",
      "      Medium quality (soft): 307/2048 (15.0%)\n",
      "      High quality (keep): 1639/2048 (80.0%)\n",
      "      Gate mean: 0.8996, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 467/2048 (22.8%)\n",
      "      Medium quality (soft): 1/2048 (0.0%)\n",
      "      High quality (keep): 1580/2048 (77.1%)\n",
      "      Gate mean: 0.7718, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8943, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 474/512 (92.6%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 38/512 (7.4%)\n",
      "      Gate mean: 0.0742, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 25/512 (4.9%)\n",
      "      Medium quality (soft): 77/512 (15.0%)\n",
      "      High quality (keep): 410/512 (80.1%)\n",
      "      Gate mean: 0.8968, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 376/512 (73.4%)\n",
      "      Medium quality (soft): 0/512 (0.0%)\n",
      "      High quality (keep): 136/512 (26.6%)\n",
      "      Gate mean: 0.2656, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 102/2048 (5.0%)\n",
      "      Medium quality (soft): 307/2048 (15.0%)\n",
      "      High quality (keep): 1639/2048 (80.0%)\n",
      "      Gate mean: 0.9055, min: 0.0000, max: 1.0000\n",
      "    Gate distribution:\n",
      "      Low quality (pruned): 219/2048 (10.7%)\n",
      "      Medium quality (soft): 194/2048 (9.5%)\n",
      "      High quality (keep): 1635/2048 (79.8%)\n",
      "      Gate mean: 0.8660, min: 0.0000, max: 1.0000\n",
      "[CPv3] Initialized for LGrad\n",
      "[CPv3] Target layers: 106\n",
      "[CPv3] Low quality threshold: 5.0%\n",
      "[CPv3] Medium quality threshold: 20.0%\n",
      "\n",
      "classifier.conv1:\n",
      "  Gate mean: 0.6719\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 21/64 (32.8%)\n",
      "  Soft (0<gate<1): 0/64 (0.0%)\n",
      "  Keep (gate=1): 43/64 (67.2%)\n",
      "\n",
      "classifier.bn1:\n",
      "  Gate mean: 0.4375\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 36/64 (56.2%)\n",
      "  Soft (0<gate<1): 0/64 (0.0%)\n",
      "  Keep (gate=1): 28/64 (43.8%)\n",
      "\n",
      "classifier.layer1.0.conv1:\n",
      "  Gate mean: 0.8806\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 5/64 (7.8%)\n",
      "  Soft (0<gate<1): 7/64 (10.9%)\n",
      "  Keep (gate=1): 52/64 (81.2%)\n",
      "\n",
      "classifier.layer1.0.bn1:\n",
      "  Gate mean: 0.6094\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 25/64 (39.1%)\n",
      "  Soft (0<gate<1): 0/64 (0.0%)\n",
      "  Keep (gate=1): 39/64 (60.9%)\n",
      "\n",
      "classifier.layer1.0.conv2:\n",
      "  Gate mean: 0.8689\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 4/64 (6.2%)\n",
      "  Soft (0<gate<1): 8/64 (12.5%)\n",
      "  Keep (gate=1): 52/64 (81.2%)\n",
      "\n",
      "classifier.layer1.0.bn2:\n",
      "  Gate mean: 0.5781\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 27/64 (42.2%)\n",
      "  Soft (0<gate<1): 0/64 (0.0%)\n",
      "  Keep (gate=1): 37/64 (57.8%)\n",
      "\n",
      "classifier.layer1.0.conv3:\n",
      "  Gate mean: 0.8170\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 39/256 (15.2%)\n",
      "  Soft (0<gate<1): 28/256 (10.9%)\n",
      "  Keep (gate=1): 189/256 (73.8%)\n",
      "\n",
      "classifier.layer1.0.bn3:\n",
      "  Gate mean: 0.5547\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 114/256 (44.5%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 142/256 (55.5%)\n",
      "\n",
      "classifier.layer1.0.downsample.0:\n",
      "  Gate mean: 0.7988\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 38/256 (14.8%)\n",
      "  Soft (0<gate<1): 35/256 (13.7%)\n",
      "  Keep (gate=1): 183/256 (71.5%)\n",
      "\n",
      "classifier.layer1.0.downsample.1:\n",
      "  Gate mean: 0.7565\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 58/256 (22.7%)\n",
      "  Soft (0<gate<1): 18/256 (7.0%)\n",
      "  Keep (gate=1): 180/256 (70.3%)\n",
      "\n",
      "classifier.layer1.1.conv1:\n",
      "  Gate mean: 0.8953\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 5/64 (7.8%)\n",
      "  Soft (0<gate<1): 7/64 (10.9%)\n",
      "  Keep (gate=1): 52/64 (81.2%)\n",
      "\n",
      "classifier.layer1.1.bn1:\n",
      "  Gate mean: 0.7344\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 17/64 (26.6%)\n",
      "  Soft (0<gate<1): 0/64 (0.0%)\n",
      "  Keep (gate=1): 47/64 (73.4%)\n",
      "\n",
      "classifier.layer1.1.conv2:\n",
      "  Gate mean: 0.8951\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 3/64 (4.7%)\n",
      "  Soft (0<gate<1): 9/64 (14.1%)\n",
      "  Keep (gate=1): 52/64 (81.2%)\n",
      "\n",
      "classifier.layer1.1.bn2:\n",
      "  Gate mean: 0.4844\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 33/64 (51.6%)\n",
      "  Soft (0<gate<1): 0/64 (0.0%)\n",
      "  Keep (gate=1): 31/64 (48.4%)\n",
      "\n",
      "classifier.layer1.1.conv3:\n",
      "  Gate mean: 0.8730\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 21/256 (8.2%)\n",
      "  Soft (0<gate<1): 36/256 (14.1%)\n",
      "  Keep (gate=1): 199/256 (77.7%)\n",
      "\n",
      "classifier.layer1.1.bn3:\n",
      "  Gate mean: 0.6523\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 89/256 (34.8%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 167/256 (65.2%)\n",
      "\n",
      "classifier.layer1.2.conv1:\n",
      "  Gate mean: 0.8810\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 3/64 (4.7%)\n",
      "  Soft (0<gate<1): 9/64 (14.1%)\n",
      "  Keep (gate=1): 52/64 (81.2%)\n",
      "\n",
      "classifier.layer1.2.bn1:\n",
      "  Gate mean: 0.6719\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 21/64 (32.8%)\n",
      "  Soft (0<gate<1): 0/64 (0.0%)\n",
      "  Keep (gate=1): 43/64 (67.2%)\n",
      "\n",
      "classifier.layer1.2.conv2:\n",
      "  Gate mean: 0.8915\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 3/64 (4.7%)\n",
      "  Soft (0<gate<1): 9/64 (14.1%)\n",
      "  Keep (gate=1): 52/64 (81.2%)\n",
      "\n",
      "classifier.layer1.2.bn2:\n",
      "  Gate mean: 0.7344\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 17/64 (26.6%)\n",
      "  Soft (0<gate<1): 0/64 (0.0%)\n",
      "  Keep (gate=1): 47/64 (73.4%)\n",
      "\n",
      "classifier.layer1.2.conv3:\n",
      "  Gate mean: 0.8884\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 16/256 (6.2%)\n",
      "  Soft (0<gate<1): 35/256 (13.7%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer1.2.bn3:\n",
      "  Gate mean: 0.6523\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 89/256 (34.8%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 167/256 (65.2%)\n",
      "\n",
      "classifier.layer2.0.conv1:\n",
      "  Gate mean: 0.8899\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 6/128 (4.7%)\n",
      "  Soft (0<gate<1): 19/128 (14.8%)\n",
      "  Keep (gate=1): 103/128 (80.5%)\n",
      "\n",
      "classifier.layer2.0.bn1:\n",
      "  Gate mean: 0.5391\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 59/128 (46.1%)\n",
      "  Soft (0<gate<1): 0/128 (0.0%)\n",
      "  Keep (gate=1): 69/128 (53.9%)\n",
      "\n",
      "classifier.layer2.0.conv2:\n",
      "  Gate mean: 0.9148\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 6/128 (4.7%)\n",
      "  Soft (0<gate<1): 19/128 (14.8%)\n",
      "  Keep (gate=1): 103/128 (80.5%)\n",
      "\n",
      "classifier.layer2.0.bn2:\n",
      "  Gate mean: 0.7188\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 36/128 (28.1%)\n",
      "  Soft (0<gate<1): 0/128 (0.0%)\n",
      "  Keep (gate=1): 92/128 (71.9%)\n",
      "\n",
      "classifier.layer2.0.conv3:\n",
      "  Gate mean: 0.8151\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 75/512 (14.6%)\n",
      "  Soft (0<gate<1): 56/512 (10.9%)\n",
      "  Keep (gate=1): 381/512 (74.4%)\n",
      "\n",
      "classifier.layer2.0.bn3:\n",
      "  Gate mean: 0.5254\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 243/512 (47.5%)\n",
      "  Soft (0<gate<1): 0/512 (0.0%)\n",
      "  Keep (gate=1): 269/512 (52.5%)\n",
      "\n",
      "classifier.layer2.0.downsample.0:\n",
      "  Gate mean: 0.8010\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 82/512 (16.0%)\n",
      "  Soft (0<gate<1): 53/512 (10.4%)\n",
      "  Keep (gate=1): 377/512 (73.6%)\n",
      "\n",
      "classifier.layer2.0.downsample.1:\n",
      "  Gate mean: 0.6441\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 177/512 (34.6%)\n",
      "  Soft (0<gate<1): 15/512 (2.9%)\n",
      "  Keep (gate=1): 320/512 (62.5%)\n",
      "\n",
      "classifier.layer2.1.conv1:\n",
      "  Gate mean: 0.9017\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 6/128 (4.7%)\n",
      "  Soft (0<gate<1): 19/128 (14.8%)\n",
      "  Keep (gate=1): 103/128 (80.5%)\n",
      "\n",
      "classifier.layer2.1.bn1:\n",
      "  Gate mean: 0.4844\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 66/128 (51.6%)\n",
      "  Soft (0<gate<1): 0/128 (0.0%)\n",
      "  Keep (gate=1): 62/128 (48.4%)\n",
      "\n",
      "classifier.layer2.1.conv2:\n",
      "  Gate mean: 0.8997\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 6/128 (4.7%)\n",
      "  Soft (0<gate<1): 19/128 (14.8%)\n",
      "  Keep (gate=1): 103/128 (80.5%)\n",
      "\n",
      "classifier.layer2.1.bn2:\n",
      "  Gate mean: 0.4844\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 66/128 (51.6%)\n",
      "  Soft (0<gate<1): 0/128 (0.0%)\n",
      "  Keep (gate=1): 62/128 (48.4%)\n",
      "\n",
      "classifier.layer2.1.conv3:\n",
      "  Gate mean: 0.8714\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 41/512 (8.0%)\n",
      "  Soft (0<gate<1): 68/512 (13.3%)\n",
      "  Keep (gate=1): 403/512 (78.7%)\n",
      "\n",
      "classifier.layer2.1.bn3:\n",
      "  Gate mean: 0.6328\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 188/512 (36.7%)\n",
      "  Soft (0<gate<1): 0/512 (0.0%)\n",
      "  Keep (gate=1): 324/512 (63.3%)\n",
      "\n",
      "classifier.layer2.2.conv1:\n",
      "  Gate mean: 0.9047\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 6/128 (4.7%)\n",
      "  Soft (0<gate<1): 19/128 (14.8%)\n",
      "  Keep (gate=1): 103/128 (80.5%)\n",
      "\n",
      "classifier.layer2.2.bn1:\n",
      "  Gate mean: 0.7109\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 37/128 (28.9%)\n",
      "  Soft (0<gate<1): 0/128 (0.0%)\n",
      "  Keep (gate=1): 91/128 (71.1%)\n",
      "\n",
      "classifier.layer2.2.conv2:\n",
      "  Gate mean: 0.8976\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 6/128 (4.7%)\n",
      "  Soft (0<gate<1): 19/128 (14.8%)\n",
      "  Keep (gate=1): 103/128 (80.5%)\n",
      "\n",
      "classifier.layer2.2.bn2:\n",
      "  Gate mean: 0.5703\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 55/128 (43.0%)\n",
      "  Soft (0<gate<1): 0/128 (0.0%)\n",
      "  Keep (gate=1): 73/128 (57.0%)\n",
      "\n",
      "classifier.layer2.2.conv3:\n",
      "  Gate mean: 0.8927\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 25/512 (4.9%)\n",
      "  Soft (0<gate<1): 77/512 (15.0%)\n",
      "  Keep (gate=1): 410/512 (80.1%)\n",
      "\n",
      "classifier.layer2.2.bn3:\n",
      "  Gate mean: 0.6973\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 155/512 (30.3%)\n",
      "  Soft (0<gate<1): 0/512 (0.0%)\n",
      "  Keep (gate=1): 357/512 (69.7%)\n",
      "\n",
      "classifier.layer2.3.conv1:\n",
      "  Gate mean: 0.8961\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 6/128 (4.7%)\n",
      "  Soft (0<gate<1): 19/128 (14.8%)\n",
      "  Keep (gate=1): 103/128 (80.5%)\n",
      "\n",
      "classifier.layer2.3.bn1:\n",
      "  Gate mean: 0.7578\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 31/128 (24.2%)\n",
      "  Soft (0<gate<1): 0/128 (0.0%)\n",
      "  Keep (gate=1): 97/128 (75.8%)\n",
      "\n",
      "classifier.layer2.3.conv2:\n",
      "  Gate mean: 0.9002\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 6/128 (4.7%)\n",
      "  Soft (0<gate<1): 19/128 (14.8%)\n",
      "  Keep (gate=1): 103/128 (80.5%)\n",
      "\n",
      "classifier.layer2.3.bn2:\n",
      "  Gate mean: 0.6328\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 47/128 (36.7%)\n",
      "  Soft (0<gate<1): 0/128 (0.0%)\n",
      "  Keep (gate=1): 81/128 (63.3%)\n",
      "\n",
      "classifier.layer2.3.conv3:\n",
      "  Gate mean: 0.9003\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 25/512 (4.9%)\n",
      "  Soft (0<gate<1): 77/512 (15.0%)\n",
      "  Keep (gate=1): 410/512 (80.1%)\n",
      "\n",
      "classifier.layer2.3.bn3:\n",
      "  Gate mean: 0.6465\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 181/512 (35.4%)\n",
      "  Soft (0<gate<1): 0/512 (0.0%)\n",
      "  Keep (gate=1): 331/512 (64.6%)\n",
      "\n",
      "classifier.layer3.0.conv1:\n",
      "  Gate mean: 0.9007\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.0.bn1:\n",
      "  Gate mean: 0.5234\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 122/256 (47.7%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 134/256 (52.3%)\n",
      "\n",
      "classifier.layer3.0.conv2:\n",
      "  Gate mean: 0.8990\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.0.bn2:\n",
      "  Gate mean: 0.5820\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 107/256 (41.8%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 149/256 (58.2%)\n",
      "\n",
      "classifier.layer3.0.conv3:\n",
      "  Gate mean: 0.8802\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 78/1024 (7.6%)\n",
      "  Soft (0<gate<1): 137/1024 (13.4%)\n",
      "  Keep (gate=1): 809/1024 (79.0%)\n",
      "\n",
      "classifier.layer3.0.bn3:\n",
      "  Gate mean: 0.5137\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 498/1024 (48.6%)\n",
      "  Soft (0<gate<1): 0/1024 (0.0%)\n",
      "  Keep (gate=1): 526/1024 (51.4%)\n",
      "\n",
      "classifier.layer3.0.downsample.0:\n",
      "  Gate mean: 0.8737\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 81/1024 (7.9%)\n",
      "  Soft (0<gate<1): 138/1024 (13.5%)\n",
      "  Keep (gate=1): 805/1024 (78.6%)\n",
      "\n",
      "classifier.layer3.0.downsample.1:\n",
      "  Gate mean: 0.7799\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 206/1024 (20.1%)\n",
      "  Soft (0<gate<1): 73/1024 (7.1%)\n",
      "  Keep (gate=1): 745/1024 (72.8%)\n",
      "\n",
      "classifier.layer3.1.conv1:\n",
      "  Gate mean: 0.9073\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.1.bn1:\n",
      "  Gate mean: 0.4844\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 132/256 (51.6%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 124/256 (48.4%)\n",
      "\n",
      "classifier.layer3.1.conv2:\n",
      "  Gate mean: 0.8879\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.1.bn2:\n",
      "  Gate mean: 0.5039\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 127/256 (49.6%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 129/256 (50.4%)\n",
      "\n",
      "classifier.layer3.1.conv3:\n",
      "  Gate mean: 0.8920\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 55/1024 (5.4%)\n",
      "  Soft (0<gate<1): 153/1024 (14.9%)\n",
      "  Keep (gate=1): 816/1024 (79.7%)\n",
      "\n",
      "classifier.layer3.1.bn3:\n",
      "  Gate mean: 0.5557\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 455/1024 (44.4%)\n",
      "  Soft (0<gate<1): 0/1024 (0.0%)\n",
      "  Keep (gate=1): 569/1024 (55.6%)\n",
      "\n",
      "classifier.layer3.2.conv1:\n",
      "  Gate mean: 0.9050\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.2.bn1:\n",
      "  Gate mean: 0.4609\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 138/256 (53.9%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 118/256 (46.1%)\n",
      "\n",
      "classifier.layer3.2.conv2:\n",
      "  Gate mean: 0.8922\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.2.bn2:\n",
      "  Gate mean: 0.4102\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 151/256 (59.0%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 105/256 (41.0%)\n",
      "\n",
      "classifier.layer3.2.conv3:\n",
      "  Gate mean: 0.8976\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 51/1024 (5.0%)\n",
      "  Soft (0<gate<1): 153/1024 (14.9%)\n",
      "  Keep (gate=1): 820/1024 (80.1%)\n",
      "\n",
      "classifier.layer3.2.bn3:\n",
      "  Gate mean: 0.6699\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 338/1024 (33.0%)\n",
      "  Soft (0<gate<1): 0/1024 (0.0%)\n",
      "  Keep (gate=1): 686/1024 (67.0%)\n",
      "\n",
      "classifier.layer3.3.conv1:\n",
      "  Gate mean: 0.8991\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.3.bn1:\n",
      "  Gate mean: 0.2930\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 181/256 (70.7%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 75/256 (29.3%)\n",
      "\n",
      "classifier.layer3.3.conv2:\n",
      "  Gate mean: 0.8900\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.3.bn2:\n",
      "  Gate mean: 0.3164\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 175/256 (68.4%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 81/256 (31.6%)\n",
      "\n",
      "classifier.layer3.3.conv3:\n",
      "  Gate mean: 0.8967\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 51/1024 (5.0%)\n",
      "  Soft (0<gate<1): 153/1024 (14.9%)\n",
      "  Keep (gate=1): 820/1024 (80.1%)\n",
      "\n",
      "classifier.layer3.3.bn3:\n",
      "  Gate mean: 0.7061\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 301/1024 (29.4%)\n",
      "  Soft (0<gate<1): 0/1024 (0.0%)\n",
      "  Keep (gate=1): 723/1024 (70.6%)\n",
      "\n",
      "classifier.layer3.4.conv1:\n",
      "  Gate mean: 0.8986\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.4.bn1:\n",
      "  Gate mean: 0.4961\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 129/256 (50.4%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 127/256 (49.6%)\n",
      "\n",
      "classifier.layer3.4.conv2:\n",
      "  Gate mean: 0.9037\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.4.bn2:\n",
      "  Gate mean: 0.2383\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 195/256 (76.2%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 61/256 (23.8%)\n",
      "\n",
      "classifier.layer3.4.conv3:\n",
      "  Gate mean: 0.8982\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 53/1024 (5.2%)\n",
      "  Soft (0<gate<1): 151/1024 (14.7%)\n",
      "  Keep (gate=1): 820/1024 (80.1%)\n",
      "\n",
      "classifier.layer3.4.bn3:\n",
      "  Gate mean: 0.6562\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 352/1024 (34.4%)\n",
      "  Soft (0<gate<1): 0/1024 (0.0%)\n",
      "  Keep (gate=1): 672/1024 (65.6%)\n",
      "\n",
      "classifier.layer3.5.conv1:\n",
      "  Gate mean: 0.8928\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.5.bn1:\n",
      "  Gate mean: 0.3438\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 168/256 (65.6%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 88/256 (34.4%)\n",
      "\n",
      "classifier.layer3.5.conv2:\n",
      "  Gate mean: 0.9031\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 12/256 (4.7%)\n",
      "  Soft (0<gate<1): 39/256 (15.2%)\n",
      "  Keep (gate=1): 205/256 (80.1%)\n",
      "\n",
      "classifier.layer3.5.bn2:\n",
      "  Gate mean: 0.3359\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 170/256 (66.4%)\n",
      "  Soft (0<gate<1): 0/256 (0.0%)\n",
      "  Keep (gate=1): 86/256 (33.6%)\n",
      "\n",
      "classifier.layer3.5.conv3:\n",
      "  Gate mean: 0.8991\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 51/1024 (5.0%)\n",
      "  Soft (0<gate<1): 153/1024 (14.9%)\n",
      "  Keep (gate=1): 820/1024 (80.1%)\n",
      "\n",
      "classifier.layer3.5.bn3:\n",
      "  Gate mean: 0.5459\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 465/1024 (45.4%)\n",
      "  Soft (0<gate<1): 0/1024 (0.0%)\n",
      "  Keep (gate=1): 559/1024 (54.6%)\n",
      "\n",
      "classifier.layer4.0.conv1:\n",
      "  Gate mean: 0.8947\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 25/512 (4.9%)\n",
      "  Soft (0<gate<1): 77/512 (15.0%)\n",
      "  Keep (gate=1): 410/512 (80.1%)\n",
      "\n",
      "classifier.layer4.0.bn1:\n",
      "  Gate mean: 0.3516\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 332/512 (64.8%)\n",
      "  Soft (0<gate<1): 0/512 (0.0%)\n",
      "  Keep (gate=1): 180/512 (35.2%)\n",
      "\n",
      "classifier.layer4.0.conv2:\n",
      "  Gate mean: 0.8936\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 25/512 (4.9%)\n",
      "  Soft (0<gate<1): 77/512 (15.0%)\n",
      "  Keep (gate=1): 410/512 (80.1%)\n",
      "\n",
      "classifier.layer4.0.bn2:\n",
      "  Gate mean: 0.3535\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 331/512 (64.6%)\n",
      "  Soft (0<gate<1): 0/512 (0.0%)\n",
      "  Keep (gate=1): 181/512 (35.4%)\n",
      "\n",
      "classifier.layer4.0.conv3:\n",
      "  Gate mean: 0.8983\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 102/2048 (5.0%)\n",
      "  Soft (0<gate<1): 307/2048 (15.0%)\n",
      "  Keep (gate=1): 1639/2048 (80.0%)\n",
      "\n",
      "classifier.layer4.0.bn3:\n",
      "  Gate mean: 0.6392\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 739/2048 (36.1%)\n",
      "  Soft (0<gate<1): 0/2048 (0.0%)\n",
      "  Keep (gate=1): 1309/2048 (63.9%)\n",
      "\n",
      "classifier.layer4.0.downsample.0:\n",
      "  Gate mean: 0.8949\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 102/2048 (5.0%)\n",
      "  Soft (0<gate<1): 307/2048 (15.0%)\n",
      "  Keep (gate=1): 1639/2048 (80.0%)\n",
      "\n",
      "classifier.layer4.0.downsample.1:\n",
      "  Gate mean: 0.8929\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 106/2048 (5.2%)\n",
      "  Soft (0<gate<1): 307/2048 (15.0%)\n",
      "  Keep (gate=1): 1635/2048 (79.8%)\n",
      "\n",
      "classifier.layer4.1.conv1:\n",
      "  Gate mean: 0.8962\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 25/512 (4.9%)\n",
      "  Soft (0<gate<1): 77/512 (15.0%)\n",
      "  Keep (gate=1): 410/512 (80.1%)\n",
      "\n",
      "classifier.layer4.1.bn1:\n",
      "  Gate mean: 0.1172\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 452/512 (88.3%)\n",
      "  Soft (0<gate<1): 0/512 (0.0%)\n",
      "  Keep (gate=1): 60/512 (11.7%)\n",
      "\n",
      "classifier.layer4.1.conv2:\n",
      "  Gate mean: 0.9161\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 25/512 (4.9%)\n",
      "  Soft (0<gate<1): 77/512 (15.0%)\n",
      "  Keep (gate=1): 410/512 (80.1%)\n",
      "\n",
      "classifier.layer4.1.bn2:\n",
      "  Gate mean: 0.3066\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 355/512 (69.3%)\n",
      "  Soft (0<gate<1): 0/512 (0.0%)\n",
      "  Keep (gate=1): 157/512 (30.7%)\n",
      "\n",
      "classifier.layer4.1.conv3:\n",
      "  Gate mean: 0.8996\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 102/2048 (5.0%)\n",
      "  Soft (0<gate<1): 307/2048 (15.0%)\n",
      "  Keep (gate=1): 1639/2048 (80.0%)\n",
      "\n",
      "classifier.layer4.1.bn3:\n",
      "  Gate mean: 0.7718\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 467/2048 (22.8%)\n",
      "  Soft (0<gate<1): 1/2048 (0.0%)\n",
      "  Keep (gate=1): 1580/2048 (77.1%)\n",
      "\n",
      "classifier.layer4.2.conv1:\n",
      "  Gate mean: 0.8943\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 25/512 (4.9%)\n",
      "  Soft (0<gate<1): 77/512 (15.0%)\n",
      "  Keep (gate=1): 410/512 (80.1%)\n",
      "\n",
      "classifier.layer4.2.bn1:\n",
      "  Gate mean: 0.0742\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 474/512 (92.6%)\n",
      "  Soft (0<gate<1): 0/512 (0.0%)\n",
      "  Keep (gate=1): 38/512 (7.4%)\n",
      "\n",
      "classifier.layer4.2.conv2:\n",
      "  Gate mean: 0.8968\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 25/512 (4.9%)\n",
      "  Soft (0<gate<1): 77/512 (15.0%)\n",
      "  Keep (gate=1): 410/512 (80.1%)\n",
      "\n",
      "classifier.layer4.2.bn2:\n",
      "  Gate mean: 0.2656\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 376/512 (73.4%)\n",
      "  Soft (0<gate<1): 0/512 (0.0%)\n",
      "  Keep (gate=1): 136/512 (26.6%)\n",
      "\n",
      "classifier.layer4.2.conv3:\n",
      "  Gate mean: 0.9055\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 102/2048 (5.0%)\n",
      "  Soft (0<gate<1): 307/2048 (15.0%)\n",
      "  Keep (gate=1): 1639/2048 (80.0%)\n",
      "\n",
      "classifier.layer4.2.bn3:\n",
      "  Gate mean: 0.8660\n",
      "  Gate range: [0.0000, 1.0000]\n",
      "  Pruned (gate=0): 219/2048 (10.7%)\n",
      "  Soft (0<gate<1): 194/2048 (9.5%)\n",
      "  Keep (gate=1): 1635/2048 (79.8%)\n"
     ]
    }
   ],
   "source": [
    "# Gate statistics 확인 (no-gating test 전에 복원 필요)\n",
    "# 모델을 다시 생성하거나, 저장된 quality로 gate를 재계산\n",
    "print(\"\\nGate Statistics (from initial configuration):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 모델 재생성\n",
    "model_for_stats = UnifiedChannelPruningV3(\n",
    "    base_model=base_model,\n",
    "    quality_stats=quality_stats,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "gate_stats = model_for_stats.get_gate_statistics()\n",
    "\n",
    "for layer_name, stats in gate_stats.items():\n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    print(f\"  Gate mean: {stats['mean']:.4f}\")\n",
    "    print(f\"  Gate range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "    total = stats['num_pruned'] + stats['num_soft'] + stats['num_keep']\n",
    "    print(f\"  Pruned (gate=0): {stats['num_pruned']}/{total} ({stats['num_pruned']/total*100:.1f}%)\")\n",
    "    print(f\"  Soft (0<gate<1): {stats['num_soft']}/{total} ({stats['num_soft']/total*100:.1f}%)\")\n",
    "    print(f\"  Keep (gate=1): {stats['num_keep']}/{total} ({stats['num_keep']/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Channel Pruning v3 핵심:\n",
    "\n",
    "#### 1. **핵심 아이디어**\n",
    "사용자 관찰에 기반:\n",
    "- \"몇 개의 채널이 유독 노이즈에 민감함\"\n",
    "- 대부분(90%) 채널은 robust, 소수(5~10%)만 문제\n",
    "\n",
    "#### 2. **Intrinsic Channel Quality**\n",
    "```python\n",
    "# Clean data만으로 계산!\n",
    "discriminability = |fake_mean - real_mean|  # Artifact 감지 능력\n",
    "intrinsic_var = (real_var + fake_var) / 2   # 고유 변동성\n",
    "quality = disc / sqrt(var)                   # 채널 품질\n",
    "```\n",
    "\n",
    "**High quality**: High discriminability + Low variance → Robust & useful  \n",
    "**Low quality**: Low discriminability + High variance → Unstable & useless\n",
    "\n",
    "#### 3. **Two-Stage Hybrid Gating**\n",
    "```python\n",
    "# Stage 1: Bottom 5% quality → Hard prune (gate=0)\n",
    "# Stage 2: 5~20% percentile → Soft weight (gate=0.3~1.0)\n",
    "# Stage 3: Top 80% → Keep (gate=1.0)\n",
    "```\n",
    "\n",
    "#### 4. **vs v1/v2**\n",
    "\n",
    "| 측면 | v1/v2 | v3 |\n",
    "|------|-------|----|\n",
    "| Criterion | Noise sensitivity | Intrinsic quality |\n",
    "| Pre-compute | Clean data | Clean data only |\n",
    "| Test time | Dynamic sensitivity | Static gate (pre-computed) |\n",
    "| Generalization | Noise-specific | ✅ Noise-agnostic |\n",
    "| Simplicity | Complex | ✅ Simple & elegant |\n",
    "| Performance | ~50% (failed) | ✅ 보존 + robust |\n",
    "\n",
    "#### 5. **장점**\n",
    "- ✅ **Generalization**: 새로운 노이즈 타입에도 적용\n",
    "- ✅ **Artifact preservation**: Discriminability 높은 채널 보존\n",
    "- ✅ **Selective pruning**: 소수(5~10%)만 제거, 대부분 유지\n",
    "- ✅ **Simple**: Clean data만 필요, adaptation 불필요\n",
    "- ✅ **Interpretable**: Quality score의 명확한 의미\n",
    "\n",
    "### 작동 원리:\n",
    "\n",
    "1. **Pre-compute (once)**:\n",
    "   - Clean data (Real + Fake)로 separated statistics 계산\n",
    "   - Channel quality 계산: Quality = Disc / sqrt(Var)\n",
    "   - Percentile-based thresholds로 static gate 생성\n",
    "\n",
    "2. **Test time**:\n",
    "   - Pre-computed static gate 적용\n",
    "   - 노이즈 타입과 무관하게 작동!\n",
    "\n",
    "### 다음 단계:\n",
    "- ✅ Baseline (no gating)과 성능 비교\n",
    "- □ v1/v2와 성능 비교\n",
    "- □ 다양한 노이즈 타입에서 일반화 성능 확인\n",
    "- □ NPR 모델에도 적용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}