{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel Pruning v1 (CPv1) Example - IMPROVED\n",
    "\n",
    "## Gradient-Pattern-Aware Channel Pruning\n",
    "\n",
    "**핵심 차이점 (vs 기존 CPv1)**:\n",
    "- ✅ **Pseudo-label 제거**: Label-free minimum deviation 사용\n",
    "- ✅ **Gradient corruption sensitivity**: Noise가 gradient pattern을 corrupt하는 정도 측정\n",
    "- ✅ **Batch aggregation**: Channel-level로 안정적인 gating\n",
    "- ✅ **LGrad 최적화**: Two-stage architecture (img2grad + classifier)에 특화\n",
    "\n",
    "**기존 CPv1의 문제**:\n",
    "- ❌ Pseudo-label이 noisy에서 부정확\n",
    "- ❌ Artifact를 noise로 오인 가능\n",
    "- ❌ Batch-wise gating으로 불안정\n",
    "\n",
    "**개선 방법**:\n",
    "```python\n",
    "# OLD (문제 있음):\n",
    "pseudo_label = estimate_label(x)  # 부정확!\n",
    "sensitivity = |curr - reference[pseudo_label]|\n",
    "\n",
    "# NEW (IMPROVED):\n",
    "dev_real = |curr - real_stats|\n",
    "dev_fake = |curr - fake_stats|\n",
    "sensitivity = min(dev_real, dev_fake)  # Label-free!\n",
    "```\n",
    "\n",
    "**기대 효과**:\n",
    "- Noisy/corrupted image에서 성능 향상\n",
    "- Gradient pattern을 robust하게 보존\n",
    "- 안정적인 channel selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Clear cache\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if any(x in mod for x in ['NPR', 'npr', 'LGrad', 'lgrad', 'networks', 'method', 'channel']):\n",
    "        del sys.modules[mod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional, Literal\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.data.dataset import CorruptedDataset\n",
    "from utils.visual.visualizer import DatasetVisualizer\n",
    "from utils.eval.metrics import PredictionCollector, MetricsCalculator\n",
    "\n",
    "# Channel Pruning v1 import\n",
    "from model.method import (\n",
    "    UnifiedChannelPruningV2,\n",
    "    CPv2Config,\n",
    "    compute_separated_statistics_v2,\n",
    ")\n",
    "from model.LGrad.lgrad_model import LGrad\n",
    "from model.NPR.npr_model import NPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU and Model select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  2 19:21:46 2026       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              27W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              24W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              27W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              25W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              26W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              27W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              25W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              25W / 250W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "MODEL_LIST = [\"lgrad\", \"npr\"]\n",
    "MODEL = MODEL_LIST[0]  # \"lgrad\" or \"npr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"corrupted_dataset\"\n",
    "DATASETS = [\n",
    "    \"corrupted_test_data_progan\",\n",
    "    \"corrupted_test_data_stylegan\",\n",
    "    \"corrupted_test_data_stylegan2\",\n",
    "    \"corrupted_test_data_biggan\",\n",
    "]\n",
    "\n",
    "CORRUPTIONS = [\n",
    "    \"original\",\n",
    "    \"gaussian_noise\",\n",
    "    \"jpeg_compression\",\n",
    "]\n",
    "\n",
    "if MODEL == \"lgrad\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 119874\n"
     ]
    }
   ],
   "source": [
    "dataset = CorruptedDataset(\n",
    "    root=ROOT,\n",
    "    datasets=DATASETS,\n",
    "    corruptions=CORRUPTIONS,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/robust_deepfake_ai/model/LGrad/lgrad_model.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(stylegan_weights, map_location=\"cpu\"),\n",
      "/workspace/robust_deepfake_ai/model/LGrad/lgrad_model.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(classifier_weights, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded: LGrad\n"
     ]
    }
   ],
   "source": [
    "# LGrad\n",
    "STYLEGAN_WEIGHTS_ROOT = \"model/LGrad/weights/karras2019stylegan-bedrooms-256x256_discriminator.pth\"\n",
    "CLASSIFIER_WEIGHTS_ROOT = \"model/LGrad/weights/LGrad-Pretrained-Model/LGrad-4class-Trainon-Progan_car_cat_chair_horse.pth\"\n",
    "\n",
    "# NPR\n",
    "NPR_WEIGHTS_ROOT = \"model/NPR/weights/NPR.pth\"\n",
    "\n",
    "if MODEL == \"lgrad\":\n",
    "    base_model = LGrad(\n",
    "        stylegan_weights=STYLEGAN_WEIGHTS_ROOT,\n",
    "        classifier_weights=CLASSIFIER_WEIGHTS_ROOT,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    model_name = \"LGrad\"\n",
    "elif MODEL == \"npr\":\n",
    "    base_model = NPR(\n",
    "        weights=NPR_WEIGHTS_ROOT,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    model_name = \"NPR\"\n",
    "\n",
    "print(f\"Base model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compute Separated Statistics from Clean Data\n",
    "\n",
    "**중요!** CPv1은 Real clean과 Fake clean의 **분리된** statistics가 필요합니다.\n",
    "\n",
    "- ProGAN의 original (uncorrupted) 데이터로 statistics 수집\n",
    "- **Labels 필수**: Real (0) vs Fake (1) 구분을 위해\n",
    "- 한 번 계산하면 저장해서 재사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProGAN clean samples: 8000\n"
     ]
    }
   ],
   "source": [
    "# Clean data 준비 (ProGAN original - LABELS 필수!)\n",
    "progan_clean_indices = [\n",
    "    i for i, s in enumerate(dataset.samples)\n",
    "    if s['dataset'] == \"corrupted_test_data_progan\" and s['corruption'] == \"original\"\n",
    "]\n",
    "\n",
    "print(f\"ProGAN clean samples: {len(progan_clean_indices)}\")\n",
    "\n",
    "# Subset & DataLoader (labels 포함!)\n",
    "clean_subset = Subset(dataset, progan_clean_indices)\n",
    "clean_loader = DataLoader(\n",
    "    clean_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CPv2] Computing separated statistics for 106 layers (ONLINE MODE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing separated statistics v2 (online): 100%|██████████| 250/250 [02:23<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  classifier.conv1: C=64\n",
      "    Real: mean=[-6.3340, 7.3021], std_mean=1.4538\n",
      "    Fake: mean=[-6.3223, 7.2592], std_mean=1.4562\n",
      "    Artifact signature: mean=0.0062, max=0.0523\n",
      "  classifier.bn1: C=64\n",
      "    Real: mean=[0.0000, 2.0776], std_mean=0.1827\n",
      "    Fake: mean=[0.0000, 2.0739], std_mean=0.1868\n",
      "    Artifact signature: mean=0.0011, max=0.0045\n",
      "  classifier.layer1.0.conv1: C=64\n",
      "    Real: mean=[-7.1698, 6.7641], std_mean=0.5785\n",
      "    Fake: mean=[-7.3122, 6.9164], std_mean=0.5762\n",
      "    Artifact signature: mean=0.0402, max=0.1523\n",
      "  classifier.layer1.0.bn1: C=64\n",
      "    Real: mean=[0.0000, 1.1708], std_mean=0.1276\n",
      "    Fake: mean=[0.0000, 1.1761], std_mean=0.1248\n",
      "    Artifact signature: mean=0.0088, max=0.0430\n",
      "  classifier.layer1.0.conv2: C=64\n",
      "    Real: mean=[-6.2954, 7.5667], std_mean=2.5168\n",
      "    Fake: mean=[-6.2174, 7.4683], std_mean=2.4162\n",
      "    Artifact signature: mean=0.1382, max=0.6342\n",
      "  classifier.layer1.0.bn2: C=64\n",
      "    Real: mean=[0.0000, 1.6930], std_mean=0.1396\n",
      "    Fake: mean=[0.0000, 1.6927], std_mean=0.1332\n",
      "    Artifact signature: mean=0.0044, max=0.0229\n",
      "  classifier.layer1.0.conv3: C=256\n",
      "    Real: mean=[-2.2648, 2.7520], std_mean=0.5381\n",
      "    Fake: mean=[-2.2287, 2.7184], std_mean=0.5136\n",
      "    Artifact signature: mean=0.0143, max=0.0695\n",
      "  classifier.layer1.0.bn3: C=256\n",
      "    Real: mean=[0.0000, 1.2194], std_mean=0.1461\n",
      "    Fake: mean=[0.0000, 1.2702], std_mean=0.1445\n",
      "    Artifact signature: mean=0.0071, max=0.0693\n",
      "  classifier.layer1.0.downsample.0: C=256\n",
      "    Real: mean=[-8.1524, 6.9612], std_mean=0.5660\n",
      "    Fake: mean=[-8.3097, 7.0982], std_mean=0.5580\n",
      "    Artifact signature: mean=0.0329, max=0.1696\n",
      "  classifier.layer1.0.downsample.1: C=256\n",
      "    Real: mean=[-0.5039, 0.5432], std_mean=0.2824\n",
      "    Fake: mean=[-0.4969, 0.5940], std_mean=0.2791\n",
      "    Artifact signature: mean=0.0126, max=0.0645\n",
      "  classifier.layer1.1.conv1: C=64\n",
      "    Real: mean=[-3.4729, 2.1518], std_mean=1.4815\n",
      "    Fake: mean=[-3.5351, 2.1794], std_mean=1.4518\n",
      "    Artifact signature: mean=0.1097, max=0.3635\n",
      "  classifier.layer1.1.bn1: C=64\n",
      "    Real: mean=[0.0000, 0.8371], std_mean=0.1346\n",
      "    Fake: mean=[0.0000, 0.8421], std_mean=0.1304\n",
      "    Artifact signature: mean=0.0096, max=0.0702\n",
      "  classifier.layer1.1.conv2: C=64\n",
      "    Real: mean=[-6.5360, 4.7898], std_mean=1.7786\n",
      "    Fake: mean=[-6.5824, 4.4466], std_mean=1.6949\n",
      "    Artifact signature: mean=0.1494, max=0.6313\n",
      "  classifier.layer1.1.bn2: C=64\n",
      "    Real: mean=[0.0000, 0.7506], std_mean=0.1146\n",
      "    Fake: mean=[0.0000, 0.7497], std_mean=0.1107\n",
      "    Artifact signature: mean=0.0055, max=0.0426\n",
      "  classifier.layer1.1.conv3: C=256\n",
      "    Real: mean=[-1.2426, 1.3051], std_mean=0.4641\n",
      "    Fake: mean=[-1.2353, 1.2885], std_mean=0.4360\n",
      "    Artifact signature: mean=0.0255, max=0.1825\n",
      "  classifier.layer1.1.bn3: C=256\n",
      "    Real: mean=[0.0000, 1.6158], std_mean=0.1625\n",
      "    Fake: mean=[0.0000, 1.6104], std_mean=0.1594\n",
      "    Artifact signature: mean=0.0074, max=0.0639\n",
      "  classifier.layer1.2.conv1: C=64\n",
      "    Real: mean=[-3.4516, 3.3084], std_mean=1.4898\n",
      "    Fake: mean=[-3.5179, 3.2457], std_mean=1.5195\n",
      "    Artifact signature: mean=0.0771, max=0.3454\n",
      "  classifier.layer1.2.bn1: C=64\n",
      "    Real: mean=[0.0000, 0.6684], std_mean=0.1332\n",
      "    Fake: mean=[0.0000, 0.6936], std_mean=0.1338\n",
      "    Artifact signature: mean=0.0061, max=0.0358\n",
      "  classifier.layer1.2.conv2: C=64\n",
      "    Real: mean=[-3.4200, 5.0893], std_mean=2.0077\n",
      "    Fake: mean=[-3.3503, 5.1617], std_mean=2.0130\n",
      "    Artifact signature: mean=0.0606, max=0.2656\n",
      "  classifier.layer1.2.bn2: C=64\n",
      "    Real: mean=[0.0000, 0.5974], std_mean=0.1071\n",
      "    Fake: mean=[0.0000, 0.5938], std_mean=0.1066\n",
      "    Artifact signature: mean=0.0044, max=0.0211\n",
      "  classifier.layer1.2.conv3: C=256\n",
      "    Real: mean=[-0.6505, 0.6515], std_mean=0.3267\n",
      "    Fake: mean=[-0.6752, 0.6494], std_mean=0.3255\n",
      "    Artifact signature: mean=0.0150, max=0.0926\n",
      "  classifier.layer1.2.bn3: C=256\n",
      "    Real: mean=[0.0000, 2.1957], std_mean=0.1538\n",
      "    Fake: mean=[0.0000, 2.2000], std_mean=0.1509\n",
      "    Artifact signature: mean=0.0073, max=0.0666\n",
      "  classifier.layer2.0.conv1: C=128\n",
      "    Real: mean=[-4.4830, 7.3265], std_mean=1.2272\n",
      "    Fake: mean=[-4.4739, 7.2719], std_mean=1.2300\n",
      "    Artifact signature: mean=0.0675, max=0.3255\n",
      "  classifier.layer2.0.bn1: C=128\n",
      "    Real: mean=[0.0000, 0.2587], std_mean=0.0656\n",
      "    Fake: mean=[0.0000, 0.2732], std_mean=0.0635\n",
      "    Artifact signature: mean=0.0044, max=0.0340\n",
      "  classifier.layer2.0.conv2: C=128\n",
      "    Real: mean=[-3.4414, 1.9123], std_mean=1.7131\n",
      "    Fake: mean=[-3.9866, 2.3522], std_mean=1.6563\n",
      "    Artifact signature: mean=0.2968, max=1.0432\n",
      "  classifier.layer2.0.bn2: C=128\n",
      "    Real: mean=[0.0000, 0.5642], std_mean=0.1304\n",
      "    Fake: mean=[0.0000, 0.5407], std_mean=0.1226\n",
      "    Artifact signature: mean=0.0223, max=0.1716\n",
      "  classifier.layer2.0.conv3: C=512\n",
      "    Real: mean=[-2.0196, 2.0433], std_mean=0.9237\n",
      "    Fake: mean=[-1.6480, 1.6331], std_mean=0.7417\n",
      "    Artifact signature: mean=0.1807, max=0.6738\n",
      "  classifier.layer2.0.bn3: C=512\n",
      "    Real: mean=[0.0000, 1.1953], std_mean=0.1886\n",
      "    Fake: mean=[0.0000, 1.2367], std_mean=0.1813\n",
      "    Artifact signature: mean=0.0206, max=0.2500\n",
      "  classifier.layer2.0.downsample.0: C=512\n",
      "    Real: mean=[-4.6705, 7.3776], std_mean=1.0276\n",
      "    Fake: mean=[-4.6031, 7.2968], std_mean=1.0163\n",
      "    Artifact signature: mean=0.0592, max=0.3518\n",
      "  classifier.layer2.0.downsample.1: C=512\n",
      "    Real: mean=[-0.4170, 0.6014], std_mean=0.2341\n",
      "    Fake: mean=[-0.4122, 0.6340], std_mean=0.2299\n",
      "    Artifact signature: mean=0.0130, max=0.1059\n",
      "  classifier.layer2.1.conv1: C=128\n",
      "    Real: mean=[-6.6032, 8.9350], std_mean=3.5054\n",
      "    Fake: mean=[-6.2894, 8.5089], std_mean=3.2880\n",
      "    Artifact signature: mean=0.3714, max=1.5496\n",
      "  classifier.layer2.1.bn1: C=128\n",
      "    Real: mean=[0.0000, 0.6601], std_mean=0.1057\n",
      "    Fake: mean=[0.0000, 0.6647], std_mean=0.1004\n",
      "    Artifact signature: mean=0.0098, max=0.0597\n",
      "  classifier.layer2.1.conv2: C=128\n",
      "    Real: mean=[-16.8200, 9.8765], std_mean=3.1576\n",
      "    Fake: mean=[-18.2920, 10.5327], std_mean=3.0160\n",
      "    Artifact signature: mean=0.2251, max=1.4720\n",
      "  classifier.layer2.1.bn2: C=128\n",
      "    Real: mean=[0.0000, 1.0938], std_mean=0.0818\n",
      "    Fake: mean=[0.0000, 1.0943], std_mean=0.0784\n",
      "    Artifact signature: mean=0.0047, max=0.0549\n",
      "  classifier.layer2.1.conv3: C=512\n",
      "    Real: mean=[-2.4357, 2.3661], std_mean=0.6513\n",
      "    Fake: mean=[-2.4989, 2.3675], std_mean=0.6164\n",
      "    Artifact signature: mean=0.0215, max=0.1228\n",
      "  classifier.layer2.1.bn3: C=512\n",
      "    Real: mean=[0.0000, 1.6702], std_mean=0.2024\n",
      "    Fake: mean=[0.0000, 1.7089], std_mean=0.1948\n",
      "    Artifact signature: mean=0.0200, max=0.2498\n",
      "  classifier.layer2.2.conv1: C=128\n",
      "    Real: mean=[-5.8644, 5.0932], std_mean=3.3030\n",
      "    Fake: mean=[-5.8262, 4.6631], std_mean=3.2033\n",
      "    Artifact signature: mean=0.3301, max=1.2925\n",
      "  classifier.layer2.2.bn1: C=128\n",
      "    Real: mean=[0.0000, 0.4293], std_mean=0.1720\n",
      "    Fake: mean=[0.0000, 0.4707], std_mean=0.1647\n",
      "    Artifact signature: mean=0.0146, max=0.0772\n",
      "  classifier.layer2.2.conv2: C=128\n",
      "    Real: mean=[-11.2769, 13.9712], std_mean=3.7196\n",
      "    Fake: mean=[-11.3197, 13.2486], std_mean=3.4521\n",
      "    Artifact signature: mean=0.3028, max=1.6443\n",
      "  classifier.layer2.2.bn2: C=128\n",
      "    Real: mean=[0.0000, 0.6343], std_mean=0.1087\n",
      "    Fake: mean=[0.0000, 0.6125], std_mean=0.0992\n",
      "    Artifact signature: mean=0.0082, max=0.0475\n",
      "  classifier.layer2.2.conv3: C=512\n",
      "    Real: mean=[-1.3138, 1.2081], std_mean=0.9144\n",
      "    Fake: mean=[-1.0942, 1.0907], std_mean=0.6924\n",
      "    Artifact signature: mean=0.0988, max=0.6714\n",
      "  classifier.layer2.2.bn3: C=512\n",
      "    Real: mean=[0.0000, 1.8837], std_mean=0.2013\n",
      "    Fake: mean=[0.0000, 1.9105], std_mean=0.1903\n",
      "    Artifact signature: mean=0.0191, max=0.2486\n",
      "  classifier.layer2.3.conv1: C=128\n",
      "    Real: mean=[-4.9806, 3.5893], std_mean=2.9476\n",
      "    Fake: mean=[-5.4120, 3.4153], std_mean=2.8086\n",
      "    Artifact signature: mean=0.2182, max=0.8349\n",
      "  classifier.layer2.3.bn1: C=128\n",
      "    Real: mean=[0.0000, 0.5354], std_mean=0.1891\n",
      "    Fake: mean=[0.0000, 0.5418], std_mean=0.1791\n",
      "    Artifact signature: mean=0.0138, max=0.0792\n",
      "  classifier.layer2.3.conv2: C=128\n",
      "    Real: mean=[-15.5063, 12.0719], std_mean=5.2749\n",
      "    Fake: mean=[-13.8219, 10.2407], std_mean=4.6419\n",
      "    Artifact signature: mean=0.5492, max=2.0702\n",
      "  classifier.layer2.3.bn2: C=128\n",
      "    Real: mean=[0.0000, 0.5110], std_mean=0.1168\n",
      "    Fake: mean=[0.0000, 0.5230], std_mean=0.0999\n",
      "    Artifact signature: mean=0.0119, max=0.0663\n",
      "  classifier.layer2.3.conv3: C=512\n",
      "    Real: mean=[-1.6463, 1.4426], std_mean=1.7099\n",
      "    Fake: mean=[-1.7698, 1.5448], std_mean=1.1538\n",
      "    Artifact signature: mean=0.2046, max=0.7087\n",
      "  classifier.layer2.3.bn3: C=512\n",
      "    Real: mean=[0.0000, 1.9312], std_mean=0.1659\n",
      "    Fake: mean=[0.0000, 1.9489], std_mean=0.1568\n",
      "    Artifact signature: mean=0.0173, max=0.2419\n",
      "  classifier.layer3.0.conv1: C=256\n",
      "    Real: mean=[-7.6368, 5.0978], std_mean=2.3198\n",
      "    Fake: mean=[-8.0895, 4.4133], std_mean=2.1920\n",
      "    Artifact signature: mean=0.5084, max=1.6790\n",
      "  classifier.layer3.0.bn1: C=256\n",
      "    Real: mean=[0.0000, 0.3833], std_mean=0.0837\n",
      "    Fake: mean=[0.0000, 0.3688], std_mean=0.0783\n",
      "    Artifact signature: mean=0.0162, max=0.0896\n",
      "  classifier.layer3.0.conv2: C=256\n",
      "    Real: mean=[-11.8043, 8.3323], std_mean=5.6257\n",
      "    Fake: mean=[-14.9955, 3.0490], std_mean=5.1637\n",
      "    Artifact signature: mean=3.4272, max=8.9090\n",
      "  classifier.layer3.0.bn2: C=256\n",
      "    Real: mean=[0.0000, 0.7759], std_mean=0.1062\n",
      "    Fake: mean=[0.0000, 0.6603], std_mean=0.1028\n",
      "    Artifact signature: mean=0.0616, max=0.3244\n",
      "  classifier.layer3.0.conv3: C=1024\n",
      "    Real: mean=[-7.0871, 7.7914], std_mean=2.1023\n",
      "    Fake: mean=[-6.3145, 5.1759], std_mean=1.9325\n",
      "    Artifact signature: mean=1.6148, max=5.1524\n",
      "  classifier.layer3.0.bn3: C=1024\n",
      "    Real: mean=[0.0000, 1.5387], std_mean=0.1850\n",
      "    Fake: mean=[0.0000, 1.5392], std_mean=0.1826\n",
      "    Artifact signature: mean=0.0961, max=0.7878\n",
      "  classifier.layer3.0.downsample.0: C=1024\n",
      "    Real: mean=[-6.9640, 5.8730], std_mean=2.0570\n",
      "    Fake: mean=[-7.0830, 5.6139], std_mean=1.9119\n",
      "    Artifact signature: mean=0.5039, max=2.3084\n",
      "  classifier.layer3.0.downsample.1: C=1024\n",
      "    Real: mean=[-0.4391, 0.5803], std_mean=0.1612\n",
      "    Fake: mean=[-0.4438, 0.5980], std_mean=0.1523\n",
      "    Artifact signature: mean=0.0371, max=0.2871\n",
      "  classifier.layer3.1.conv1: C=256\n",
      "    Real: mean=[-19.8754, 16.4126], std_mean=13.8774\n",
      "    Fake: mean=[-28.9168, 18.1834], std_mean=10.7814\n",
      "    Artifact signature: mean=9.1496, max=23.8022\n",
      "  classifier.layer3.1.bn1: C=256\n",
      "    Real: mean=[0.0000, 0.7424], std_mean=0.0873\n",
      "    Fake: mean=[0.0000, 0.6521], std_mean=0.0573\n",
      "    Artifact signature: mean=0.0492, max=0.2624\n",
      "  classifier.layer3.1.conv2: C=256\n",
      "    Real: mean=[-44.9926, 22.8978], std_mean=10.5379\n",
      "    Fake: mean=[-34.2872, 25.9957], std_mean=6.6231\n",
      "    Artifact signature: mean=6.2591, max=21.8688\n",
      "  classifier.layer3.1.bn2: C=256\n",
      "    Real: mean=[0.0000, 0.4287], std_mean=0.0839\n",
      "    Fake: mean=[0.0000, 0.5867], std_mean=0.0757\n",
      "    Artifact signature: mean=0.0379, max=0.3608\n",
      "  classifier.layer3.1.conv3: C=1024\n",
      "    Real: mean=[-2.8144, 2.1861], std_mean=1.3276\n",
      "    Fake: mean=[-4.8102, 4.8278], std_mean=1.3366\n",
      "    Artifact signature: mean=0.8168, max=3.4500\n",
      "  classifier.layer3.1.bn3: C=1024\n",
      "    Real: mean=[0.0000, 1.7219], std_mean=0.1833\n",
      "    Fake: mean=[0.0000, 1.7226], std_mean=0.1814\n",
      "    Artifact signature: mean=0.0805, max=1.0113\n",
      "  classifier.layer3.2.conv1: C=256\n",
      "    Real: mean=[-28.1077, 15.3036], std_mean=7.2167\n",
      "    Fake: mean=[-12.7548, 14.2654], std_mean=8.9722\n",
      "    Artifact signature: mean=5.8673, max=20.2012\n",
      "  classifier.layer3.2.bn1: C=256\n",
      "    Real: mean=[0.0000, 0.6781], std_mean=0.0823\n",
      "    Fake: mean=[0.0000, 0.6770], std_mean=0.1133\n",
      "    Artifact signature: mean=0.0625, max=0.3592\n",
      "  classifier.layer3.2.conv2: C=256\n",
      "    Real: mean=[-23.3786, 19.4579], std_mean=5.8373\n",
      "    Fake: mean=[-40.1271, 22.9216], std_mean=9.0485\n",
      "    Artifact signature: mean=5.8444, max=24.5080\n",
      "  classifier.layer3.2.bn2: C=256\n",
      "    Real: mean=[0.0000, 0.7140], std_mean=0.0564\n",
      "    Fake: mean=[0.0000, 0.6606], std_mean=0.0805\n",
      "    Artifact signature: mean=0.0324, max=0.3780\n",
      "  classifier.layer3.2.conv3: C=1024\n",
      "    Real: mean=[-2.2057, 2.9870], std_mean=1.0150\n",
      "    Fake: mean=[-3.1281, 2.8628], std_mean=1.4433\n",
      "    Artifact signature: mean=0.6223, max=2.2703\n",
      "  classifier.layer3.2.bn3: C=1024\n",
      "    Real: mean=[0.0000, 2.2350], std_mean=0.1921\n",
      "    Fake: mean=[0.0000, 2.1136], std_mean=0.2022\n",
      "    Artifact signature: mean=0.0958, max=1.1149\n",
      "  classifier.layer3.3.conv1: C=256\n",
      "    Real: mean=[-27.6452, 18.0739], std_mean=6.0070\n",
      "    Fake: mean=[-14.3773, 15.4560], std_mean=7.1460\n",
      "    Artifact signature: mean=5.2224, max=22.9360\n",
      "  classifier.layer3.3.bn1: C=256\n",
      "    Real: mean=[0.0000, 0.4518], std_mean=0.0340\n",
      "    Fake: mean=[0.0000, 0.8391], std_mean=0.0510\n",
      "    Artifact signature: mean=0.0307, max=0.3873\n",
      "  classifier.layer3.3.conv2: C=256\n",
      "    Real: mean=[-11.6873, 4.6812], std_mean=1.9731\n",
      "    Fake: mean=[-20.0137, 10.0927], std_mean=3.0672\n",
      "    Artifact signature: mean=2.0989, max=8.8247\n",
      "  classifier.layer3.3.bn2: C=256\n",
      "    Real: mean=[0.0000, 0.5536], std_mean=0.0402\n",
      "    Fake: mean=[0.0000, 0.5289], std_mean=0.0530\n",
      "    Artifact signature: mean=0.0193, max=0.3151\n",
      "  classifier.layer3.3.conv3: C=1024\n",
      "    Real: mean=[-2.6677, 2.7689], std_mean=0.8253\n",
      "    Fake: mean=[-2.6934, 2.4733], std_mean=1.0919\n",
      "    Artifact signature: mean=0.4466, max=1.8160\n",
      "  classifier.layer3.3.bn3: C=1024\n",
      "    Real: mean=[0.0000, 2.7390], std_mean=0.1804\n",
      "    Fake: mean=[0.0000, 2.5915], std_mean=0.1957\n",
      "    Artifact signature: mean=0.0931, max=1.3337\n",
      "  classifier.layer3.4.conv1: C=256\n",
      "    Real: mean=[-24.6389, 16.4596], std_mean=5.8626\n",
      "    Fake: mean=[-17.6364, 16.6590], std_mean=7.0599\n",
      "    Artifact signature: mean=7.0041, max=22.5921\n",
      "  classifier.layer3.4.bn1: C=256\n",
      "    Real: mean=[0.0000, 0.8104], std_mean=0.0475\n",
      "    Fake: mean=[0.0000, 0.6116], std_mean=0.0677\n",
      "    Artifact signature: mean=0.0733, max=0.5341\n",
      "  classifier.layer3.4.conv2: C=256\n",
      "    Real: mean=[-38.9547, 14.9722], std_mean=3.1377\n",
      "    Fake: mean=[-16.6645, 13.8081], std_mean=3.7469\n",
      "    Artifact signature: mean=4.7535, max=30.1397\n",
      "  classifier.layer3.4.bn2: C=256\n",
      "    Real: mean=[0.0000, 0.5648], std_mean=0.0158\n",
      "    Fake: mean=[0.0000, 0.6889], std_mean=0.0263\n",
      "    Artifact signature: mean=0.0078, max=0.2819\n",
      "  classifier.layer3.4.conv3: C=1024\n",
      "    Real: mean=[-2.6632, 3.1652], std_mean=0.3894\n",
      "    Fake: mean=[-2.9526, 3.7178], std_mean=0.7262\n",
      "    Artifact signature: mean=0.1055, max=0.6997\n",
      "  classifier.layer3.4.bn3: C=1024\n",
      "    Real: mean=[0.0000, 2.8040], std_mean=0.1625\n",
      "    Fake: mean=[0.0000, 2.6455], std_mean=0.1826\n",
      "    Artifact signature: mean=0.0807, max=1.3343\n",
      "  classifier.layer3.5.conv1: C=256\n",
      "    Real: mean=[-19.9755, 14.8345], std_mean=4.8150\n",
      "    Fake: mean=[-17.3320, 15.1983], std_mean=5.5134\n",
      "    Artifact signature: mean=3.7051, max=15.4026\n",
      "  classifier.layer3.5.bn1: C=256\n",
      "    Real: mean=[0.0000, 0.7318], std_mean=0.0467\n",
      "    Fake: mean=[0.0000, 0.8181], std_mean=0.0633\n",
      "    Artifact signature: mean=0.0329, max=0.3866\n",
      "  classifier.layer3.5.conv2: C=256\n",
      "    Real: mean=[-12.5582, 11.1471], std_mean=2.6220\n",
      "    Fake: mean=[-10.8840, 12.2496], std_mean=3.5190\n",
      "    Artifact signature: mean=1.8479, max=8.7370\n",
      "  classifier.layer3.5.bn2: C=256\n",
      "    Real: mean=[0.0000, 0.6279], std_mean=0.0276\n",
      "    Fake: mean=[0.0000, 0.7843], std_mean=0.0529\n",
      "    Artifact signature: mean=0.0170, max=0.2584\n",
      "  classifier.layer3.5.conv3: C=1024\n",
      "    Real: mean=[-2.2187, 1.9317], std_mean=0.5613\n",
      "    Fake: mean=[-3.2339, 3.3536], std_mean=1.2842\n",
      "    Artifact signature: mean=0.3841, max=1.5428\n",
      "  classifier.layer3.5.bn3: C=1024\n",
      "    Real: mean=[0.0000, 3.1536], std_mean=0.1332\n",
      "    Fake: mean=[0.0000, 2.8542], std_mean=0.1501\n",
      "    Artifact signature: mean=0.0736, max=1.3804\n",
      "  classifier.layer4.0.conv1: C=512\n",
      "    Real: mean=[-18.5167, 24.9220], std_mean=4.7748\n",
      "    Fake: mean=[-16.4736, 18.8618], std_mean=5.2541\n",
      "    Artifact signature: mean=4.5690, max=21.4780\n",
      "  classifier.layer4.0.bn1: C=512\n",
      "    Real: mean=[0.0000, 0.8406], std_mean=0.0672\n",
      "    Fake: mean=[0.0000, 0.8645], std_mean=0.0724\n",
      "    Artifact signature: mean=0.0400, max=0.6140\n",
      "  classifier.layer4.0.conv2: C=512\n",
      "    Real: mean=[-33.0632, 29.6789], std_mean=8.8711\n",
      "    Fake: mean=[-54.7369, 32.6961], std_mean=10.9566\n",
      "    Artifact signature: mean=11.9316, max=41.0039\n",
      "  classifier.layer4.0.bn2: C=512\n",
      "    Real: mean=[0.0000, 0.8073], std_mean=0.0615\n",
      "    Fake: mean=[0.0000, 1.0767], std_mean=0.0537\n",
      "    Artifact signature: mean=0.0613, max=0.5719\n",
      "  classifier.layer4.0.conv3: C=2048\n",
      "    Real: mean=[-13.4940, 12.3595], std_mean=3.1336\n",
      "    Fake: mean=[-17.0306, 17.9241], std_mean=2.7819\n",
      "    Artifact signature: mean=3.8554, max=12.1766\n",
      "  classifier.layer4.0.bn3: C=2048\n",
      "    Real: mean=[0.0000, 2.0845], std_mean=0.2029\n",
      "    Fake: mean=[0.0000, 1.6222], std_mean=0.1961\n",
      "    Artifact signature: mean=0.1975, max=1.9232\n",
      "  classifier.layer4.0.downsample.0: C=2048\n",
      "    Real: mean=[-27.0844, 27.9139], std_mean=6.6574\n",
      "    Fake: mean=[-25.2110, 21.3230], std_mean=6.6381\n",
      "    Artifact signature: mean=6.5633, max=22.6370\n",
      "  classifier.layer4.0.downsample.1: C=2048\n",
      "    Real: mean=[-0.7315, 1.3154], std_mean=0.2627\n",
      "    Fake: mean=[-0.8655, 0.9984], std_mean=0.2634\n",
      "    Artifact signature: mean=0.2407, max=2.1809\n",
      "  classifier.layer4.1.conv1: C=512\n",
      "    Real: mean=[-76.2575, 51.0400], std_mean=10.8520\n",
      "    Fake: mean=[-49.6918, 44.0641], std_mean=11.5482\n",
      "    Artifact signature: mean=17.4977, max=95.3970\n",
      "  classifier.layer4.1.bn1: C=512\n",
      "    Real: mean=[0.0000, 0.6435], std_mean=0.0141\n",
      "    Fake: mean=[0.0000, 0.7319], std_mean=0.0182\n",
      "    Artifact signature: mean=0.0308, max=0.5986\n",
      "  classifier.layer4.1.conv2: C=512\n",
      "    Real: mean=[-18.1562, 10.3861], std_mean=1.6763\n",
      "    Fake: mean=[-26.1013, 19.9944], std_mean=3.8935\n",
      "    Artifact signature: mean=8.9025, max=30.4236\n",
      "  classifier.layer4.1.bn2: C=512\n",
      "    Real: mean=[0.0000, 0.5857], std_mean=0.0041\n",
      "    Fake: mean=[0.0000, 0.4977], std_mean=0.0158\n",
      "    Artifact signature: mean=0.0110, max=0.5101\n",
      "  classifier.layer4.1.conv3: C=2048\n",
      "    Real: mean=[-0.9052, 0.9210], std_mean=0.1903\n",
      "    Fake: mean=[-1.6173, 1.1624], std_mean=1.1011\n",
      "    Artifact signature: mean=0.6057, max=1.8454\n",
      "  classifier.layer4.1.bn3: C=2048\n",
      "    Real: mean=[0.0000, 2.2254], std_mean=0.2026\n",
      "    Fake: mean=[0.0000, 2.4337], std_mean=0.2418\n",
      "    Artifact signature: mean=0.2194, max=2.3674\n",
      "  classifier.layer4.2.conv1: C=512\n",
      "    Real: mean=[-68.9699, 44.9891], std_mean=6.3845\n",
      "    Fake: mean=[-56.6599, 55.8959], std_mean=10.0054\n",
      "    Artifact signature: mean=8.7317, max=105.0210\n",
      "  classifier.layer4.2.bn1: C=512\n",
      "    Real: mean=[0.0000, 0.7951], std_mean=0.0039\n",
      "    Fake: mean=[0.0000, 0.7319], std_mean=0.0115\n",
      "    Artifact signature: mean=0.0145, max=0.6322\n",
      "  classifier.layer4.2.conv2: C=512\n",
      "    Real: mean=[-3.6733, 4.1220], std_mean=0.5842\n",
      "    Fake: mean=[-14.2991, 8.3483], std_mean=2.1751\n",
      "    Artifact signature: mean=3.6970, max=13.5926\n",
      "  classifier.layer4.2.bn2: C=512\n",
      "    Real: mean=[0.0000, 0.6126], std_mean=0.0043\n",
      "    Fake: mean=[0.0000, 0.6130], std_mean=0.0338\n",
      "    Artifact signature: mean=0.0101, max=0.4700\n",
      "  classifier.layer4.2.conv3: C=2048\n",
      "    Real: mean=[-1.4222, 1.2173], std_mean=0.2080\n",
      "    Fake: mean=[-1.5545, 1.3516], std_mean=2.5286\n",
      "    Artifact signature: mean=0.6284, max=1.6526\n",
      "  classifier.layer4.2.bn3: C=2048\n",
      "    Real: mean=[0.0000, 2.2616], std_mean=0.1826\n",
      "    Fake: mean=[0.0000, 2.1799], std_mean=1.1742\n",
      "    Artifact signature: mean=0.2949, max=2.1141\n",
      "[CPv2] Separated statistics computed for 106 layers\n"
     ]
    }
   ],
   "source": [
    "# Separated statistics 파일 경로\n",
    "STATS_PATH_V2 = f\"separated_stats_v2_{MODEL}_progan.pth\"\n",
    "if os.path.exists(STATS_PATH_V2):\n",
    "    separated_stats_v2 = torch.load(STATS_PATH_V2)\n",
    "else:\n",
    "    separated_stats_v2 = compute_separated_statistics_v2(\n",
    "        model=base_model,\n",
    "        dataloader=clean_loader,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    torch.save(separated_stats_v2, STATS_PATH_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Channel Pruning v1 Model (IMPROVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  classifier.conv1 (C=64):\n",
      "    Artifact disc: mean=0.0062, max=0.0523\n",
      "  classifier.bn1 (C=64):\n",
      "    Artifact disc: mean=0.0011, max=0.0045\n",
      "  classifier.layer1.0.conv1 (C=64):\n",
      "    Artifact disc: mean=0.0402, max=0.1523\n",
      "  classifier.layer1.0.bn1 (C=64):\n",
      "    Artifact disc: mean=0.0088, max=0.0430\n",
      "  classifier.layer1.0.conv2 (C=64):\n",
      "    Artifact disc: mean=0.1382, max=0.6342\n",
      "  classifier.layer1.0.bn2 (C=64):\n",
      "    Artifact disc: mean=0.0044, max=0.0229\n",
      "  classifier.layer1.0.conv3 (C=256):\n",
      "    Artifact disc: mean=0.0143, max=0.0695\n",
      "  classifier.layer1.0.bn3 (C=256):\n",
      "    Artifact disc: mean=0.0071, max=0.0693\n",
      "  classifier.layer1.0.downsample.0 (C=256):\n",
      "    Artifact disc: mean=0.0329, max=0.1696\n",
      "  classifier.layer1.0.downsample.1 (C=256):\n",
      "    Artifact disc: mean=0.0126, max=0.0645\n",
      "  classifier.layer1.1.conv1 (C=64):\n",
      "    Artifact disc: mean=0.1097, max=0.3635\n",
      "  classifier.layer1.1.bn1 (C=64):\n",
      "    Artifact disc: mean=0.0096, max=0.0702\n",
      "  classifier.layer1.1.conv2 (C=64):\n",
      "    Artifact disc: mean=0.1494, max=0.6313\n",
      "  classifier.layer1.1.bn2 (C=64):\n",
      "    Artifact disc: mean=0.0055, max=0.0426\n",
      "  classifier.layer1.1.conv3 (C=256):\n",
      "    Artifact disc: mean=0.0255, max=0.1825\n",
      "  classifier.layer1.1.bn3 (C=256):\n",
      "    Artifact disc: mean=0.0074, max=0.0639\n",
      "  classifier.layer1.2.conv1 (C=64):\n",
      "    Artifact disc: mean=0.0771, max=0.3454\n",
      "  classifier.layer1.2.bn1 (C=64):\n",
      "    Artifact disc: mean=0.0061, max=0.0358\n",
      "  classifier.layer1.2.conv2 (C=64):\n",
      "    Artifact disc: mean=0.0606, max=0.2656\n",
      "  classifier.layer1.2.bn2 (C=64):\n",
      "    Artifact disc: mean=0.0044, max=0.0211\n",
      "  classifier.layer1.2.conv3 (C=256):\n",
      "    Artifact disc: mean=0.0150, max=0.0926\n",
      "  classifier.layer1.2.bn3 (C=256):\n",
      "    Artifact disc: mean=0.0073, max=0.0666\n",
      "  classifier.layer2.0.conv1 (C=128):\n",
      "    Artifact disc: mean=0.0675, max=0.3255\n",
      "  classifier.layer2.0.bn1 (C=128):\n",
      "    Artifact disc: mean=0.0044, max=0.0340\n",
      "  classifier.layer2.0.conv2 (C=128):\n",
      "    Artifact disc: mean=0.2968, max=1.0432\n",
      "  classifier.layer2.0.bn2 (C=128):\n",
      "    Artifact disc: mean=0.0223, max=0.1716\n",
      "  classifier.layer2.0.conv3 (C=512):\n",
      "    Artifact disc: mean=0.1807, max=0.6738\n",
      "  classifier.layer2.0.bn3 (C=512):\n",
      "    Artifact disc: mean=0.0206, max=0.2500\n",
      "  classifier.layer2.0.downsample.0 (C=512):\n",
      "    Artifact disc: mean=0.0592, max=0.3518\n",
      "  classifier.layer2.0.downsample.1 (C=512):\n",
      "    Artifact disc: mean=0.0130, max=0.1059\n",
      "  classifier.layer2.1.conv1 (C=128):\n",
      "    Artifact disc: mean=0.3714, max=1.5496\n",
      "  classifier.layer2.1.bn1 (C=128):\n",
      "    Artifact disc: mean=0.0098, max=0.0597\n",
      "  classifier.layer2.1.conv2 (C=128):\n",
      "    Artifact disc: mean=0.2251, max=1.4720\n",
      "  classifier.layer2.1.bn2 (C=128):\n",
      "    Artifact disc: mean=0.0047, max=0.0549\n",
      "  classifier.layer2.1.conv3 (C=512):\n",
      "    Artifact disc: mean=0.0215, max=0.1228\n",
      "  classifier.layer2.1.bn3 (C=512):\n",
      "    Artifact disc: mean=0.0200, max=0.2498\n",
      "  classifier.layer2.2.conv1 (C=128):\n",
      "    Artifact disc: mean=0.3301, max=1.2925\n",
      "  classifier.layer2.2.bn1 (C=128):\n",
      "    Artifact disc: mean=0.0146, max=0.0772\n",
      "  classifier.layer2.2.conv2 (C=128):\n",
      "    Artifact disc: mean=0.3028, max=1.6443\n",
      "  classifier.layer2.2.bn2 (C=128):\n",
      "    Artifact disc: mean=0.0082, max=0.0475\n",
      "  classifier.layer2.2.conv3 (C=512):\n",
      "    Artifact disc: mean=0.0988, max=0.6714\n",
      "  classifier.layer2.2.bn3 (C=512):\n",
      "    Artifact disc: mean=0.0191, max=0.2486\n",
      "  classifier.layer2.3.conv1 (C=128):\n",
      "    Artifact disc: mean=0.2182, max=0.8349\n",
      "  classifier.layer2.3.bn1 (C=128):\n",
      "    Artifact disc: mean=0.0138, max=0.0792\n",
      "  classifier.layer2.3.conv2 (C=128):\n",
      "    Artifact disc: mean=0.5492, max=2.0702\n",
      "  classifier.layer2.3.bn2 (C=128):\n",
      "    Artifact disc: mean=0.0119, max=0.0663\n",
      "  classifier.layer2.3.conv3 (C=512):\n",
      "    Artifact disc: mean=0.2046, max=0.7087\n",
      "  classifier.layer2.3.bn3 (C=512):\n",
      "    Artifact disc: mean=0.0173, max=0.2419\n",
      "  classifier.layer3.0.conv1 (C=256):\n",
      "    Artifact disc: mean=0.5084, max=1.6790\n",
      "  classifier.layer3.0.bn1 (C=256):\n",
      "    Artifact disc: mean=0.0162, max=0.0896\n",
      "  classifier.layer3.0.conv2 (C=256):\n",
      "    Artifact disc: mean=3.4272, max=8.9090\n",
      "  classifier.layer3.0.bn2 (C=256):\n",
      "    Artifact disc: mean=0.0616, max=0.3244\n",
      "  classifier.layer3.0.conv3 (C=1024):\n",
      "    Artifact disc: mean=1.6148, max=5.1524\n",
      "  classifier.layer3.0.bn3 (C=1024):\n",
      "    Artifact disc: mean=0.0961, max=0.7878\n",
      "  classifier.layer3.0.downsample.0 (C=1024):\n",
      "    Artifact disc: mean=0.5039, max=2.3084\n",
      "  classifier.layer3.0.downsample.1 (C=1024):\n",
      "    Artifact disc: mean=0.0371, max=0.2871\n",
      "  classifier.layer3.1.conv1 (C=256):\n",
      "    Artifact disc: mean=9.1496, max=23.8022\n",
      "  classifier.layer3.1.bn1 (C=256):\n",
      "    Artifact disc: mean=0.0492, max=0.2624\n",
      "  classifier.layer3.1.conv2 (C=256):\n",
      "    Artifact disc: mean=6.2591, max=21.8688\n",
      "  classifier.layer3.1.bn2 (C=256):\n",
      "    Artifact disc: mean=0.0379, max=0.3608\n",
      "  classifier.layer3.1.conv3 (C=1024):\n",
      "    Artifact disc: mean=0.8168, max=3.4500\n",
      "  classifier.layer3.1.bn3 (C=1024):\n",
      "    Artifact disc: mean=0.0805, max=1.0113\n",
      "  classifier.layer3.2.conv1 (C=256):\n",
      "    Artifact disc: mean=5.8673, max=20.2012\n",
      "  classifier.layer3.2.bn1 (C=256):\n",
      "    Artifact disc: mean=0.0625, max=0.3592\n",
      "  classifier.layer3.2.conv2 (C=256):\n",
      "    Artifact disc: mean=5.8444, max=24.5080\n",
      "  classifier.layer3.2.bn2 (C=256):\n",
      "    Artifact disc: mean=0.0324, max=0.3780\n",
      "  classifier.layer3.2.conv3 (C=1024):\n",
      "    Artifact disc: mean=0.6223, max=2.2703\n",
      "  classifier.layer3.2.bn3 (C=1024):\n",
      "    Artifact disc: mean=0.0958, max=1.1149\n",
      "  classifier.layer3.3.conv1 (C=256):\n",
      "    Artifact disc: mean=5.2224, max=22.9360\n",
      "  classifier.layer3.3.bn1 (C=256):\n",
      "    Artifact disc: mean=0.0307, max=0.3873\n",
      "  classifier.layer3.3.conv2 (C=256):\n",
      "    Artifact disc: mean=2.0989, max=8.8247\n",
      "  classifier.layer3.3.bn2 (C=256):\n",
      "    Artifact disc: mean=0.0193, max=0.3151\n",
      "  classifier.layer3.3.conv3 (C=1024):\n",
      "    Artifact disc: mean=0.4466, max=1.8160\n",
      "  classifier.layer3.3.bn3 (C=1024):\n",
      "    Artifact disc: mean=0.0931, max=1.3337\n",
      "  classifier.layer3.4.conv1 (C=256):\n",
      "    Artifact disc: mean=7.0041, max=22.5921\n",
      "  classifier.layer3.4.bn1 (C=256):\n",
      "    Artifact disc: mean=0.0733, max=0.5341\n",
      "  classifier.layer3.4.conv2 (C=256):\n",
      "    Artifact disc: mean=4.7535, max=30.1397\n",
      "  classifier.layer3.4.bn2 (C=256):\n",
      "    Artifact disc: mean=0.0078, max=0.2819\n",
      "  classifier.layer3.4.conv3 (C=1024):\n",
      "    Artifact disc: mean=0.1055, max=0.6997\n",
      "  classifier.layer3.4.bn3 (C=1024):\n",
      "    Artifact disc: mean=0.0807, max=1.3343\n",
      "  classifier.layer3.5.conv1 (C=256):\n",
      "    Artifact disc: mean=3.7051, max=15.4026\n",
      "  classifier.layer3.5.bn1 (C=256):\n",
      "    Artifact disc: mean=0.0329, max=0.3866\n",
      "  classifier.layer3.5.conv2 (C=256):\n",
      "    Artifact disc: mean=1.8479, max=8.7370\n",
      "  classifier.layer3.5.bn2 (C=256):\n",
      "    Artifact disc: mean=0.0170, max=0.2584\n",
      "  classifier.layer3.5.conv3 (C=1024):\n",
      "    Artifact disc: mean=0.3841, max=1.5428\n",
      "  classifier.layer3.5.bn3 (C=1024):\n",
      "    Artifact disc: mean=0.0736, max=1.3804\n",
      "  classifier.layer4.0.conv1 (C=512):\n",
      "    Artifact disc: mean=4.5690, max=21.4780\n",
      "  classifier.layer4.0.bn1 (C=512):\n",
      "    Artifact disc: mean=0.0400, max=0.6140\n",
      "  classifier.layer4.0.conv2 (C=512):\n",
      "    Artifact disc: mean=11.9316, max=41.0039\n",
      "  classifier.layer4.0.bn2 (C=512):\n",
      "    Artifact disc: mean=0.0613, max=0.5719\n",
      "  classifier.layer4.0.conv3 (C=2048):\n",
      "    Artifact disc: mean=3.8554, max=12.1766\n",
      "  classifier.layer4.0.bn3 (C=2048):\n",
      "    Artifact disc: mean=0.1975, max=1.9232\n",
      "  classifier.layer4.0.downsample.0 (C=2048):\n",
      "    Artifact disc: mean=6.5633, max=22.6370\n",
      "  classifier.layer4.0.downsample.1 (C=2048):\n",
      "    Artifact disc: mean=0.2407, max=2.1809\n",
      "  classifier.layer4.1.conv1 (C=512):\n",
      "    Artifact disc: mean=17.4977, max=95.3970\n",
      "  classifier.layer4.1.bn1 (C=512):\n",
      "    Artifact disc: mean=0.0308, max=0.5986\n",
      "  classifier.layer4.1.conv2 (C=512):\n",
      "    Artifact disc: mean=8.9025, max=30.4236\n",
      "  classifier.layer4.1.bn2 (C=512):\n",
      "    Artifact disc: mean=0.0110, max=0.5101\n",
      "  classifier.layer4.1.conv3 (C=2048):\n",
      "    Artifact disc: mean=0.6057, max=1.8454\n",
      "  classifier.layer4.1.bn3 (C=2048):\n",
      "    Artifact disc: mean=0.2194, max=2.3674\n",
      "  classifier.layer4.2.conv1 (C=512):\n",
      "    Artifact disc: mean=8.7317, max=105.0210\n",
      "  classifier.layer4.2.bn1 (C=512):\n",
      "    Artifact disc: mean=0.0145, max=0.6322\n",
      "  classifier.layer4.2.conv2 (C=512):\n",
      "    Artifact disc: mean=3.6970, max=13.5926\n",
      "  classifier.layer4.2.bn2 (C=512):\n",
      "    Artifact disc: mean=0.0101, max=0.4700\n",
      "  classifier.layer4.2.conv3 (C=2048):\n",
      "    Artifact disc: mean=0.6284, max=1.6526\n",
      "  classifier.layer4.2.bn3 (C=2048):\n",
      "    Artifact disc: mean=0.2949, max=2.1141\n",
      "[CPv2] Initialized for LGrad\n",
      "[CPv2] Target layers: 106\n",
      "[CPv2] Keep ratio: 70.00% (learnable=False)\n",
      "[CPv2] Gating type: hard\n",
      "[CPv2] Z-score normalization: True\n",
      "\n",
      "Channel Pruning v1 (IMPROVED) model created!\n"
     ]
    }
   ],
   "source": [
    "# Config 설정 (IMPROVED!)\n",
    "config_v2 = CPv2Config(\n",
    "    model=\"LGrad\",\n",
    "    keep_ratio=0.7,  # Keep top 70% channels\n",
    "    gating_type=\"hard\",  # Hard gating (0 or 1)\n",
    "    use_zscore=True,  # Z-score normalization\n",
    ")\n",
    "\n",
    "# Model 생성\n",
    "model = UnifiedChannelPruningV2(\n",
    "    base_model=base_model,\n",
    "    separated_stats=separated_stats_v2,\n",
    "    config=config_v2,\n",
    ")\n",
    "\n",
    "print(\"\\nChannel Pruning v1 (IMPROVED) model created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Test-Time Adaptation\n",
    "\n",
    "Noisy validation data로 temperature와 channel bias를 fine-tuning할 수 있습니다.\n",
    "\n",
    "**Skip 가능!** Adaptation 없이도 사용 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adaptation을 원하면 주석 해제\n",
    "# ENABLE_ADAPTATION = True\n",
    "\n",
    "# if ENABLE_ADAPTATION:\n",
    "#     # Noisy validation data 준비\n",
    "#     progan_noisy_indices = [\n",
    "#         i for i, s in enumerate(dataset.samples)\n",
    "#         if s['dataset'] == \"corrupted_test_data_progan\" and s['corruption'] == \"gaussian_noise\"\n",
    "#     ]\n",
    "    \n",
    "#     print(f\"ProGAN noisy samples for adaptation: {len(progan_noisy_indices)}\")\n",
    "    \n",
    "#     noisy_subset = Subset(dataset, progan_noisy_indices[:500])\n",
    "#     noisy_loader = DataLoader(\n",
    "#         noisy_subset,\n",
    "#         batch_size=32,\n",
    "#         shuffle=True,\n",
    "#         num_workers=4,\n",
    "#         drop_last=False\n",
    "#     )\n",
    "    \n",
    "#     # Adaptation 실행\n",
    "#     print(\"\\nStarting test-time adaptation...\\n\")\n",
    "#     model.adapt(\n",
    "#         dataloader=noisy_loader,\n",
    "#         epochs=5,\n",
    "#         lr=1e-4,\n",
    "#         verbose=True,\n",
    "#     )\n",
    "#     print(\"\\nAdaptation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Dataset별, Corruption별로 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_progan-original\n",
      "샘플 수: 8000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_progan-original: 100%|██████████| 500/500 [02:11<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 51.31%\n",
      "  AUC:      55.00%\n",
      "  AP:       55.01%\n",
      "  F1:       11.01%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_progan-gaussian_noise\n",
      "샘플 수: 8000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_progan-gaussian_noise: 100%|██████████| 500/500 [02:14<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 49.00%\n",
      "  AUC:      50.10%\n",
      "  AP:       50.13%\n",
      "  F1:       27.32%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_progan-jpeg_compression\n",
      "샘플 수: 8000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_progan-jpeg_compression: 100%|██████████| 500/500 [02:20<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 50.35%\n",
      "  AUC:      50.41%\n",
      "  AP:       50.27%\n",
      "  F1:       8.61%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_stylegan-original\n",
      "샘플 수: 11982\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_stylegan-original: 100%|██████████| 748/748 [03:19<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 52.65%\n",
      "  AUC:      57.24%\n",
      "  AP:       58.33%\n",
      "  F1:       15.10%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_stylegan-gaussian_noise\n",
      "샘플 수: 11982\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_stylegan-gaussian_noise: 100%|██████████| 748/748 [03:17<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 50.08%\n",
      "  AUC:      48.56%\n",
      "  AP:       49.66%\n",
      "  F1:       30.66%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_stylegan-jpeg_compression\n",
      "샘플 수: 11982\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_stylegan-jpeg_compression: 100%|██████████| 748/748 [03:17<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 50.15%\n",
      "  AUC:      48.51%\n",
      "  AP:       49.65%\n",
      "  F1:       5.90%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_stylegan2-original\n",
      "샘플 수: 15976\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_stylegan2-original: 100%|██████████| 998/998 [04:25<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 50.58%\n",
      "  AUC:      53.34%\n",
      "  AP:       52.93%\n",
      "  F1:       9.85%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_stylegan2-gaussian_noise\n",
      "샘플 수: 15976\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_stylegan2-gaussian_noise: 100%|██████████| 998/998 [04:22<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 48.75%\n",
      "  AUC:      48.98%\n",
      "  AP:       48.55%\n",
      "  F1:       25.78%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_stylegan2-jpeg_compression\n",
      "샘플 수: 15976\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_stylegan2-jpeg_compression: 100%|██████████| 998/998 [04:28<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과:\n",
      "  Accuracy: 49.69%\n",
      "  AUC:      48.19%\n",
      "  AP:       48.35%\n",
      "  F1:       5.82%\n",
      "\n",
      "============================================================\n",
      "평가 중: corrupted_test_data_biggan-original\n",
      "샘플 수: 4000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_biggan-original:  36%|███▌      | 90/250 [00:24<00:43,  3.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     23\u001b[39m dataloader = DataLoader(\n\u001b[32m     24\u001b[39m     subset,\n\u001b[32m     25\u001b[39m     batch_size=\u001b[32m16\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 평가\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m metrics = \u001b[43mcalc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcorruption\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# 즉시 결과 출력\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m결과:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/utils/eval/metrics.py:164\u001b[39m, in \u001b[36mMetricsCalculator.evaluate\u001b[39m\u001b[34m(self, model, dataloader, device, name)\u001b[39m\n\u001b[32m    161\u001b[39m images.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Forward pass (model.eval() prevents parameter updates)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Handle different output formats\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outputs.min() < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m outputs.max() > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/method/channel_pruningv2.py:598\u001b[39m, in \u001b[36mUnifiedChannelPruningV2.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.model == \u001b[33m\"\u001b[39m\u001b[33mLGrad\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# NPR\u001b[39;00m\n\u001b[32m    600\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.model(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/LGrad/lgrad_model.py:152\u001b[39m, in \u001b[36mLGrad.forward\u001b[39m\u001b[34m(self, x, return_grad)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    x: Input images [B, 3, H, W], range [0, 1]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m \u001b[33;03m    grad (optional): [B, 3, 256, 256] gradient images\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    151\u001b[39m grad = \u001b[38;5;28mself\u001b[39m.img2grad(x)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_grad:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/LGrad/lgrad_model.py:135\u001b[39m, in \u001b[36mLGrad.classify\u001b[39m\u001b[34m(self, grad)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03mclassification of Gradient image\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    132\u001b[39m \u001b[33;03m    Logits [B, 1] (positive = fake)\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    134\u001b[39m x = \u001b[38;5;28mself\u001b[39m.grad2clf_transform(grad)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/LGrad/lgrad/CNNDetection/networks/resnet.py:155\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    152\u001b[39m x = \u001b[38;5;28mself\u001b[39m.relu(x)\n\u001b[32m    153\u001b[39m x = \u001b[38;5;28mself\u001b[39m.maxpool(x)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer2(x)\n\u001b[32m    157\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer3(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/LGrad/lgrad/CNNDetection/networks/resnet.py:87\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     84\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn2(out)\n\u001b[32m     85\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu(out)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn3(out)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1844\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1843\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1849\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1803\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1801\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1802\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1803\u001b[39m     hook_result = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1806\u001b[39m     result = hook_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/method/channel_pruningv2.py:582\u001b[39m, in \u001b[36mUnifiedChannelPruningV2._setup_gates.<locals>.make_hook.<locals>.hook\u001b[39m\u001b[34m(module, input, output)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhook\u001b[39m(module, \u001b[38;5;28minput\u001b[39m, output):\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     gated_output, gate_weights = \u001b[43mgate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m gated_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/method/channel_pruningv2.py:419\u001b[39m, in \u001b[36mRobustChannelGating.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[33;03mApply percentile-based robust channel gating\u001b[39;00m\n\u001b[32m    410\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m \u001b[33;03m    gate: [C] - binary gate values (0 or 1 for hard, 0~1 for soft)\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# Step 1: Robustness score (작을수록 robust)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m robustness = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_robustness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, C]\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# Step 2: Artifact discriminability (클수록 중요)\u001b[39;00m\n\u001b[32m    422\u001b[39m disc = \u001b[38;5;28mself\u001b[39m.artifact_discriminability.unsqueeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# [1, C]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/robust_deepfake_ai/model/method/channel_pruningv2.py:380\u001b[39m, in \u001b[36mRobustChannelGating.compute_robustness\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    378\u001b[39m real_mean = \u001b[38;5;28mself\u001b[39m.separated_stats[\u001b[33m'\u001b[39m\u001b[33mreal\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m].to(x.device)  \u001b[38;5;66;03m# [C]\u001b[39;00m\n\u001b[32m    379\u001b[39m fake_mean = \u001b[38;5;28mself\u001b[39m.separated_stats[\u001b[33m'\u001b[39m\u001b[33mfake\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m].to(x.device)  \u001b[38;5;66;03m# [C]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m real_std = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseparated_stats\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstd\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [C]\u001b[39;00m\n\u001b[32m    381\u001b[39m fake_std = \u001b[38;5;28mself\u001b[39m.separated_stats[\u001b[33m'\u001b[39m\u001b[33mfake\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m'\u001b[39m].to(x.device)  \u001b[38;5;66;03m# [C]\u001b[39;00m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_zscore:\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# Z-score normalization (batch variance 고려!)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "calc = MetricsCalculator()\n",
    "all_results = {}\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    for corruption in CORRUPTIONS:\n",
    "        combination_indices = [\n",
    "            i for i, s in enumerate(dataset.samples)\n",
    "            if s['dataset'] == dataset_name and s['corruption'] == corruption\n",
    "        ]\n",
    "        \n",
    "        if len(combination_indices) == 0:\n",
    "            print(f\"{dataset_name}-{corruption}: 샘플 없음, 스킵\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"평가 중: {dataset_name}-{corruption}\")\n",
    "        print(f\"샘플 수: {len(combination_indices)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Subset과 DataLoader 생성\n",
    "        subset = Subset(dataset, combination_indices)\n",
    "        dataloader = DataLoader(\n",
    "            subset,\n",
    "            batch_size=16,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        # 평가\n",
    "        metrics = calc.evaluate(\n",
    "            model=model,\n",
    "            dataloader=dataloader,\n",
    "            device=DEVICE,\n",
    "            name=f\"{dataset_name}-{corruption}\"\n",
    "        )\n",
    "        \n",
    "        # 즉시 결과 출력\n",
    "        print(f\"\\n결과:\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "        print(f\"  AUC:      {metrics['auc']*100:.2f}%\")\n",
    "        print(f\"  AP:       {metrics['ap']*100:.2f}%\")\n",
    "        print(f\"  F1:       {metrics['f1']*100:.2f}%\")\n",
    "        \n",
    "        # 결과 저장\n",
    "        all_results[(dataset_name, corruption)] = metrics\n",
    "\n",
    "# 전체 결과 테이블 출력\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"전체 결과 요약\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "calc.print_results_table()\n",
    "calc.summarize_by_corruption(all_results)\n",
    "calc.summarize_by_dataset(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling all gates (forcing gate=1.0)...\n",
      "All gates disabled. Re-evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no-gating-test: 100%|██████████| 500/500 [02:00<00:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With gating disabled:\n",
      "  Accuracy: 99.76%\n",
      "  AUC:      99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gating 강제로 끄기 (모든 채널 사용)\n",
    "print(\"Disabling all gates (forcing gate=1.0)...\")\n",
    "\n",
    "for gate_module in model.gates.values():\n",
    "    # Temperature를 0으로 설정 + bias를 매우 크게\n",
    "    gate_module.temperature.data.fill_(0.0)\n",
    "    gate_module.channel_bias.data.fill_(100.0)  # sigmoid(100) ≈ 1.0\n",
    "\n",
    "print(\"All gates disabled. Re-evaluating...\")\n",
    "\n",
    "# ProGAN original 다시 평가\n",
    "progan_orig_indices = [i for i, s in enumerate(dataset.samples)\n",
    "                        if s['dataset'] == \"corrupted_test_data_progan\"\n",
    "                        and s['corruption'] == \"original\"]\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Subset(dataset, progan_orig_indices),\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "calc = MetricsCalculator()\n",
    "metrics = calc.evaluate(model, test_loader, DEVICE, \"no-gating-test\")\n",
    "\n",
    "print(f\"\\nWith gating disabled:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"  AUC:      {metrics['auc']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Artifact Discriminability (|fake_mean - real_mean|) 확인\n",
      "============================================================\n",
      "\n",
      "classifier.conv1:\n",
      "  Artifact signature: mean=0.006160, max=0.052292, min=0.000000\n",
      "  Real mean: min=-6.3340, max=7.3021, avg=-0.0532\n",
      "  Fake mean: min=-6.3223, max=7.2592, avg=-0.0534\n",
      "\n",
      "classifier.bn1:\n",
      "  Artifact signature: mean=0.001062, max=0.004508, min=0.000000\n",
      "  Real mean: min=0.0000, max=2.0776, avg=0.4093\n",
      "  Fake mean: min=0.0000, max=2.0739, avg=0.4099\n",
      "\n",
      "classifier.layer1.0.conv1:\n",
      "  Artifact signature: mean=0.040236, max=0.152285, min=0.000000\n",
      "  Real mean: min=-7.1698, max=6.7641, avg=-0.6328\n",
      "  Fake mean: min=-7.3122, max=6.9164, avg=-0.6532\n",
      "\n",
      "classifier.layer1.0.bn1:\n",
      "  Artifact signature: mean=0.008797, max=0.043005, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.1708, avg=0.1708\n",
      "  Fake mean: min=0.0000, max=1.1761, avg=0.1664\n",
      "\n",
      "classifier.layer1.0.conv2:\n",
      "  Artifact signature: mean=0.138221, max=0.634240, min=0.000000\n",
      "  Real mean: min=-6.2954, max=7.5667, avg=0.3800\n",
      "  Fake mean: min=-6.2174, max=7.4683, avg=0.2925\n",
      "\n",
      "classifier.layer1.0.bn2:\n",
      "  Artifact signature: mean=0.004441, max=0.022851, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.6930, avg=0.3566\n",
      "  Fake mean: min=0.0000, max=1.6927, avg=0.3532\n",
      "\n",
      "classifier.layer1.0.conv3:\n",
      "  Artifact signature: mean=0.014271, max=0.069504, min=0.000000\n",
      "  Real mean: min=-2.2648, max=2.7520, avg=-0.0384\n",
      "  Fake mean: min=-2.2287, max=2.7184, avg=-0.0392\n",
      "\n",
      "classifier.layer1.0.bn3:\n",
      "  Artifact signature: mean=0.007128, max=0.069318, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.2194, avg=0.1586\n",
      "  Fake mean: min=0.0000, max=1.2702, avg=0.1557\n",
      "\n",
      "classifier.layer1.0.downsample.0:\n",
      "  Artifact signature: mean=0.032858, max=0.169576, min=0.000000\n",
      "  Real mean: min=-8.1524, max=6.9612, avg=-0.7878\n",
      "  Fake mean: min=-8.3097, max=7.0982, avg=-0.8061\n",
      "\n",
      "classifier.layer1.0.downsample.1:\n",
      "  Artifact signature: mean=0.012633, max=0.064510, min=0.000000\n",
      "  Real mean: min=-0.5039, max=0.5432, avg=-0.0341\n",
      "  Fake mean: min=-0.4969, max=0.5940, avg=-0.0410\n",
      "\n",
      "classifier.layer1.1.conv1:\n",
      "  Artifact signature: mean=0.109683, max=0.363508, min=0.000000\n",
      "  Real mean: min=-3.4729, max=2.1518, avg=-0.0674\n",
      "  Fake mean: min=-3.5351, max=2.1794, avg=-0.1101\n",
      "\n",
      "classifier.layer1.1.bn1:\n",
      "  Artifact signature: mean=0.009597, max=0.070184, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.8371, avg=0.1324\n",
      "  Fake mean: min=0.0000, max=0.8421, avg=0.1307\n",
      "\n",
      "classifier.layer1.1.conv2:\n",
      "  Artifact signature: mean=0.149446, max=0.631307, min=0.011085\n",
      "  Real mean: min=-6.5360, max=4.7898, avg=-1.1022\n",
      "  Fake mean: min=-6.5824, max=4.4466, avg=-1.1114\n",
      "\n",
      "classifier.layer1.1.bn2:\n",
      "  Artifact signature: mean=0.005540, max=0.042646, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.7506, avg=0.1226\n",
      "  Fake mean: min=0.0000, max=0.7497, avg=0.1226\n",
      "\n",
      "classifier.layer1.1.conv3:\n",
      "  Artifact signature: mean=0.025543, max=0.182522, min=0.000000\n",
      "  Real mean: min=-1.2426, max=1.3051, avg=-0.0357\n",
      "  Fake mean: min=-1.2353, max=1.2885, avg=-0.0339\n",
      "\n",
      "classifier.layer1.1.bn3:\n",
      "  Artifact signature: mean=0.007447, max=0.063873, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.6158, avg=0.1964\n",
      "  Fake mean: min=0.0000, max=1.6104, avg=0.1933\n",
      "\n",
      "classifier.layer1.2.conv1:\n",
      "  Artifact signature: mean=0.077064, max=0.345442, min=0.001682\n",
      "  Real mean: min=-3.4516, max=3.3084, avg=-0.3383\n",
      "  Fake mean: min=-3.5179, max=3.2457, avg=-0.3751\n",
      "\n",
      "classifier.layer1.2.bn1:\n",
      "  Artifact signature: mean=0.006088, max=0.035817, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.6684, avg=0.1150\n",
      "  Fake mean: min=0.0000, max=0.6936, avg=0.1151\n",
      "\n",
      "classifier.layer1.2.conv2:\n",
      "  Artifact signature: mean=0.060573, max=0.265571, min=0.003946\n",
      "  Real mean: min=-3.4200, max=5.0892, avg=-1.0897\n",
      "  Fake mean: min=-3.3503, max=5.1617, avg=-1.0765\n",
      "\n",
      "classifier.layer1.2.bn2:\n",
      "  Artifact signature: mean=0.004374, max=0.021055, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.5974, avg=0.0612\n",
      "  Fake mean: min=0.0000, max=0.5938, avg=0.0618\n",
      "\n",
      "classifier.layer1.2.conv3:\n",
      "  Artifact signature: mean=0.015048, max=0.092575, min=0.000031\n",
      "  Real mean: min=-0.6505, max=0.6515, avg=-0.0377\n",
      "  Fake mean: min=-0.6752, max=0.6494, avg=-0.0394\n",
      "\n",
      "classifier.layer1.2.bn3:\n",
      "  Artifact signature: mean=0.007336, max=0.066601, min=0.000000\n",
      "  Real mean: min=0.0000, max=2.1957, avg=0.2001\n",
      "  Fake mean: min=0.0000, max=2.2000, avg=0.1978\n",
      "\n",
      "classifier.layer2.0.conv1:\n",
      "  Artifact signature: mean=0.067525, max=0.325496, min=0.000030\n",
      "  Real mean: min=-4.4830, max=7.3265, avg=-0.9393\n",
      "  Fake mean: min=-4.4739, max=7.2719, avg=-0.9453\n",
      "\n",
      "classifier.layer2.0.bn1:\n",
      "  Artifact signature: mean=0.004364, max=0.033985, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.2587, avg=0.0270\n",
      "  Fake mean: min=0.0000, max=0.2732, avg=0.0271\n",
      "\n",
      "classifier.layer2.0.conv2:\n",
      "  Artifact signature: mean=0.296765, max=1.043208, min=0.004678\n",
      "  Real mean: min=-3.4414, max=1.9123, avg=-0.5239\n",
      "  Fake mean: min=-3.9866, max=2.3522, avg=-0.5123\n",
      "\n",
      "classifier.layer2.0.bn2:\n",
      "  Artifact signature: mean=0.022333, max=0.171617, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.5642, avg=0.0775\n",
      "  Fake mean: min=0.0000, max=0.5407, avg=0.0738\n",
      "\n",
      "classifier.layer2.0.conv3:\n",
      "  Artifact signature: mean=0.180663, max=0.673760, min=0.000000\n",
      "  Real mean: min=-2.0196, max=2.0433, avg=-0.0802\n",
      "  Fake mean: min=-1.6480, max=1.6331, avg=-0.0443\n",
      "\n",
      "classifier.layer2.0.bn3:\n",
      "  Artifact signature: mean=0.020638, max=0.249950, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.1953, avg=0.1481\n",
      "  Fake mean: min=0.0000, max=1.2367, avg=0.1465\n",
      "\n",
      "classifier.layer2.0.downsample.0:\n",
      "  Artifact signature: mean=0.059210, max=0.351812, min=0.000000\n",
      "  Real mean: min=-4.6705, max=7.3776, avg=-0.3643\n",
      "  Fake mean: min=-4.6031, max=7.2968, avg=-0.3796\n",
      "\n",
      "classifier.layer2.0.downsample.1:\n",
      "  Artifact signature: mean=0.012955, max=0.105944, min=0.000000\n",
      "  Real mean: min=-0.4170, max=0.6014, avg=-0.0335\n",
      "  Fake mean: min=-0.4122, max=0.6340, avg=-0.0366\n",
      "\n",
      "classifier.layer2.1.conv1:\n",
      "  Artifact signature: mean=0.371363, max=1.549579, min=0.000578\n",
      "  Real mean: min=-6.6032, max=8.9350, avg=0.0345\n",
      "  Fake mean: min=-6.2894, max=8.5090, avg=0.1804\n",
      "\n",
      "classifier.layer2.1.bn1:\n",
      "  Artifact signature: mean=0.009787, max=0.059697, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.6601, avg=0.1247\n",
      "  Fake mean: min=0.0000, max=0.6647, avg=0.1270\n",
      "\n",
      "classifier.layer2.1.conv2:\n",
      "  Artifact signature: mean=0.225084, max=1.471972, min=0.006662\n",
      "  Real mean: min=-16.8201, max=9.8765, avg=-1.1937\n",
      "  Fake mean: min=-18.2920, max=10.5327, avg=-1.2153\n",
      "\n",
      "classifier.layer2.1.bn2:\n",
      "  Artifact signature: mean=0.004656, max=0.054878, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.0938, avg=0.1338\n",
      "  Fake mean: min=0.0000, max=1.0943, avg=0.1326\n",
      "\n",
      "classifier.layer2.1.conv3:\n",
      "  Artifact signature: mean=0.021467, max=0.122756, min=0.000000\n",
      "  Real mean: min=-2.4357, max=2.3661, avg=-0.0948\n",
      "  Fake mean: min=-2.4989, max=2.3675, avg=-0.0966\n",
      "\n",
      "classifier.layer2.1.bn3:\n",
      "  Artifact signature: mean=0.020007, max=0.249845, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.6702, avg=0.1404\n",
      "  Fake mean: min=0.0000, max=1.7089, avg=0.1383\n",
      "\n",
      "classifier.layer2.2.conv1:\n",
      "  Artifact signature: mean=0.330105, max=1.292479, min=0.004082\n",
      "  Real mean: min=-5.8644, max=5.0932, avg=-1.0452\n",
      "  Fake mean: min=-5.8262, max=4.6631, avg=-1.0833\n",
      "\n",
      "classifier.layer2.2.bn1:\n",
      "  Artifact signature: mean=0.014550, max=0.077188, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.4293, avg=0.1212\n",
      "  Fake mean: min=0.0000, max=0.4707, avg=0.1181\n",
      "\n",
      "classifier.layer2.2.conv2:\n",
      "  Artifact signature: mean=0.302832, max=1.644287, min=0.001590\n",
      "  Real mean: min=-11.2769, max=13.9712, avg=-1.2518\n",
      "  Fake mean: min=-11.3197, max=13.2486, avg=-1.2967\n",
      "\n",
      "classifier.layer2.2.bn2:\n",
      "  Artifact signature: mean=0.008210, max=0.047483, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.6343, avg=0.0748\n",
      "  Fake mean: min=0.0000, max=0.6125, avg=0.0717\n",
      "\n",
      "classifier.layer2.2.conv3:\n",
      "  Artifact signature: mean=0.098754, max=0.671351, min=0.000372\n",
      "  Real mean: min=-1.3138, max=1.2081, avg=-0.0181\n",
      "  Fake mean: min=-1.0942, max=1.0907, avg=0.0097\n",
      "\n",
      "classifier.layer2.2.bn3:\n",
      "  Artifact signature: mean=0.019095, max=0.248587, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.8837, avg=0.1197\n",
      "  Fake mean: min=0.0000, max=1.9105, avg=0.1173\n",
      "\n",
      "classifier.layer2.3.conv1:\n",
      "  Artifact signature: mean=0.218210, max=0.834892, min=0.000792\n",
      "  Real mean: min=-4.9806, max=3.5893, avg=-0.7868\n",
      "  Fake mean: min=-5.4120, max=3.4153, avg=-0.7938\n",
      "\n",
      "classifier.layer2.3.bn1:\n",
      "  Artifact signature: mean=0.013817, max=0.079206, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.5354, avg=0.1387\n",
      "  Fake mean: min=0.0000, max=0.5418, avg=0.1335\n",
      "\n",
      "classifier.layer2.3.conv2:\n",
      "  Artifact signature: mean=0.549221, max=2.070206, min=0.001971\n",
      "  Real mean: min=-15.5063, max=12.0718, avg=-2.1145\n",
      "  Fake mean: min=-13.8219, max=10.2407, avg=-2.0097\n",
      "\n",
      "classifier.layer2.3.bn2:\n",
      "  Artifact signature: mean=0.011916, max=0.066257, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.5110, avg=0.0530\n",
      "  Fake mean: min=0.0000, max=0.5230, avg=0.0493\n",
      "\n",
      "classifier.layer2.3.conv3:\n",
      "  Artifact signature: mean=0.204595, max=0.708648, min=0.000068\n",
      "  Real mean: min=-1.6463, max=1.4426, avg=-0.1419\n",
      "  Fake mean: min=-1.7698, max=1.5448, avg=-0.0734\n",
      "\n",
      "classifier.layer2.3.bn3:\n",
      "  Artifact signature: mean=0.017337, max=0.241893, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.9312, avg=0.0910\n",
      "  Fake mean: min=0.0000, max=1.9489, avg=0.0901\n",
      "\n",
      "classifier.layer3.0.conv1:\n",
      "  Artifact signature: mean=0.508447, max=1.678991, min=0.005013\n",
      "  Real mean: min=-7.6368, max=5.0978, avg=-0.2638\n",
      "  Fake mean: min=-8.0895, max=4.4133, avg=-0.1658\n",
      "\n",
      "classifier.layer3.0.bn1:\n",
      "  Artifact signature: mean=0.016169, max=0.089555, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.3833, avg=0.0320\n",
      "  Fake mean: min=0.0000, max=0.3688, avg=0.0319\n",
      "\n",
      "classifier.layer3.0.conv2:\n",
      "  Artifact signature: mean=3.427187, max=8.909027, min=0.001123\n",
      "  Real mean: min=-11.8043, max=8.3323, avg=-2.2709\n",
      "  Fake mean: min=-14.9955, max=3.0490, avg=-2.3051\n",
      "\n",
      "classifier.layer3.0.bn2:\n",
      "  Artifact signature: mean=0.061598, max=0.324446, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.7759, avg=0.1191\n",
      "  Fake mean: min=0.0000, max=0.6603, avg=0.1127\n",
      "\n",
      "classifier.layer3.0.conv3:\n",
      "  Artifact signature: mean=1.614803, max=5.152377, min=0.000000\n",
      "  Real mean: min=-7.0871, max=7.7914, avg=-0.0105\n",
      "  Fake mean: min=-6.3145, max=5.1759, avg=0.1837\n",
      "\n",
      "classifier.layer3.0.bn3:\n",
      "  Artifact signature: mean=0.096079, max=0.787804, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.5387, avg=0.1631\n",
      "  Fake mean: min=0.0000, max=1.5392, avg=0.1580\n",
      "\n",
      "classifier.layer3.0.downsample.0:\n",
      "  Artifact signature: mean=0.503924, max=2.308414, min=0.000000\n",
      "  Real mean: min=-6.9640, max=5.8730, avg=-0.4068\n",
      "  Fake mean: min=-7.0830, max=5.6139, avg=-0.3314\n",
      "\n",
      "classifier.layer3.0.downsample.1:\n",
      "  Artifact signature: mean=0.037089, max=0.287095, min=0.000000\n",
      "  Real mean: min=-0.4391, max=0.5803, avg=-0.0360\n",
      "  Fake mean: min=-0.4438, max=0.5980, avg=-0.0288\n",
      "\n",
      "classifier.layer3.1.conv1:\n",
      "  Artifact signature: mean=9.149601, max=23.802191, min=0.005644\n",
      "  Real mean: min=-19.8754, max=16.4126, avg=-2.8713\n",
      "  Fake mean: min=-28.9168, max=18.1834, avg=-4.9904\n",
      "\n",
      "classifier.layer3.1.bn1:\n",
      "  Artifact signature: mean=0.049151, max=0.262441, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.7424, avg=0.1197\n",
      "  Fake mean: min=0.0000, max=0.6521, avg=0.0920\n",
      "\n",
      "classifier.layer3.1.conv2:\n",
      "  Artifact signature: mean=6.259137, max=21.868786, min=0.005547\n",
      "  Real mean: min=-44.9926, max=22.8978, avg=-10.2118\n",
      "  Fake mean: min=-34.2872, max=25.9957, avg=-5.3067\n",
      "\n",
      "classifier.layer3.1.bn2:\n",
      "  Artifact signature: mean=0.037887, max=0.360836, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.4287, avg=0.0554\n",
      "  Fake mean: min=0.0000, max=0.5867, avg=0.0748\n",
      "\n",
      "classifier.layer3.1.conv3:\n",
      "  Artifact signature: mean=0.816845, max=3.449968, min=0.000010\n",
      "  Real mean: min=-2.8144, max=2.1861, avg=0.0549\n",
      "  Fake mean: min=-4.8102, max=4.8278, avg=0.0382\n",
      "\n",
      "classifier.layer3.1.bn3:\n",
      "  Artifact signature: mean=0.080469, max=1.011302, min=0.000000\n",
      "  Real mean: min=0.0000, max=1.7219, avg=0.1563\n",
      "  Fake mean: min=0.0000, max=1.7226, avg=0.1545\n",
      "\n",
      "classifier.layer3.2.conv1:\n",
      "  Artifact signature: mean=5.867269, max=20.201172, min=0.002540\n",
      "  Real mean: min=-28.1077, max=15.3036, avg=-5.8337\n",
      "  Fake mean: min=-12.7548, max=14.2654, avg=-3.7401\n",
      "\n",
      "classifier.layer3.2.bn1:\n",
      "  Artifact signature: mean=0.062526, max=0.359195, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.6781, avg=0.0890\n",
      "  Fake mean: min=0.0000, max=0.6770, avg=0.1212\n",
      "\n",
      "classifier.layer3.2.conv2:\n",
      "  Artifact signature: mean=5.844403, max=24.507938, min=0.018515\n",
      "  Real mean: min=-23.3786, max=19.4579, avg=-5.7022\n",
      "  Fake mean: min=-40.1271, max=22.9216, avg=-7.0423\n",
      "\n",
      "classifier.layer3.2.bn2:\n",
      "  Artifact signature: mean=0.032404, max=0.377968, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.7140, avg=0.0523\n",
      "  Fake mean: min=0.0000, max=0.6606, avg=0.0592\n",
      "\n",
      "classifier.layer3.2.conv3:\n",
      "  Artifact signature: mean=0.622308, max=2.270295, min=0.000254\n",
      "  Real mean: min=-2.2057, max=2.9870, avg=-0.0514\n",
      "  Fake mean: min=-3.1281, max=2.8628, avg=-0.1176\n",
      "\n",
      "classifier.layer3.2.bn3:\n",
      "  Artifact signature: mean=0.095798, max=1.114943, min=0.000000\n",
      "  Real mean: min=0.0000, max=2.2350, avg=0.1818\n",
      "  Fake mean: min=0.0000, max=2.1136, avg=0.1786\n",
      "\n",
      "classifier.layer3.3.conv1:\n",
      "  Artifact signature: mean=5.222344, max=22.935982, min=0.003066\n",
      "  Real mean: min=-27.6452, max=18.0739, avg=-5.2905\n",
      "  Fake mean: min=-14.3773, max=15.4560, avg=-2.9481\n",
      "\n",
      "classifier.layer3.3.bn1:\n",
      "  Artifact signature: mean=0.030692, max=0.387301, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.4518, avg=0.0347\n",
      "  Fake mean: min=0.0000, max=0.8391, avg=0.0591\n",
      "\n",
      "classifier.layer3.3.conv2:\n",
      "  Artifact signature: mean=2.098914, max=8.824677, min=0.002721\n",
      "  Real mean: min=-11.6873, max=4.6812, avg=-1.9061\n",
      "  Fake mean: min=-20.0137, max=10.0927, avg=-3.4590\n",
      "\n",
      "classifier.layer3.3.bn2:\n",
      "  Artifact signature: mean=0.019287, max=0.315080, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.5536, avg=0.0359\n",
      "  Fake mean: min=0.0000, max=0.5289, avg=0.0334\n",
      "\n",
      "classifier.layer3.3.conv3:\n",
      "  Artifact signature: mean=0.446612, max=1.816047, min=0.000002\n",
      "  Real mean: min=-2.6677, max=2.7689, avg=0.0881\n",
      "  Fake mean: min=-2.6934, max=2.4733, avg=0.0119\n",
      "\n",
      "classifier.layer3.3.bn3:\n",
      "  Artifact signature: mean=0.093146, max=1.333650, min=0.000000\n",
      "  Real mean: min=0.0000, max=2.7390, avg=0.1690\n",
      "  Fake mean: min=0.0000, max=2.5915, avg=0.1702\n",
      "\n",
      "classifier.layer3.4.conv1:\n",
      "  Artifact signature: mean=7.004134, max=22.592058, min=0.017373\n",
      "  Real mean: min=-24.6389, max=16.4596, avg=-2.7960\n",
      "  Fake mean: min=-17.6364, max=16.6590, avg=-0.6063\n",
      "\n",
      "classifier.layer3.4.bn1:\n",
      "  Artifact signature: mean=0.073334, max=0.534135, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.8104, avg=0.0777\n",
      "  Fake mean: min=0.0000, max=0.6116, avg=0.0782\n",
      "\n",
      "classifier.layer3.4.conv2:\n",
      "  Artifact signature: mean=4.753474, max=30.139694, min=0.015394\n",
      "  Real mean: min=-38.9546, max=14.9722, avg=-3.3497\n",
      "  Fake mean: min=-16.6645, max=13.8081, avg=-2.4114\n",
      "\n",
      "classifier.layer3.4.bn2:\n",
      "  Artifact signature: mean=0.007811, max=0.281917, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.5648, avg=0.0261\n",
      "  Fake mean: min=0.0000, max=0.6889, avg=0.0313\n",
      "\n",
      "classifier.layer3.4.conv3:\n",
      "  Artifact signature: mean=0.105507, max=0.699716, min=0.000559\n",
      "  Real mean: min=-2.6632, max=3.1652, avg=0.0258\n",
      "  Fake mean: min=-2.9526, max=3.7178, avg=0.0073\n",
      "\n",
      "classifier.layer3.4.bn3:\n",
      "  Artifact signature: mean=0.080684, max=1.334305, min=0.000000\n",
      "  Real mean: min=0.0000, max=2.8040, avg=0.1597\n",
      "  Fake mean: min=0.0000, max=2.6455, avg=0.1642\n",
      "\n",
      "classifier.layer3.5.conv1:\n",
      "  Artifact signature: mean=3.705061, max=15.402603, min=0.015745\n",
      "  Real mean: min=-19.9755, max=14.8345, avg=-2.7819\n",
      "  Fake mean: min=-17.3320, max=15.1983, avg=-1.8353\n",
      "\n",
      "classifier.layer3.5.bn1:\n",
      "  Artifact signature: mean=0.032853, max=0.386641, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.7318, avg=0.0454\n",
      "  Fake mean: min=0.0000, max=0.8181, avg=0.0529\n",
      "\n",
      "classifier.layer3.5.conv2:\n",
      "  Artifact signature: mean=1.847902, max=8.736993, min=0.039248\n",
      "  Real mean: min=-12.5582, max=11.1471, avg=-1.5400\n",
      "  Fake mean: min=-10.8840, max=12.2496, avg=-1.7911\n",
      "\n",
      "classifier.layer3.5.bn2:\n",
      "  Artifact signature: mean=0.017040, max=0.258410, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.6279, avg=0.0393\n",
      "  Fake mean: min=0.0000, max=0.7843, avg=0.0496\n",
      "\n",
      "classifier.layer3.5.conv3:\n",
      "  Artifact signature: mean=0.384106, max=1.542799, min=0.001693\n",
      "  Real mean: min=-2.2187, max=1.9317, avg=0.0179\n",
      "  Fake mean: min=-3.2339, max=3.3536, avg=-0.0691\n",
      "\n",
      "classifier.layer3.5.bn3:\n",
      "  Artifact signature: mean=0.073626, max=1.380392, min=0.000000\n",
      "  Real mean: min=0.0000, max=3.1536, avg=0.1381\n",
      "  Fake mean: min=0.0000, max=2.8542, avg=0.1404\n",
      "\n",
      "classifier.layer4.0.conv1:\n",
      "  Artifact signature: mean=4.568976, max=21.478003, min=0.000007\n",
      "  Real mean: min=-18.5167, max=24.9220, avg=-1.3353\n",
      "  Fake mean: min=-16.4736, max=18.8618, avg=-0.7679\n",
      "\n",
      "classifier.layer4.0.bn1:\n",
      "  Artifact signature: mean=0.039991, max=0.613998, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.8406, avg=0.0356\n",
      "  Fake mean: min=0.0000, max=0.8645, avg=0.0441\n",
      "\n",
      "classifier.layer4.0.conv2:\n",
      "  Artifact signature: mean=11.931600, max=41.003876, min=0.024293\n",
      "  Real mean: min=-33.0632, max=29.6789, avg=-4.0542\n",
      "  Fake mean: min=-54.7369, max=32.6961, avg=-6.0695\n",
      "\n",
      "classifier.layer4.0.bn2:\n",
      "  Artifact signature: mean=0.061344, max=0.571903, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.8073, avg=0.0840\n",
      "  Fake mean: min=0.0000, max=1.0767, avg=0.0670\n",
      "\n",
      "classifier.layer4.0.conv3:\n",
      "  Artifact signature: mean=3.855442, max=12.176587, min=0.000355\n",
      "  Real mean: min=-13.4940, max=12.3595, avg=-0.3691\n",
      "  Fake mean: min=-17.0306, max=17.9241, avg=0.3302\n",
      "\n",
      "classifier.layer4.0.bn3:\n",
      "  Artifact signature: mean=0.197473, max=1.923204, min=0.000000\n",
      "  Real mean: min=0.0000, max=2.0845, avg=0.1337\n",
      "  Fake mean: min=0.0000, max=1.6222, avg=0.1345\n",
      "\n",
      "classifier.layer4.0.downsample.0:\n",
      "  Artifact signature: mean=6.563287, max=22.636986, min=0.001811\n",
      "  Real mean: min=-27.0844, max=27.9139, avg=-1.4710\n",
      "  Fake mean: min=-25.2110, max=21.3230, avg=-1.8014\n",
      "\n",
      "classifier.layer4.0.downsample.1:\n",
      "  Artifact signature: mean=0.240736, max=2.180858, min=0.000127\n",
      "  Real mean: min=-0.7315, max=1.3154, avg=-0.1726\n",
      "  Fake mean: min=-0.8655, max=0.9984, avg=-0.1765\n",
      "\n",
      "classifier.layer4.1.conv1:\n",
      "  Artifact signature: mean=17.497644, max=95.396881, min=0.001791\n",
      "  Real mean: min=-76.2574, max=51.0400, avg=-6.5670\n",
      "  Fake mean: min=-49.6917, max=44.0640, avg=0.4061\n",
      "\n",
      "classifier.layer4.1.bn1:\n",
      "  Artifact signature: mean=0.030795, max=0.598609, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.6435, avg=0.0114\n",
      "  Fake mean: min=0.0000, max=0.7319, avg=0.0259\n",
      "\n",
      "classifier.layer4.1.conv2:\n",
      "  Artifact signature: mean=8.902537, max=30.423588, min=0.188284\n",
      "  Real mean: min=-18.1562, max=10.3861, avg=-1.3230\n",
      "  Fake mean: min=-26.1013, max=19.9944, avg=0.0320\n",
      "\n",
      "classifier.layer4.1.bn2:\n",
      "  Artifact signature: mean=0.010984, max=0.510069, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.5857, avg=0.0042\n",
      "  Fake mean: min=0.0000, max=0.4977, avg=0.0094\n",
      "\n",
      "classifier.layer4.1.conv3:\n",
      "  Artifact signature: mean=0.605691, max=1.845322, min=0.000710\n",
      "  Real mean: min=-0.9052, max=0.9210, avg=0.0267\n",
      "  Fake mean: min=-1.6173, max=1.1623, avg=-0.2773\n",
      "\n",
      "classifier.layer4.1.bn3:\n",
      "  Artifact signature: mean=0.219400, max=2.367414, min=0.000000\n",
      "  Real mean: min=0.0000, max=2.2254, avg=0.1390\n",
      "  Fake mean: min=0.0000, max=2.4337, avg=0.1457\n",
      "\n",
      "classifier.layer4.2.conv1:\n",
      "  Artifact signature: mean=8.731661, max=105.020798, min=0.009969\n",
      "  Real mean: min=-68.9698, max=44.9890, avg=-4.0418\n",
      "  Fake mean: min=-56.6598, max=55.8957, avg=-1.3738\n",
      "\n",
      "classifier.layer4.2.bn1:\n",
      "  Artifact signature: mean=0.014451, max=0.632232, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.7951, avg=0.0056\n",
      "  Fake mean: min=0.0000, max=0.7319, avg=0.0134\n",
      "\n",
      "classifier.layer4.2.conv2:\n",
      "  Artifact signature: mean=3.696981, max=13.592614, min=0.038380\n",
      "  Real mean: min=-3.6733, max=4.1220, avg=0.1711\n",
      "  Fake mean: min=-14.2992, max=8.3483, avg=-1.7716\n",
      "\n",
      "classifier.layer4.2.bn2:\n",
      "  Artifact signature: mean=0.010084, max=0.470010, min=0.000000\n",
      "  Real mean: min=0.0000, max=0.6126, avg=0.0065\n",
      "  Fake mean: min=0.0000, max=0.6130, avg=0.0088\n",
      "\n",
      "classifier.layer4.2.conv3:\n",
      "  Artifact signature: mean=0.628369, max=1.652568, min=0.000941\n",
      "  Real mean: min=-1.4222, max=1.2173, avg=-0.1824\n",
      "  Fake mean: min=-1.5544, max=1.3515, avg=-0.0289\n",
      "\n",
      "classifier.layer4.2.bn3:\n",
      "  Artifact signature: mean=0.294870, max=2.114047, min=0.000000\n",
      "  Real mean: min=0.0000, max=2.2616, avg=0.1104\n",
      "  Fake mean: min=0.0000, max=2.1799, avg=0.3002\n",
      "\n",
      "============================================================\n",
      "Overall average artifact signature: 1.255952\n",
      "============================================================\n",
      "✅ 정상! Artifact signature가 충분히 큼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6743/156959523.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  separated_stats = torch.load(STATS_PATH)\n"
     ]
    }
   ],
   "source": [
    "# Stats 파일 로드\n",
    "import torch\n",
    "\n",
    "STATS_PATH = f\"separated_stats_{MODEL}_progan.pth\"\n",
    "separated_stats = torch.load(STATS_PATH)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Artifact Discriminability (|fake_mean - real_mean|) 확인\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# 각 레이어별로 확인\n",
    "for layer_name, stats in separated_stats.items():\n",
    "    real_mean = stats['real']['mean']\n",
    "    fake_mean = stats['fake']['mean']\n",
    "\n",
    "    artifact_sig = (fake_mean - real_mean).abs()\n",
    "\n",
    "    print(f\"{layer_name}:\")\n",
    "    print(f\"  Artifact signature: mean={artifact_sig.mean():.6f}, max={artifact_sig.max():.6f}, min={artifact_sig.min():.6f}\")\n",
    "\n",
    "    # Real/Fake mean 범위 확인\n",
    "    print(f\"  Real mean: min={real_mean.min():.4f}, max={real_mean.max():.4f}, avg={real_mean.mean():.4f}\")\n",
    "    print(f\"  Fake mean: min={fake_mean.min():.4f}, max={fake_mean.max():.4f}, avg={fake_mean.mean():.4f}\")\n",
    "    print()\n",
    "\n",
    "# 전체 평균\n",
    "all_artifact_sigs = []\n",
    "for stats in separated_stats.values():\n",
    "    artifact_sig = (stats['fake']['mean'] - stats['real']['mean']).abs()\n",
    "    all_artifact_sigs.append(artifact_sig.mean().item())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Overall average artifact signature: {sum(all_artifact_sigs)/len(all_artifact_sigs):.6f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 문제 진단\n",
    "avg_sig = sum(all_artifact_sigs)/len(all_artifact_sigs)\n",
    "if avg_sig < 0.001:\n",
    "    print(\"🚨 문제! Artifact signature가 너무 작습니다!\")\n",
    "    print(\"   Real과 Fake의 mean이 거의 같음 → Stats 계산 오류 가능성\")\n",
    "elif avg_sig < 0.01:\n",
    "    print(\"⚠️  주의! Artifact signature가 작습니다.\")\n",
    "    print(\"   Real/Fake 구분이 약함\")\n",
    "else:\n",
    "    print(\"✅ 정상! Artifact signature가 충분히 큼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Gate Values 확인 (첫 번째 레이어)\n",
      "============================================================\n",
      "\n",
      "classifier.conv1:\n",
      "  Temperature: 1.0000\n",
      "  Channel bias (mean): +0.0000\n",
      "  Artifact disc (mean): 0.0062\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "exp(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m expected_sens = \u001b[32m0.01\u001b[39m  \u001b[38;5;66;03m# 가정\u001b[39;00m\n\u001b[32m     50\u001b[39m expected_score = disc_mean / expected_sens\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m expected_gate = \u001b[32m1\u001b[39m / (\u001b[32m1\u001b[39m + \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_score\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_mean\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Expected score (if sens=0.01): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Expected gate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_gate\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: exp(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "# Gate values 실시간 확인\n",
    "import torch\n",
    "\n",
    "# ProGAN original 샘플 하나로 테스트\n",
    "progan_orig_indices = [i for i, s in enumerate(dataset.samples)\n",
    "                        if s['dataset'] == \"corrupted_test_data_progan\"\n",
    "                        and s['corruption'] == \"original\"]\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Subset(dataset, progan_orig_indices[:32]),  # 1 batch만\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Forward pass하면서 gate values 캡처\n",
    "batch = next(iter(test_loader))\n",
    "images = batch[0].to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Forward하면서 gate module에서 직접 확인\n",
    "    # 첫 번째 gate만 체크\n",
    "    gate_name = list(model.gates.keys())[0]\n",
    "    gate_module = model.gates[gate_name]\n",
    "\n",
    "    # Dummy forward to get intermediate activations\n",
    "    _ = model(images)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Gate Values 확인 (첫 번째 레이어)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 수동으로 gate 계산 확인\n",
    "for sanitized_name, gate in list(model.gates.items())[:3]:  # 처음 3개만\n",
    "    original_name = model.gate_name_mapping[sanitized_name]\n",
    "\n",
    "    # Temperature, bias\n",
    "    temp = gate.temperature.item()\n",
    "    bias_mean = gate.channel_bias.mean().item()\n",
    "    disc_mean = gate.artifact_discriminability.mean().item()\n",
    "\n",
    "    print(f\"\\n{original_name}:\")\n",
    "    print(f\"  Temperature: {temp:.4f}\")\n",
    "    print(f\"  Channel bias (mean): {bias_mean:+.4f}\")\n",
    "    print(f\"  Artifact disc (mean): {disc_mean:.4f}\")\n",
    "\n",
    "    # 예상 score 계산 (rough estimate)\n",
    "    # sensitivity는 original이므로 매우 작아야 함 (예: 0.01)\n",
    "    expected_sens = 0.01  # 가정\n",
    "    expected_score = disc_mean / expected_sens\n",
    "    expected_gate = 1 / (1 + torch.exp(-temp * expected_score - bias_mean))\n",
    "\n",
    "    print(f\"  Expected score (if sens=0.01): {expected_score:.2f}\")\n",
    "    print(f\"  Expected gate: {expected_gate:.4f}\")\n",
    "\n",
    "    # 실제로는 sensitivity가 얼마인지 확인 필요\n",
    "    print(f\"  ⚠️  Actual sensitivity를 확인해야 정확함!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Gate Debug Info (Original Clean Data)\n",
      "======================================================================\n",
      "\n",
      "classifier.conv1:\n",
      "  Sensitivity: min=0.000000, mean=0.497076, max=14.903045\n",
      "  Score:       min=0.00, mean=1.84, max=3005.12\n",
      "  Gate:        min=0.5005, mean=0.5567, max=1.0000\n",
      "  Pruned:      0/64 (0.0%)\n",
      "\n",
      "classifier.bn1:\n",
      "  Sensitivity: min=0.000000, mean=0.078641, max=1.081109\n",
      "  Score:       min=0.00, mean=0.23, max=97.60\n",
      "  Gate:        min=0.5000, mean=0.5479, max=0.9579\n",
      "  Pruned:      0/64 (0.0%)\n",
      "\n",
      "classifier.layer1.0.conv1:\n",
      "  Sensitivity: min=0.000000, mean=0.805715, max=4.691023\n",
      "  Score:       min=0.00, mean=0.42, max=332.76\n",
      "  Gate:        min=0.5000, mean=0.5484, max=1.0000\n",
      "  Pruned:      0/64 (0.0%)\n",
      "\n",
      "classifier.layer1.0.bn1:\n",
      "  Sensitivity: min=0.000000, mean=0.408792, max=1.631517\n",
      "  Score:       min=0.00, mean=0.09, max=37.88\n",
      "  Gate:        min=0.5000, mean=0.5195, max=0.8477\n",
      "  Pruned:      0/64 (0.0%)\n",
      "\n",
      "classifier.layer1.0.conv2:\n",
      "  Sensitivity: min=0.000000, mean=2.104465, max=10.741886\n",
      "  Score:       min=0.00, mean=0.37, max=218.89\n",
      "  Gate:        min=0.5000, mean=0.5453, max=1.0000\n",
      "  Pruned:      0/64 (0.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실제 Gate 동작 확인\n",
    "import torch\n",
    "\n",
    "# Forward 시 gate values를 캡처하는 hook 추가\n",
    "gate_debug_info = {}\n",
    "\n",
    "def debug_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        x = input[0]  # Input activation\n",
    "\n",
    "        # Sensitivity 계산\n",
    "        sensitivity = module.compute_gradient_corruption_sensitivity(x)  # [B, C]\n",
    "\n",
    "        # Discriminability\n",
    "        disc = module.artifact_discriminability  # [C]\n",
    "\n",
    "        # Score\n",
    "        score = disc.unsqueeze(0) / (sensitivity + 1e-6)  # [B, C]\n",
    "\n",
    "        # Gate (batch aggregation)\n",
    "        score_agg = score.mean(dim=0)  # [C]\n",
    "        gate_logits = module.temperature * score_agg + module.channel_bias\n",
    "        gate = torch.sigmoid(gate_logits)  # [C]\n",
    "\n",
    "        # 저장\n",
    "        gate_debug_info[name] = {\n",
    "            'sensitivity_min': sensitivity.min().item(),\n",
    "            'sensitivity_mean': sensitivity.mean().item(),\n",
    "            'sensitivity_max': sensitivity.max().item(),\n",
    "            'score_min': score.min().item(),\n",
    "            'score_mean': score.mean().item(),\n",
    "            'score_max': score.max().item(),\n",
    "            'gate_min': gate.min().item(),\n",
    "            'gate_mean': gate.mean().item(),\n",
    "            'gate_max': gate.max().item(),\n",
    "            'gate_below_05': (gate < 0.5).sum().item(),  # 얼마나 많은 채널이 pruned?\n",
    "        }\n",
    "    return hook\n",
    "\n",
    "# 처음 5개 레이어에 debug hook 추가\n",
    "debug_handles = []\n",
    "for i, (sanitized_name, gate_module) in enumerate(list(model.gates.items())[:5]):\n",
    "    original_name = model.gate_name_mapping[sanitized_name]\n",
    "    handle = gate_module.register_forward_hook(debug_hook(original_name))\n",
    "    debug_handles.append(handle)\n",
    "\n",
    "# ProGAN original 데이터로 forward\n",
    "progan_orig_indices = [i for i, s in enumerate(dataset.samples)\n",
    "                        if s['dataset'] == \"corrupted_test_data_progan\"\n",
    "                        and s['corruption'] == \"original\"]\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Subset(dataset, progan_orig_indices[:32]),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "images = batch[0].to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = model(images)\n",
    "\n",
    "# Hook 제거\n",
    "for handle in debug_handles:\n",
    "    handle.remove()\n",
    "\n",
    "# 결과 출력\n",
    "print(\"=\"*70)\n",
    "print(\"Gate Debug Info (Original Clean Data)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "for layer_name, info in gate_debug_info.items():\n",
    "    print(f\"{layer_name}:\")\n",
    "    print(f\"  Sensitivity: min={info['sensitivity_min']:.6f}, mean={info['sensitivity_mean']:.6f}, max={info['sensitivity_max']:.6f}\")\n",
    "    print(f\"  Score:       min={info['score_min']:.2f}, mean={info['score_mean']:.2f}, max={info['score_max']:.2f}\")\n",
    "    print(f\"  Gate:        min={info['gate_min']:.4f}, mean={info['gate_mean']:.4f}, max={info['gate_max']:.4f}\")\n",
    "\n",
    "    num_channels = len(model.gates[layer_name.replace('.', '_')].artifact_discriminability)\n",
    "    pruned_ratio = info['gate_below_05'] / num_channels\n",
    "    print(f\"  Pruned:      {info['gate_below_05']}/{num_channels} ({pruned_ratio*100:.1f}%)\")\n",
    "\n",
    "    if pruned_ratio > 0.5:\n",
    "        print(f\"  🚨 문제! 50% 이상 채널이 pruned됨!\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Parameters 확인\n",
    "\n",
    "각 layer의 temperature가 어떻게 설정/학습되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Learned Parameters (Temperature & Channel Bias)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for sanitized_name, gate in model.gates.items():\n",
    "    # Get original layer name\n",
    "    original_name = model.gate_name_mapping[sanitized_name]\n",
    "    temp_value = gate.temperature.item()\n",
    "    print(f\"{original_name:50s}: temperature = {temp_value:.4f}\")\n",
    "    \n",
    "    # Artifact discriminability\n",
    "    artifact_disc = gate.artifact_discriminability\n",
    "    disc_mean = artifact_disc.mean().item()\n",
    "    disc_std = artifact_disc.std().item()\n",
    "    disc_max = artifact_disc.max().item()\n",
    "    print(f\"{'':50s}  artifact_disc: mean={disc_mean:.4f}, std={disc_std:.4f}, max={disc_max:.4f}\")\n",
    "    \n",
    "    # Channel bias statistics\n",
    "    if config.use_channel_bias:\n",
    "        bias_mean = gate.channel_bias.mean().item()\n",
    "        bias_std = gate.channel_bias.std().item()\n",
    "        bias_min = gate.channel_bias.min().item()\n",
    "        bias_max = gate.channel_bias.max().item()\n",
    "        print(f\"{'':50s}  channel_bias: mean={bias_mean:+.4f}, std={bias_std:.4f}, range=[{bias_min:+.4f}, {bias_max:+.4f}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Channel Pruning v1 (IMPROVED) 핵심:\n",
    "\n",
    "#### 1. **LGrad의 작동 원리**\n",
    "```\n",
    "Image → Gradient (StyleGAN) → Classifier → Prediction\n",
    "        ↓                      ↓\n",
    "    GAN pattern          Pruned channels\n",
    "```\n",
    "- Real: Natural gradient pattern\n",
    "- Fake: GAN-specific gradient pattern  \n",
    "- Noise: Corrupts gradient pattern\n",
    "\n",
    "#### 2. **핵심 아이디어**\n",
    "- Gradient pattern을 robust하게 보존하는 채널 유지\n",
    "- Noise corruption에 민감한 채널 제거\n",
    "\n",
    "#### 3. **방법론 (IMPROVED)**\n",
    "\n",
    "**OLD (기존 CPv1)**:\n",
    "```python\n",
    "# ❌ Pseudo-label 의존\n",
    "pseudo_label = estimate_label(x)  # Noisy에서 부정확!\n",
    "if pseudo_label == Fake:\n",
    "    sensitivity = |curr - fake_stats|  # Artifact를 noise로 오인\n",
    "else:\n",
    "    sensitivity = |curr - real_stats|\n",
    "```\n",
    "\n",
    "**NEW (Gradient-Pattern-Aware)**:\n",
    "```python\n",
    "# ✅ Label-free minimum deviation\n",
    "dev_real = |curr - real_stats|\n",
    "dev_fake = |curr - fake_stats|\n",
    "sensitivity = min(dev_real, dev_fake)\n",
    "\n",
    "# Clean: close to at least one → small\n",
    "# Noisy: far from both → large\n",
    "```\n",
    "\n",
    "**Score 계산**:\n",
    "```python\n",
    "discriminability = |fake_stats - real_stats|  # Gradient pattern 차이\n",
    "score = discriminability / sensitivity\n",
    "\n",
    "# High score: Strong pattern detection + Low corruption sensitivity → KEEP\n",
    "# Low score: Weak detection + High sensitivity → PRUNE\n",
    "```\n",
    "\n",
    "#### 4. **주요 개선사항**\n",
    "\n",
    "| 측면 | 기존 CPv1 | IMPROVED CPv1 |\n",
    "|------|-----------|---------------|\n",
    "| Pseudo-label | ❌ 필요 (부정확) | ✅ 불필요 (label-free) |\n",
    "| Sensitivity | ❌ Label-aware (오류 가능) | ✅ Minimum deviation |\n",
    "| Gating | ❌ Batch-wise (불안정) | ✅ Channel-level (안정) |\n",
    "| LGrad 이해 | ❌ 일반적 | ✅ Gradient-specific |\n",
    "\n",
    "#### 5. **vs Channel Reweight v1**\n",
    "\n",
    "| 특징 | CRv1 | CPv1 (IMPROVED) |\n",
    "|------|------|-----------------|\n",
    "| Statistics | Mixed (Real+Fake) | Separated (Real, Fake) |\n",
    "| Artifact info | ❌ 구분 불가 | ✅ 보존 |\n",
    "| Method | Reweighting | Gradient-pattern-aware pruning |\n",
    "| Label-free | ✅ Yes | ✅ Yes (improved!) |\n",
    "\n",
    "### 작동 원리:\n",
    "\n",
    "1. **Pre-compute**: Real/Fake clean gradient features의 separated statistics\n",
    "2. **Test time**: Noisy gradient → classifier features\n",
    "3. **Sensitivity**: `min(|curr - real|, |curr - fake|)` (gradient corruption)\n",
    "4. **Discriminability**: `|fake - real|` (pattern difference)\n",
    "5. **Score**: `disc / sens`\n",
    "6. **Gating**: High score channels만 유지\n",
    "\n",
    "### 다음 단계:\n",
    "- ✅ Baseline (no gating)과 성능 비교\n",
    "- ✅ 기존 CPv1과 성능 비교\n",
    "- ✅ CRv1과 성능 비교\n",
    "- □ SGS, SAS 등 다른 방법들과 비교\n",
    "- □ NPR 모델에도 적용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
