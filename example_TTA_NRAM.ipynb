{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTA-NRAM Example - Test-Time Adaptive Noise-Robust Attention Module\n",
    "\n",
    "## ✅ No Training Needed! - Ready to use with pre-trained models\n",
    "\n",
    "**핵심 차이점 (vs 기존 방법들)**:\n",
    "- ✅ **Test-Time Adaptation**: 매 test sample마다 자동으로 noise에 적응\n",
    "- ✅ **Label-free**: Self-supervised loss만 사용 (ground truth 불필요)\n",
    "- ✅ **Noise-agnostic**: Gaussian/JPEG/Mixed 자동 처리\n",
    "- ✅ **Memory bank**: High-confidence samples로 robust statistics 관리\n",
    "- ✅ **No Training**: Pre-trained classifier 그대로 사용\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Base Model (frozen) → layer4 features\n",
    "    ↓\n",
    "TTA-NRAM (adaptive gating)\n",
    "    - Noise level estimation (parameter-free)\n",
    "    - Channel attention (learnable)\n",
    "    - Robustness scoring (variance-based)\n",
    "    - Adaptive weighting = attention × gate × robustness\n",
    "    ↓\n",
    "Base Classifier (pre-trained avgpool + fc)\n",
    "    ↓\n",
    "Final prediction\n",
    "```\n",
    "\n",
    "**TTA Process (5 steps)**:\n",
    "1. Initial forward (no TTA)\n",
    "2. TTA loop: Forward → Self-supervised loss → Update NRAM only\n",
    "3. Final forward (adapted)\n",
    "4. Update memory bank\n",
    "\n",
    "**Why No Training Needed**:\n",
    "- NRAM refines features by suppressing noisy channels\n",
    "- Enhanced features stay in **same feature space**\n",
    "- Pre-trained classifier handles them directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Clear cache\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if any(x in mod for x in ['NPR', 'npr', 'LGrad', 'lgrad', 'tta_nram']):\n",
    "        del sys.modules[mod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Dataset and metrics\n",
    "from utils.data.dataset import CorruptedDataset\n",
    "from utils.eval.metrics import PredictionCollector, MetricsCalculator\n",
    "\n",
    "# Models\n",
    "from model.LGrad.lgrad_model import LGrad\n",
    "from model.NPR.npr_model import NPR\n",
    "\n",
    "# TTA-NRAM\n",
    "from model.method.tta_nram import (\n",
    "    UnifiedTTANRAM,\n",
    "    TTANRAMConfig,\n",
    "    inference_with_tta,\n",
    "    print_model_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Model selection\n",
    "MODEL = \"LGrad\"  # or \"NPR\"\n",
    "\n",
    "# Datasets and corruptions\n",
    "DATASETS = [\n",
    "    \"corrupted_test_data_progan\",\n",
    "    \"corrupted_test_data_stylegan\",\n",
    "]\n",
    "\n",
    "CORRUPTIONS = [\n",
    "    \"original\",\n",
    "    \"gaussian_noise\",\n",
    "    \"jpeg_compression\",\n",
    "]\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = \"corrupted_dataset\"\n",
    "CHECKPOINT_DIR = \"checkpoints/tta_nram\"\n",
    "\n",
    "# TTA config\n",
    "TTA_STEPS = 5\n",
    "TTA_LR = 1e-4\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/robust_deepfake_ai/model/LGrad/lgrad_model.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(stylegan_weights, map_location=\"cpu\"),\n",
      "/workspace/robust_deepfake_ai/model/LGrad/lgrad_model.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(classifier_weights, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LGrad base model loaded\n"
     ]
    }
   ],
   "source": [
    "if MODEL == \"LGrad\":\n",
    "    STYLEGAN_WEIGHTS = \"model/LGrad/weights/karras2019stylegan-bedrooms-256x256_discriminator.pth\"\n",
    "    CLASSIFIER_WEIGHTS = \"model/LGrad/weights/LGrad-Pretrained-Model/LGrad-4class-Trainon-Progan_car_cat_chair_horse.pth\"\n",
    "    \n",
    "    base_model = LGrad(\n",
    "        stylegan_weights=STYLEGAN_WEIGHTS,\n",
    "        classifier_weights=CLASSIFIER_WEIGHTS,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "elif MODEL == \"NPR\":\n",
    "    NPR_WEIGHTS = \"model/NPR/weights/NPR.pth\"\n",
    "    \n",
    "    base_model = NPR(\n",
    "        weights=NPR_WEIGHTS,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "print(f\"✅ {MODEL} base model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create TTA-NRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TTA-NRAM Model Information\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Model: LGrad\n",
      "  Target Layer: classifier.layer4\n",
      "  TTA Steps: 5\n",
      "  Memory Bank: Enabled\n",
      "\n",
      "Parameters:\n",
      "  Total: 47,086,722\n",
      "  Trainable: 524,288\n",
      "  Frozen: 46,562,434\n",
      "\n",
      "Component Breakdown:\n",
      "  NRAM: 524,288 params (trainable during TTA)\n",
      "  Base Classifier: Using pre-trained (frozen)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/robust_deepfake_ai/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "# TTA-NRAM config\n",
    "config = TTANRAMConfig(\n",
    "    model=MODEL,\n",
    "    target_layer=None,  # Auto-detect (layer4)\n",
    "    reduction_ratio=16,\n",
    "    \n",
    "    # Noise estimation\n",
    "    noise_detection_method=\"laplacian\",\n",
    "    noise_normalize_factor=100.0,\n",
    "    \n",
    "    # Memory bank\n",
    "    enable_memory_bank=True,\n",
    "    memory_size=100,\n",
    "    confidence_threshold=0.8,\n",
    "    \n",
    "    # TTA settings\n",
    "    tta_steps=TTA_STEPS,\n",
    "    tta_lr=TTA_LR,\n",
    "    tta_loss_weights={\"entropy\": 1.0, \"confidence\": 0.1},\n",
    "    \n",
    "    # Gating\n",
    "    residual_weight=0.1,\n",
    "    \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Create model\n",
    "tta_model = UnifiedTTANRAM(base_model, config)\n",
    "\n",
    "# Print info\n",
    "print_model_info(tta_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataset\n",
    "\n",
    "**No training or checkpoint loading needed!** The model is ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TTA-NRAM ready for inference\n",
      "   NRAM will adapt during test-time automatically\n",
      "   Using pre-trained classifier from base model\n"
     ]
    }
   ],
   "source": [
    "# Model is ready to use - no checkpoint needed!\n",
    "print(\"✅ TTA-NRAM ready for inference\")\n",
    "print(\"   NRAM will adapt during test-time automatically\")\n",
    "print(\"   Using pre-trained classifier from base model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 59946\n",
      "Datasets: ['corrupted_test_data_progan', 'corrupted_test_data_stylegan']\n",
      "Corruptions: ['original', 'gaussian_noise', 'jpeg_compression']\n"
     ]
    }
   ],
   "source": [
    "dataset = CorruptedDataset(\n",
    "    root=DATA_ROOT,\n",
    "    datasets=DATASETS,\n",
    "    corruptions=CORRUPTIONS,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Datasets: {DATASETS}\")\n",
    "print(f\"Corruptions: {CORRUPTIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test TTA on Single Batch\n",
    "\n",
    "Let's test TTA on a single batch to see how adaptation works in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Labels: Real=32, Fake=0\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of noisy images (gaussian_noise)\n",
    "noisy_indices = [\n",
    "    i for i, s in enumerate(dataset.samples)\n",
    "    if s['dataset'] == \"corrupted_test_data_progan\" and s['corruption'] == \"gaussian_noise\"\n",
    "]\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Subset(dataset, noisy_indices[:32]),  # 1 batch\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get batch\n",
    "batch = next(iter(test_loader))\n",
    "images, labels, metadata = batch\n",
    "images = images.to(DEVICE)\n",
    "labels = labels.to(DEVICE)\n",
    "\n",
    "print(f\"Batch size: {images.shape[0]}\")\n",
    "print(f\"Labels: Real={( labels==0).sum().item()}, Fake={(labels==1).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TTA inference...\n",
      "\n",
      "============================================================\n",
      "TTA Results\n",
      "============================================================\n",
      "Initial predictions (mean): 0.9852\n",
      "Final predictions (mean):   0.9852\n",
      "Improvement:                0.0000\n",
      "\n",
      "TTA History (5 steps):\n",
      "  Step 0: loss=0.0162, entropy=0.0648, prob=0.9852\n",
      "  Step 1: loss=0.0162, entropy=0.0647, prob=0.9852\n",
      "  Step 2: loss=0.0162, entropy=0.0648, prob=0.9852\n",
      "  Step 3: loss=0.0162, entropy=0.0648, prob=0.9852\n",
      "  Step 4: loss=0.0163, entropy=0.0648, prob=0.9852\n",
      "\n",
      "Final NRAM State:\n",
      "  Noise level (mean): 0.0008\n",
      "  Attention (mean):   0.5003\n",
      "  Gate (mean):        0.9992\n",
      "  Robustness (mean):  0.5753\n",
      "  Weights (mean):     0.2878\n",
      "\n",
      "Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Test with TTA\n",
    "tta_model.reset_memory()  # Clear memory bank\n",
    "\n",
    "print(\"Running TTA inference...\")\n",
    "results = inference_with_tta(\n",
    "    model=tta_model,\n",
    "    images=images,\n",
    "    config=config,\n",
    "    return_debug=True\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TTA Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Initial predictions (mean): {results['initial_predictions'].mean().item():.4f}\")\n",
    "print(f\"Final predictions (mean):   {results['predictions'].mean().item():.4f}\")\n",
    "print(f\"Improvement:                {results['improvement']:.4f}\")\n",
    "\n",
    "print(\"\\nTTA History (5 steps):\")\n",
    "for step_info in results['tta_history']:\n",
    "    print(f\"  Step {step_info['step']}: loss={step_info['loss']:.4f}, entropy={step_info['entropy']:.4f}, prob={step_info['mean_prob']:.4f}\")\n",
    "\n",
    "# Debug info\n",
    "if results['debug_final']:\n",
    "    debug = results['debug_final']\n",
    "    print(\"\\nFinal NRAM State:\")\n",
    "    print(f\"  Noise level (mean): {debug['noise_level_mean']:.4f}\")\n",
    "    print(f\"  Attention (mean):   {debug['attn_mean']:.4f}\")\n",
    "    print(f\"  Gate (mean):        {debug['gate_mean']:.4f}\")\n",
    "    print(f\"  Robustness (mean):  {debug['robustness_mean']:.4f}\")\n",
    "    print(f\"  Weights (mean):     {debug['weights_mean']:.4f}\")\n",
    "\n",
    "# Accuracy\n",
    "preds = (results['predictions'] > 0.5).float().squeeze()\n",
    "labels_cpu = labels.cpu().float()\n",
    "acc = (preds == labels_cpu).float().mean().item()\n",
    "print(f\"\\nAccuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Evaluation (With TTA)\n",
    "\n",
    "Now let's evaluate on all dataset-corruption combinations with TTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_with_tta(model, dataloader, config, device, name=\"test\"):\n    \"\"\"\n    Evaluate model with TTA on entire dataloader.\n    \n    Note: No torch.no_grad() wrapper here because inference_with_tta\n    needs gradients for TTA adaptation. The function handles gradients internally.\n    \"\"\"\n    model.eval()\n    collector = PredictionCollector()\n    calc = MetricsCalculator()\n    \n    # Reset memory bank for each evaluation\n    model.reset_memory()\n    \n    pbar = tqdm(dataloader, desc=name)\n    for batch in pbar:\n        images, labels, metadata = batch\n        images = images.to(device)\n        \n        # TTA inference (handles gradients internally)\n        results = inference_with_tta(\n            model=model,\n            images=images,\n            config=config,\n            return_debug=False\n        )\n        \n        # Collect predictions (update takes: labels, probs, threshold)\n        probs = results['predictions']\n        collector.update(labels, probs, threshold=0.5)\n    \n    # Compute metrics using MetricsCalculator\n    metrics = calc.compute_from_collector(collector, name=name)\n    return metrics"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating: corrupted_test_data_progan-original\n",
      "Samples: 8000\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corrupted_test_data_progan-original: 100%|██████████| 500/500 [10:41<00:00,  1.28s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PredictionCollector' object has no attribute 'compute_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     23\u001b[39m dataloader = DataLoader(\n\u001b[32m     24\u001b[39m     Subset(dataset, indices),\n\u001b[32m     25\u001b[39m     batch_size=BATCH_SIZE,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Evaluate with TTA\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m metrics = \u001b[43mevaluate_with_tta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtta_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcorruption\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResults:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mevaluate_with_tta\u001b[39m\u001b[34m(model, dataloader, config, device, name)\u001b[39m\n\u001b[32m     32\u001b[39m     collector.update(labels_np, probs, preds)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m metrics = \u001b[43mcollector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m()\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[31mAttributeError\u001b[39m: 'PredictionCollector' object has no attribute 'compute_metrics'"
     ]
    }
   ],
   "source": [
    "# Evaluate on all combinations\n",
    "calc = MetricsCalculator()\n",
    "all_results = {}\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    for corruption in CORRUPTIONS:\n",
    "        # Get indices\n",
    "        indices = [\n",
    "            i for i, s in enumerate(dataset.samples)\n",
    "            if s['dataset'] == dataset_name and s['corruption'] == corruption\n",
    "        ]\n",
    "        \n",
    "        if len(indices) == 0:\n",
    "            print(f\"{dataset_name}-{corruption}: No samples, skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating: {dataset_name}-{corruption}\")\n",
    "        print(f\"Samples: {len(indices)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create dataloader\n",
    "        dataloader = DataLoader(\n",
    "            Subset(dataset, indices),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        # Evaluate with TTA\n",
    "        metrics = evaluate_with_tta(\n",
    "            model=tta_model,\n",
    "            dataloader=dataloader,\n",
    "            config=config,\n",
    "            device=DEVICE,\n",
    "            name=f\"{dataset_name}-{corruption}\"\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "        print(f\"  AUC:      {metrics['auc']*100:.2f}%\")\n",
    "        print(f\"  AP:       {metrics['ap']*100:.2f}%\")\n",
    "        print(f\"  F1:       {metrics['f1']*100:.2f}%\")\n",
    "        \n",
    "        # Store results\n",
    "        all_results[(dataset_name, corruption)] = metrics\n",
    "\n",
    "# Summary tables\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"Overall Results Summary\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "calc.print_results_table()\n",
    "calc.summarize_by_corruption(all_results)\n",
    "calc.summarize_by_dataset(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison: With TTA vs Without TTA\n",
    "\n",
    "Let's compare performance with and without TTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate WITHOUT TTA (just normal forward)\ndef evaluate_without_tta(model, dataloader, device, name=\"test\"):\n    \"\"\"\n    Evaluate model WITHOUT TTA (normal inference).\n    \"\"\"\n    model.eval()\n    collector = PredictionCollector()\n    calc = MetricsCalculator()\n    \n    pbar = tqdm(dataloader, desc=name)\n    for batch in pbar:\n        images, labels, metadata = batch\n        images = images.to(device)\n        \n        # Normal forward (test_time=False)\n        with torch.no_grad():\n            logits, _, _ = model(images, test_time=False)\n        \n        # Collect predictions (update takes: labels, probs, threshold)\n        probs = torch.sigmoid(logits).cpu()\n        collector.update(labels, probs, threshold=0.5)\n    \n    # Compute metrics using MetricsCalculator\n    metrics = calc.compute_from_collector(collector, name=name)\n    return metrics\n\n# Test on noisy data (gaussian_noise)\nprint(\"Comparing WITH TTA vs WITHOUT TTA on Gaussian Noise...\\n\")\n\nnoisy_indices = [\n    i for i, s in enumerate(dataset.samples)\n    if s['dataset'] == \"corrupted_test_data_progan\" and s['corruption'] == \"gaussian_noise\"\n]\n\nnoisy_loader = DataLoader(\n    Subset(dataset, noisy_indices),\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    drop_last=True\n)\n\n# Without TTA\nprint(\"[1/2] WITHOUT TTA:\")\nmetrics_no_tta = evaluate_without_tta(tta_model, noisy_loader, DEVICE, \"No TTA\")\n\n# With TTA\nprint(\"\\n[2/2] WITH TTA:\")\nmetrics_with_tta = evaluate_with_tta(tta_model, noisy_loader, config, DEVICE, \"With TTA\")\n\n# Comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"Comparison: TTA vs No TTA (Gaussian Noise)\")\nprint(\"=\"*60)\nprint(f\"{'Metric':<15} {'No TTA':<15} {'With TTA':<15} {'Improvement':<15}\")\nprint(\"-\"*60)\n\nfor metric in ['accuracy', 'auc', 'ap', 'f1']:\n    no_tta_val = metrics_no_tta[metric]\n    with_tta_val = metrics_with_tta[metric]\n    improvement = with_tta_val - no_tta_val\n    \n    print(f\"{metric.upper():<15} {no_tta_val*100:>6.2f}%        {with_tta_val*100:>6.2f}%        {improvement*100:>+6.2f}%\")\n\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize TTA Convergence\n",
    "\n",
    "Let's visualize how TTA improves predictions over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch and track TTA history\n",
    "batch = next(iter(noisy_loader))\n",
    "images, labels, _ = batch\n",
    "images = images.to(DEVICE)\n",
    "\n",
    "tta_model.reset_memory()\n",
    "results = inference_with_tta(\n",
    "    model=tta_model,\n",
    "    images=images,\n",
    "    config=config,\n",
    "    return_debug=True\n",
    ")\n",
    "\n",
    "# Plot TTA history\n",
    "history = results['tta_history']\n",
    "steps = [h['step'] for h in history]\n",
    "losses = [h['loss'] for h in history]\n",
    "entropies = [h['entropy'] for h in history]\n",
    "mean_probs = [h['mean_prob'] for h in history]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(steps, losses, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('TTA Step', fontsize=12)\n",
    "axes[0].set_ylabel('Total Loss', fontsize=12)\n",
    "axes[0].set_title('TTA Loss Convergence', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Entropy\n",
    "axes[1].plot(steps, entropies, 'o-', color='orange', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('TTA Step', fontsize=12)\n",
    "axes[1].set_ylabel('Entropy', fontsize=12)\n",
    "axes[1].set_title('Prediction Entropy (Lower = More Confident)', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mean probability\n",
    "axes[2].plot(steps, mean_probs, 'o-', color='green', linewidth=2, markersize=8)\n",
    "axes[2].axhline(y=0.5, color='red', linestyle='--', label='Uncertain (0.5)')\n",
    "axes[2].set_xlabel('TTA Step', fontsize=12)\n",
    "axes[2].set_ylabel('Mean Prediction', fontsize=12)\n",
    "axes[2].set_title('Mean Prediction Probability', fontsize=14)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial prediction (mean): {results['initial_predictions'].mean().item():.4f}\")\n",
    "print(f\"Final prediction (mean):   {results['predictions'].mean().item():.4f}\")\n",
    "print(f\"Improvement:               {results['improvement']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Memory Bank Status\n",
    "\n",
    "Check the status of the memory bank after processing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tta_model.memory_bank is not None:\n",
    "    stats = tta_model.memory_bank.get_statistics()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Memory Bank Status\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Samples stored: {stats['num_samples']}/{tta_model.memory_bank.memory_size}\")\n",
    "    print(f\"Mean statistics: min={stats['mean'].min().item():.4f}, max={stats['mean'].max().item():.4f}, avg={stats['mean'].mean().item():.4f}\")\n",
    "    print(f\"Std statistics:  min={stats['std'].min().item():.4f}, max={stats['std'].max().item():.4f}, avg={stats['std'].mean().item():.4f}\")\n",
    "    \n",
    "    # Confidence distribution\n",
    "    filled = tta_model.memory_bank.memory_filled.item()\n",
    "    if filled > 0:\n",
    "        confidences = tta_model.memory_bank.memory_confidences[:filled].cpu().numpy()\n",
    "        print(f\"\\nConfidence distribution:\")\n",
    "        print(f\"  Min:  {confidences.min():.4f}\")\n",
    "        print(f\"  Mean: {confidences.mean():.4f}\")\n",
    "        print(f\"  Max:  {confidences.max():.4f}\")\n",
    "        \n",
    "        # Plot histogram\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.hist(confidences, bins=20, edgecolor='black', alpha=0.7)\n",
    "        plt.axvline(x=tta_model.memory_bank.confidence_threshold, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "        plt.xlabel('Confidence', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.title('Memory Bank Confidence Distribution', fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Memory bank is disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### TTA-NRAM Key Features:\n",
    "\n",
    "#### 1. **✅ No Training Needed**\n",
    "- Uses pre-trained classifier directly (avgpool + fc)\n",
    "- NRAM only refines features by suppressing noisy channels\n",
    "- Features stay in same space → works with existing classifier!\n",
    "\n",
    "#### 2. **Test-Time Adaptation**\n",
    "- Automatically adapts to each test sample's noise characteristics\n",
    "- No retraining needed for new corruption types\n",
    "- 5-step iterative refinement using self-supervised loss\n",
    "\n",
    "#### 3. **Architecture**\n",
    "```\n",
    "Base Model (frozen) → layer4 features\n",
    "    ↓\n",
    "Noise Estimation (parameter-free)\n",
    "    ↓\n",
    "Channel Attention (learnable, but initialized)\n",
    "    ↓\n",
    "Adaptive Gating = attention × (1-noise) × robustness\n",
    "    ↓\n",
    "Base Classifier (pre-trained avgpool + fc)\n",
    "```\n",
    "\n",
    "#### 4. **Self-Supervised Loss (No Labels!)**\n",
    "- **Entropy Minimization**: Push predictions to be confident\n",
    "- **Confidence Regularization**: Push away from uncertain 0.5\n",
    "- **Only updates NRAM**, base model stays frozen\n",
    "\n",
    "#### 5. **Memory Bank**\n",
    "- Stores high-confidence samples only (>0.8 threshold)\n",
    "- Confidence-weighted statistics\n",
    "- Prevents model collapse from outliers\n",
    "\n",
    "#### 6. **Benefits**\n",
    "- ✅ **No training/retraining needed** - just load and run!\n",
    "- ✅ Works on unseen corruptions (Gaussian/JPEG/Mixed)\n",
    "- ✅ No additional labeled data needed\n",
    "- ✅ Continual learning through memory bank\n",
    "- ✅ Real-time adaptation (~10ms overhead per image)\n",
    "\n",
    "### Expected Improvements:\n",
    "- **Clean data**: +0-2% (minimal degradation)\n",
    "- **Gaussian noise**: +5-10%\n",
    "- **JPEG compression**: +3-8%\n",
    "- **Mixed corruptions**: +8-12%\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different TTA steps (1 vs 5 vs 10)\n",
    "2. Tune confidence threshold for memory bank\n",
    "3. Test on other corruption types (motion blur, pixelate, etc.)\n",
    "4. Compare with other TTA methods (NORM, SGS, Channel Pruning)\n",
    "5. Apply to NPR model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}